   #Tom's Hardware

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

Sign Up for e-mail newsletters

   Oops, something went wrong, please try again later.

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate next

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * CPUs

     Review

      AMD Ryzen 9 3900X and Ryzen 7 3700X Review: Zen 2 and 7nm Unleashed

   by Paul Alcorn July 7, 2019 at 7:07 AM

     *
     *
     *
     *
     *
     *

   180 Comments

   [ ] (*) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
   Page 1:Into the 7nm Era
   Page 2:7nm Process, Zen 2, and the X570 Chipset
   Page 3:Ryzen 3000 IPC Measurements and Power Consumption
   Page 4:Overclocking and Ryzen Master
   Page 5:Updated Test Metholodgy and Setup
   Page 6:VRmark, 3DMark and AotS: Escalation
   Page 7:Civilization VI Graphics and AI, Dawn of War III
   Page 8:Far Cry 5 and Final Fantasy XV
   Page 9:GTA: V and Hitman 2
   Page 10:Project Cars 2, The Division 2, and World of Tanks enCore
   Page 11:Office, Web Browser, and Productivity
   Page 12:Rendering, Encoding, Compression, Encryption
   Page 13:Conclusion
     *

Page 1:Into the 7nm Era
     *

Page 2:7nm Process, Zen 2, and the X570 Chipset
     *

Page 3:Ryzen 3000 IPC Measurements and Power Consumption
     *

Page 4:Overclocking and Ryzen Master
     *

Page 5:Updated Test Metholodgy and Setup
     *

Page 6:VRmark, 3DMark and AotS: Escalation
     *

Page 7:Civilization VI Graphics and AI, Dawn of War III
     *

Page 8:Far Cry 5 and Final Fantasy XV
     *

Page 9:GTA: V and Hitman 2
     *

Page 10:Project Cars 2, The Division 2, and World of Tanks enCore
     *

Page 11:Office, Web Browser, and Productivity
     *

Page 12:Rendering, Encoding, Compression, Encryption
     *

Page 13:Conclusion

   AMD's launch of the Ryzen 3000 series processors marks an occasion that
   was nearly unthinkable a few short years ago: AMD has taken the process
   lead over Intel by fielding new 7nm processors that contain smaller and
   more densely-packed transistors than Intel's competing 14nm chips. The
   advantages of increased density come in the form of higher performance,
   better power efficiency, more cores, and more cache packed into a
   smaller area than the first-gen Ryzen models, all of which makes
   third-gen Ryzen a potent adversary for Intel both on the desktop and in
   the data center.

   AMD paved the way for the 'Matisse' Ryzen 3000 series several years ago
   when it unveiled the revolutionary chiplet-based Zen microarchitecture.
   At the time, AMD laid out a roadmap that included a steady cadence of
   tick-tock-like updates interspersed with new revisions of the scalable
   microarchitecture. After the company's sophomore effort with the
   second-gen Ryzen processors, which featured a faster process paired
   with the same first-gen Zen design, the company is plowing forward with
   its Zen 2 architecture that AMD says offers up to 15% more instructions
   per cycle (IPC). Paired with the advantages of the 7nm process and more
   cores, not to mention AMD's trailblazing of the PCIe 4.0 interface on
   desktop platforms, the Ryzen 3000 chips promise an explosive step
   forward in performance.

   AMD's first chips to come packing TSMC's 7nm process span the entire
   range of the mainstream desktop stack, but push core counts up from
   eight cores to 12 cores and 24 threads with the Ryzen 9 3900X we have
   in the lab today, upsetting the status quo and bringing mainstream
   platforms into what used to be the realm of the pricey high end
   desktop. If you're looking for something even beefier, AMD also has the
   16-core 32-thread Ryzen 9 3950X coming in September.
                               SEP (USD)
   Cores / Threads
   TDP (Watts)
   Base / Boost Frequency (GHz)
   L3 Cache (MB)
   PCIe 4.0 Lanes
   Ryzen 9 3950X
   $749
   16 / 32
   105W
   3.5 / 4.7
   64
   24
   Ryzen 9 3900X
   $499
   12 / 24
   105W
   3.8 / 4.6
   64
   24
   Ryzen 7 3800X
   $399
   8 / 16
   105W
   3.9 / 4.5
   32
   24
   Ryzen 7 3700X
   $329
   8 / 16
   65W
   3.6 / 4.4
   32
   24
   Ryzen 5 3600X
   $249
   6 / 12
   95W
   3.8 / 4.4
   32
   24
   Ryzen 5 3600
   $199
   6 / 12
   65W
   3.6 / 4.2
   32
   24

   Aside from those halo parts, AMD also has plenty of models that address
   the bulk of casual users, gamers, and enthusiasts, like the eight-core
   16-thread Ryzen 7 3700X we also have in the lab, and a lineup of
   six-core 12-thread Ryzen 5 models.

   AMD is staying true to its enthusiast-friendly roots: Although you can
   pair the Ryzen 3000 chips with the new X570 chipset, they are also
   backward compatible with most AM4 socket motherboards. All of the
   models also come with beefy stock coolers, solder thermal interface
   material between the heat spreader and die to improve thermal transfer,
   and unlocked multipliers for easy overclocking. AMD even added support
   for auto-overclocking for mainstream processors. Pair that with the
   lower per-core pricing and the debut of the PCIe 4.0 interface for the
   desktop, and the Ryzen 3000 series appears to be a potent force.

   AMD's ability to deliver on its optimistic roadmap in the waning light
   of Moore's Law is truly impressive, especially as we have become
   accustomed to never-ending cadences of incremental updates. But at the
   end of the day it all boils down to real-world performance. Let's see
   what the Ryzen 3000 series has in store.

Ryzen 9 3900X

                         Process
   SEP / RCP (USD)
   Cores / Threads
   TDP (Watts)
   Base Frequency (GHz)
   L3 Cache (MB)
   PCIe Lanes
   Memory Support
   iGPU
   Price Per Thread
   Intel Core i9-9920X
   14nm
   $1199
   12 / 24
   165W
   3.5 / 4.4
   19.25
   16 Gen3
   Quad-Channel DDR4-2666
   No
   $49.95
   Ryzen 9 3900X
   7nm
   $499
   12 / 24
   105W
   3.8 / 4.6
   64
   24 Gen4
   Dual-Channel DDR4-3200
   No
   $20.79
   Threadripper 2920X
   12nm
   $625
   12 / 24
   180W
   3.5 / 4.3
   32
   64 Gen3
   Quad-Channel DDR4-2933
   No
   $26.04
   Core i9-9900K
   14nm
   $488
   8 / 16
   95W
   3.6 / 5.0
   16
   16 Gen3
   Dual-Channel DDR4-2666
   Yes
   $61

   Make no mistake - from a core count perspective, the $500 12-core
   24-thread Ryzen 9 3900X really has no comparison on the mainstream
   desktop. We have to reach up to Intel's high end desktop (HEDT)
   platform to find a fair comparison based on core counts. Intel's Core
   i9-9920X slots in with 12 cores and 24-threads for $1,199, a $700
   premium over AMD's Ryzen 9 3900X.

   There's no doubt the 3900X also blurs the line between the AMD's own
   HEDT Threadripper platform and the mainstream desktop: The Threadripper
   1920X is AMD's only core-comparable processor. That processors has its
   own advantages, like access to 64 lanes of PCIe 3.0, and like the
   -9920X, it supports quad-channel memory. But both company's HEDT chips
   are much more expensive than the 3900X and require pricey HEDT
   motherboards.

   Back in the familiar realm of the mainstream desktop, Intel's $488 Core
   i9-9900K serves as the 3900X's primary competitor. The -9900K comes
   with four fewer cores and eight fewer threads than the 3900X, marking a
   distinct difference in the price you pay per thread, but the -9900K
   does hold the clock speed advantage. AMD hopes to offset that advantage
   with its increased IPC throughput and the 3900X also supports the PCIe
   4.0 interface with twice the bandwidth of the -9900K's PCIe 3.0
   interface. You'll also notice the Core i9-9900K, known for its high
   power consumption and intense heat generation, has a lower 95W TDP than
   the 3900X's 105W rating. We can chalk that up to different measurement
   techniques. We'll provide extensive power and efficiency testing on the
   following pages to get a more accurate picture of actual power
   consumption.

   (*) ( ) ( )
     * 09
       09
     * 07
       07
     * 02
       02

   (*) ( ) ( )
     * 09

09
     * 07

07
     * 02

02

   As pictured here, the 39000X comes packing AMD's Zen 2
   microarchitecture spread across two small 7nm eight-core compute
   chiplets tied together with the Infinity Fabric interconnect via a
   larger 12nm I/O die (IOD). Each small 3900X compute chiplet, referred
   to as a CCD (Core Chiplet Die), comes with eight physical cores spread
   across two four-core Core Complexes (CCXes). Each CCX has 16MB of
   shared L3 cache, totaling 32MB of L3 cache per CCD, and 64MB of total
   cache for the entire chip. AMD disables two cores per CCD to create the
   12-core 3900X.

   Each 7nm CCD measures ~74mm^2 and has 3.9 billion transistors, while
   the 12nm IOD is ~125mm^2 and has 2.09 billion transistors. That means
   the 3900X comes with ~273mm^2 of silicon that sports ~9.89 billion
   transistors.

   The 3900X's larger cache comes courtesy of the denser 7nm manufacturing
   process, but it does have a slightly higher latency (on the order of
   "five or six" clocks) than the 16MB of L3 cache found on
   previous-generation models. However, the increased capacity allows the
   processor to store more data closer to the execution cores, thus
   increasing cache hit rates that ultimately yield more performance. AMD
   also decreased the size of its L1 instruction cache from 64KB with the
   first-gen Zen processors to 32KB for Zen 2 chips. This allowed the
   company to expand its microop cache, and paired with changing the L1
   instruction cache from 4-way to 8-way associativity, AMD feels this
   provides a more balanced approach to its cache subsystem.

   The -9900K's 16MB of L3 cache pales in comparison from a capacity
   standpoint, but cache bandwidth and latency are more important metrics.
   We'll put hard numbers behind the differences on the following pages.

   As a sidenote, AMD now calls its combined L2+L3 cache "GameCache" to
   highlight to casual consumers the importance of cache to gaming
   performance, but we'll stick with the established terms.

Ryzen 7 3700X

   The eight-core 16-thread Ryzen 7 3700X slots in at $329 and comes with
   a 65W TDP rating, which is significantly lower than the competing Core
   i7-9700K's 95W rating. You'll notice that AMD has maintained similar
   price points for the new models compared to the previous-gen Ryzen 7's,
   but we caution that pricing is a moving target for the last-gen chips.
                       Process
   SEP / RCP (USD)
   Cores / Threads
   TDP (Watts)
   Base Frequency (GHz)
   Total Cache (MB)
   PCIe Lanes
   iGPU
   Price Per Thread
   Core i9-9900K
   14nm
   $488
   8 / 16
   95w
   3.6 / 5.0
   16
   16 Gen3
   Yes
   $30.05
   Ryzen 7 3800X
   7nm
   $399
   8 / 16
   105W
   3.9 / 4.5
   32
   24 Gen4
   No
   $24.94
   Core i9-9700K
   14nm
   $374
   8 / 8
   95W
   3.6 / 4.9
   12
   16 Gen3
   Yes
   $46.75
   Ryzen 7 2700X
   12nm
   $329
   8 / 16
   105W
   3.7 / 4.3
   16
   20 Gen3
   No
   $20.56
   Ryzen 7 3700X
   7nm
   $329
   8 / 16
   65W
   3.6 / 4.4
   32
   24 Gen4
   No
   $20.56
   Core i7-9700
   14nm
   $323
   8 / 8
   95W
   3.6 / 4.9
   12
   16 Gen3
   Yes
   $40.38

   Although third-gen Ryzen pricing is close to the current-gen processors
   on sale, this is far lower than the per-core pricing at the launch of
   the previous gen. Normalize the numbers to price-per-thread, and its
   clear AMD maintains a pricing advantage over Intel's lineup. But
   performance varies based on architecture, so the price-to-performance
   ratio is where the rubber meets the road.

   (*) ( )
     * 10
       10
     * 01
       01

   (*) ( )
     * 10

10
     * 01

01

   The Ryzen 7 3700X features a single CCD with all eight active cores
   connected to the I/O die, highlighting that the company's Zen 2
   architecture is inherently scalable. Threadripper processors also come
   with varying numbers of compute dies, but substitute in 'dummy' dies to
   ensure structural rigidity and prevent crushing the integrated heat
   spreader (IHS) when you tighten down your cooler. The smaller surface
   area of the 2700X's IHS doesn't require a dummy die, so this pad is
   simply left unoccupied.

   AMD hasn't sampled the Ryzen 7 3800X yet, which features a higher 105W
   rating and 3.9 / 4.5 GHz base/boost clocks, which is higher than the
   Ryzen 7 3700X's 3.6 / 4.4 GHz base/boost frequency. It also looks like
   a compelling part, so look to these pages for a review soon.

   Credit: AMD Credit: AMD

   Both the Ryzen 9 3900X and the Ryzen 7 3700X come with the bundled
   Wraith Prism RGB cooler that features four direct-contact copper heat
   pipes, three independent RGB zones, switchable fan profiles, and a 39
   dB(A) noise rating. The cooler is rated to dissipate 116W of waste heat
   in "L" mode (2800 RPM) and 124W in "H" mode (3600 RPM). Cooler Master
   manufactures the heat sink/fan, while AMD provides software for
   controlling the lighting and fan profiles. Company representatives
   claim the cooler represents a roughly $43 value, and that it also
   allows for some overclocking headroom. Intel's K-series models, in
   contrast, don't come with a bundled cooler.

Memory Subsystem and Overclocking, Infinity Fabric

   Ryzen 3000 chips support dual-channel DDR4-3200, a step up from the
   previous-gen's support for DDR4-2966. That should boost performance
   significantly because the Zen 2 microarchitecture, like its
   predecessor, benefits heavily from increased memory performance
   (particularly in gaming). Credit: AMD Credit: AMD

   AMD's new Zen 2 microarchitecture uses a centralized memory controller
   on the I/O die, which helps ensure consistent memory latency in the
   multi-die Ryzen 9 models. It also improves cache access latency. AMD
   has also overhauled the Infinity Fabric, doubling its throughput by
   increasing the previous-gen 256-bit interconnect to 512-bit, which
   facilitates access to memory and enables the PCIe 4.0 interface. AMD
   also instituted more fine-grained Infinity Fabric quality of service
   controls and claims to have reduced the amount of energy required to
   transfer a bit by 27%.  Credit: AMD Credit: AMD

   AMD has improved memory overclocking substantially, partly due to
   decoupling the Infinity Fabric from the memory clock. AMD's first-gen
   Ryzen processors had plenty of difficulties with memory overclocking
   when they first launched, but AMD has addressed those concerns with the
   second-gen products and has even demoed an air-cooled Ryzen platform
   running at DDR4-5100. We also didn't encounter any issues during our
   testing.

   As with previous-gen Ryzen, memory overclocking confers big performance
   speedups for gaming. To sidestep the Infinity Fabric's maximum
   frequency of 2,000 MHz, which effectively constrains memory
   overclocking, AMD allows users to separate the memory and Infinity
   Fabric clock dependencies. The domains remain tied together at a 1:1
   ratio up to DDR4-3600, but run at a 2:1 ratio beyond that transfer
   rate. This setting, which is also user-adjustable in the BIOS, improves
   memory bandwidth but comes with a latency penalty (~9ns). Tuners can
   also adjust the Infinity Fabric clock (fclk) in 33Mhz increments to get
   an extra kicker during overclocking. AMD says that the
   price/performance sweetspot will be around DDR4-3600.

   As before, AMD supports up to 128GB of RAM and enables ECC support, but
   AMD leaves qualification and enablement of the feature up to
   motherboard vendors.
   DIMM Config
   Memory Ranks
   Official Supported Transfer Rate (MT/s)
   2 of 2
   Single
                                          DDR4-3200
                                          2 of 4
   DDR4-3200
   4 of 4
   DDR4-2933
   2 of 2
   Dual
                                          DDR4-3200
                                          2 of 4
   DDR4-3200
   4 of 4
   DDR4-2667

   As seen with the first-gen Zen chips, AMD's official supported memory
   data transfer rates vary based on the type of DIMM (single rank or dual
   rank) and the number of populated channels, as outlined above.

PCIe 4.0 Comes to the Desktop

   Ryzen 3000 processors support the PCIe interface on X570 motherboards,
   and while the chips will drop into some previous-gen AM4 motherboards,
   the processor will downshift into PCIe 3.0 on those platforms. AMD has
   also infused the new technology into its "Navi" Radeon 5000 series GPUs
   and worked with storage vendors to assure a supply of speedy new PCIe
   4.0 SSDs. We recently had the opportunity to take an early look at PCIe
   4.0 SSD performance, which you can see here.
               Bandwidth
   Gigatransfer
   Frequency
   Encoding
   PCIe 3.0
   32 GB/s
   8 GT/s
   8.0 GHz
   128b/130b
   PCIe 4.0
   128 GB/s
   16 GT/s
   32.0 GHz
   128b/130b

   PCIe 4.0 provides yet another advantage for performance seekers,
   particularly in the content creation realm, over Intel's platform, but
   it doesn't materially impact gaming performance (at least not yet). The
   new interface also comes at the cost of higher pricing for
   X570-equipped motherboards due to tighter signalling requirements.
   Those prices could recede over time as the pricing of the PCIe 4.0
   component ecosystem, like switches and redrivers, benefit from
   economies of scale, but AMD has wisely encouraged its partners to
   continue offering the current-gen X470 motherboards that will now serve
   as a lower tier of motherboards.

   AMD's new Ryzen 3000 series lineup is fully compatible with existing
   X470 motherboards and will operate at their full performance on the
   previous-gen boards, albeit at the loss of PCIe 4.0 connectivity. That
   shouldn't be too much of a concern for users without PCIe 4.0 devices
   or SSD RAID storage arrays that hang off the chipset. Fast storage
   arrays will certainly benefit from the faster PCIe 4.0 connection
   between the chipset and processor, though.

Ryzen-Specific Windows 10 Scheduler Updates

   AMD worked with Microsoft to deliver on a much needed feature: A
   Ryzen-aware scheduler. The new scheduler arrived with the Windows 10
   May update and benefits both current-gen and previous-gen Ryzen models
   (Threadripper and Ryzen 3000 processors).

   (*) ( )
     * David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-009
       David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-009
     * David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-008
       David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-008

   (*) ( )
     * David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-009

David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-009
     * David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-008

David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-008

   The new scheduler pins threads within a single CCX (the four-core
   clusters inside each CCD) before scheduling threads to other CCXes.
   This approach reduces latency during thread synchronizations or
   frequent cache accesses, thus improving performance for all existing
   Ryzen processors. AMD says the feature doesn't benefit all
   applications, but can result in significant performance improvements in
   those that do.

   AMD also introduced its Collaborative Power Performance Control 2
   (CPPC2) feature, which is a software feature that manipulates Ryzen
   3000's power states from within the operating system. This is similar
   to Intel's Speed Shift technology and reduces power state transition
   latency from 30ns to 1ns, which ultimately saves power and boosts
   efficiency. The feature comes enabled in the latest AMD chipset drivers
   and the Windows 10 May update (and newer).

   As before, these mainstream models don't come with integrated graphics,
   meaning you'll need a discrete GPU.

   MORE: Best CPUs

   MORE: Intel & AMD Processor Hierarchy

   MORE: All CPUs Content

   IFRAME: https://content.jwplatform.com/players/zYBgfFoA.html

   Next

   Summary
    1. Into the 7nm Era
    2. 7nm Process, Zen 2, and the X570 Chipset
    3. Ryzen 3000 IPC Measurements and Power Consumption
    4. Overclocking and Ryzen Master
    5. Updated Test Metholodgy and Setup
    6. VRmark, 3DMark and AotS: Escalation
    7. Civilization VI Graphics and AI, Dawn of War III
    8. Far Cry 5 and Final Fantasy XV
    9. GTA: V and Hitman 2
   10. Project Cars 2, The Division 2, and World of Tanks enCore
   11. Office, Web Browser, and Productivity
   12. Rendering, Encoding, Compression, Encryption
   13. Conclusion

   About the author
   Paul Alcorn @PaulyAlcorn

   Paul Alcorn is a Senior Editor for Tom's Hardware US. He writes news
   and reviews on CPUs, storage and enterprise hardware.
   [javascript]
   Read more
     * CPUs
     * AMD
     * Components

   180 comments
   Comment from the forums
       Your comment
     *
   Isokolon [X]
       [ ]
       too bad there wasn't a 3800X included, would be interesting to see
       if the price tag for the 3800X over the 3700X is indeed worth it
     *
   feelinfroggy777 [X]
       [ ]
       Nice for worksation task, but disappointing it basically ties Intel
       in gaming if not just a tad behind.
     *
   salgado18 [X]
       [ ]

     Quote:

     Nice for worksation task, but disappointing it basically ties Intel
     in gaming if not just a tad behind.
       It tied to Intel for less money, with a great bundled cooler, and a
       cheaper platform (edit: and less power too). Also, unless you use a
       144Hz monitor, the diference is purely synthetic. Did you expect it
       to be way faster than a 5 GHz Intel magically?
     *
   Phaaze88 [X]
       [ ]

     Quote:

     Thank you so much and to be honest the only reason why I bought the
     750tx Corsair psu is because it was 30$, can I ask you if the gtx770
     2gb will run battlefield 1 on high?

     Quote:

     too bad there wasn't a 3800X included, would be interesting to see
     if the price tag for the 3800X over the 3700X is indeed worth it
       I can't imagine it would be.
       If looking at the 3800x as a binned 3700x - that's basically what
       it would be - grab one if it goes on sale closer to the 3700x's
       price.
       These chips don't overclock any better than their predecessors,
       which wasn't good to begin with, so whatever extra clocks you get
       with a 3800x will hardly be noticeable and won't be worth a $50+
       price increase over 3700x.
     *
   velocityg4 [X]
       [ ]
       The overclocking results were disappointing. 4.1Ghz max on all
       cores. Given that the 3950x does a 4.7Ghz single core turbo boost
       and the 3900x does 4.6Hz single core turbo boost. I'd have assumed
       any of the Ryzen 3000 would OC to 4.6/4.7Ghz on all cores with
       decent air/water cooling.
       Power consumption: AIDA 64 seems to punish AMD a lot more than
       Intel. When you were using Prime95 Intel was punished a lot more.
       It seems the switch from Prime95 to AIDA 64 gives Intel an unfair
       advantage in the stress test power consumption test. While Prime95
       gave AMD an unfair advantage. I'd suggest using both in reviews or
       find another torture test that will fully punish both AMD and Intel
       for a max load test. With such wild variation. I can't see how
       either is an accurate measure of a CPU under full load.
       Example Review:
       https://www.tomshardware.com/reviews/intel-core-i9-9900k-9th-gen-cp
       u,5847-11.html
       The Intel i9-9900K hit 204.6W in your old reviews stress test. This
       time it is only 113W.
       The AMD Ryzen 2700x hit 104.7W in your old review. Now it is 133W.
     *
   feelinfroggy777 [X]
       [ ]

     Quote:

     It tied to Intel for less money, with a great bundled cooler, and a
     cheaper platform (edit: and less power too). Also, unless you use a
     144Hz monitor, the diference is purely synthetic. Did you expect it
     to be way faster than a 5 GHz Intel magically?
       It did tie Intel in gaming. It tied basically the same Intel CPUs
       that have been on the market since 2015 with Skylake. We are in the
       back half of 2019 and we see the same gaming performance that we
       had in 2015 mainstream CPUs.
       We know AMD is cheaper and comparing clockspeeds against different
       architectures between Intel and AMD is silly. But it would be nice
       to see some tangible improvement regarding fps with CPUs. The GPU
       still remains king when it comes to a quality gaming build.
     *
   delaro [X]
       [ ]
       I've seen reviews from 5 different sites and the conclusions bounce
       all over the place, which makes me think there is much to do on the
       software optimization side. :unsure: I was expecting gaming FPS to
       not change all that much with many of the titles being tested have
       partnered or optimized around Intel.
     *
   jimmysmitty [X]
       [ ]

     Quote:

     Nice for worksation task, but disappointing it basically ties Intel
     in gaming if not just a tad behind.
       Minus the ability to overclock yes tied. Most people who buy the
       9900K will not be buying it to leave it stock.

     Quote:

     The overclocking results were disappointing. 4.1Ghz max on all
     cores. Given that the 3950x does a 4.7Ghz single core turbo boost
     and the 3900x does 4.6Hz single core turbo boost. I'd have assumed
     any of the Ryzen 3000 would OC to 4.6/4.7Ghz on all cores with
     decent air/water cooling. Power consumption: AIDA 64 seems to punish
     AMD a lot more than Intel. When you were using Prime95 Intel was
     punished a lot more. It seems the switch from Prime95 to AIDA 64
     gives Intel an unfair advantage in the stress test power consumption
     test. While Prime95 gave AMD an unfair advantage. I'd suggest using
     both in reviews or find another torture test that will fully punish
     both AMD and Intel for a max load test. With such wild variation. I
     can't see how either is an accurate measure of a CPU under full
     load. Example Review:
     https://www.tomshardware.com/reviews/intel-core-i9-9900k-9th-gen-cpu
     ,5847-11.html The Intel i9-9900K hit 204.6W in your old reviews
     stress test. This time it is only 113W. The AMD Ryzen 2700x hit
     104.7W in your old review. Now it is 133W.
       Its what I wanted to know. Ryzen has always been pushed to the
       limit in terms of clock speed and Zen 2 is no different it seems.
       Little to no headroom. AnandTech was able to get it to 4.3GHz all
       core but with manual OCing it seems to disable boost clocking which
       in turn cuts 300MHz from single core performance.
       As for the power consumption, the differences are probably what
       they prioritize. I know Prime 95 heavily uses AVX which is a power
       hog. Not as sure on AIDA 64 since I never used it. I always use
       Prime 95 and IBT for stability.

     Quote:

     It did tie Intel in gaming. It tied basically the same Intel CPUs
     that have been on the market since 2015 with Skylake. We are in the
     back half of 2019 and we see the same gaming performance that we had
     in 2015 mainstream CPUs. We know AMD is cheaper and comparing
     clockspeeds against different architectures between Intel and AMD is
     silly. But it would be nice to see some tangible improvement
     regarding fps with CPUs. The GPU still remains king when it comes to
     a quality gaming build.
       Its not silly to compare clock speeds as those can be advantages.
       Intel still clearly has a clock speed advantage and that advantage
       will keep them priced higher. We might see some drops but I doubt
       we will see enough to make it feel like Athlon 64 again.
       As much crap as people give Intel for getting stuck at 14nm I have
       to give them props for having a 5 year old process tech beat modern
       process tech, especially one that's supposed to be "half" the size.
       I know its not quite as most 7nms out there are still less dense
       than Intels initial 10nm plans but still it goes to show that the
       nm part has become pointless and a marketing gimmick more than
       anything.
       The only thing a CPU matters gaming wise is how long it will last
       before it will bottleneck the GPU. While its still early the clock
       speed and overclocking advantage Intel has might make their CPUs
       last longer in gaming than Zen 2. Only time will tell but maybe AMD
       will get a better process tech in a few years and finally compete
       like the old days.
     *
   martinch [X]
       [ ]

     Quote:

     Its not silly to compare clock speeds [between AMD and Intel] as
     those can be advantages.
       Unless you're trying to give an indication of "performance-per-MHz"
       of varying architectures, yes, comparing clock speeds between
       differing architectures is a fundamentally invalid comparison (it's
       also not exactly an accurate predictor of per-core performance).
     *
   feelinfroggy777 [X]
       [ ]

     Quote:

     Its not silly to compare clock speeds as those can be advantages.
     Intel still clearly has a clock speed advantage and that advantage
     will keep them priced higher. We might see some drops but I doubt we
     will see enough to make it feel like Athlon 64 again.
       Clockspeeds between AMD and Intel are not apples to apples.
       Bulldozer hit 5ghz and it was a terrible CPU. Just because it could
       hit 5ghz, did not make a good chip.
     *
   InvalidError [X]
       [ ]

     Quote:

     Unless you're trying to give an indication of "performance-per-MHz"
     of varying architectures, yes, comparing clock speeds between
     differing architectures is a fundamentally invalid comparison (it's
     also not exactly an accurate predictor of per-core performance).
       Even real-world aren't reliable indicators unless you can find
       benchmarks of the specific software and task within that software
       you are interested in. Anything else is a general indicator at
       best.
       So, I predicted all-core overclocks on par with XFR boost at best
       and it seems Tom's review samples get nowhere near that on
       practical cooling. Looks like AMD is doing a pretty good job of
       leaving little to nothing for practical overclockers to do with the
       X-chips already squeezing everything they are worth out of
       themselves with little more than a few checkboxes.
     *
   feelinfroggy777 [X]
       [ ]

     Quote:

     Even real-world aren't reliable indicators unless you can find
     benchmarks of the specific software and task within that software
     you are interested in. Anything else is a general indicator at best.
     So, I predicted all-core overclocks on par with XFR boost at best
     and it seems Tom's review samples get nowhere near that on practical
     cooling. Looks like AMD is doing a pretty good job of leaving little
     to nothing for practical overclockers to do with the X-chips already
     squeezing everything they are worth out of themselves with little
     more than a few checkboxes.
       They pretty much overclock themselves these days.
     *
   yeti_yeti [X]
       [ ]
       I think this Ryzen lineup is great and delivers awesome
       performance, especially in professional applications. Some people
       seem to be disappointed, that Ryzen 7/9 didn't beat their Intel
       counterparts in gaming, which was something a lot of
       people(myself included) didn't think was going to happen anyway.
       However, I do think that AMD could have been a bit more transparent
       and modest, when showcasing their own benchmarks of their products
       beating or matching the competing Intel chips, which would result
       in less people being let down.
       Other than that, I think AMD did great in both GPU and CPU
       department and look forward to more products from them.
     *
   kinggremlin [X]
       [ ]
       Good effort by AMD, but still doesn't move the performance bar any
       for home users. For anyone who has been waiting 4 years to get a
       5960x for less than $500, you've now got your CPU. For anyone
       looking for something tangibly faster than Intel's Haswell
       generation CPU's, you're still waiting.
     *
   lxtbell2 [X]
       [ ]

     Quote:

     Good effort by AMD, but still doesn't move the performance bar any
     for home users. For anyone who has been waiting 4 years to get a
     5960x for less than $500, you've now got your CPU. For anyone
     looking for something tangibly faster than Intel's Haswell
     generation CPU's, you're still waiting.
       Zen 2 seems to have some 30% better power efficiency than Intel
       processors, and that's huge. Power efficiency is everything for
       SFF, laptops, HTPC, fanless builds etc etc. So I would say it can
       potentially move the performance bar a lot for those.
       Regarding absolute performance, the 12-core destroys all Haswell
       desktop CPUs in productivity, and I can't care less about "tangibly
       faster" in gaming as long as my graphics card is the bottleneck in
       1440p+.
     *
   InvalidError [X]
       [ ]

     Quote:

     Zen 2 seems to have some 30% better power efficiency than Intel
     processors, and that's huge. Power efficiency is everything for SFF,
     laptops, HTPC, fanless builds etc etc. So I would say it can
     potentially move the performance bar a lot for those.
       For the lower-power segments, Intel has Icelake which is more than
       a match for Zen 2 CPU-wise.
     *
   salgado18 [X]
       [ ]
       About bad overclock numbers, my 2 cents:
       1. Nobody had the time to properly test them. I believe Tom's had
       like two or so days to run dozens of tests on two brand new
       processors, and still produce the article. Also, these are early
       BIOS and drivers. It could improve with time.
       2. The 3700x reaches a max of 4.3, but the 3950x reaches 4.6. Maybe
       it's inconsistent because of such a new process, after all, these
       are the first 7nm cpus produced, right? And lots of changes, some
       sillicon revisions could help a lot.
     *
   kinggremlin [X]
       [ ]

     Quote:

     Regarding absolute performance, the 12-core destroys all Haswell
     desktop CPUs in productivity, and I can't care less about "tangibly
     faster" in gaming as long as my graphics card is the bottleneck in
     1440p+.
       A 12 core CPU will not be any faster than an 8 core for home users.
       The software used does not scale with that many cores. If it was so
       easy to make typical software scale with more cores, programmers
       would have already done it. Most mainstream software is not capable
       of being highly parallelized like a graphics workload, so this
       problem isn't going to get fixed without a completely different
       type of computing platform. Adding more cores doesn't make a PC
       faster when what you really need is faster cores.
     *
   kinggremlin [X]
       [ ]

     Quote:

     About bad overclock numbers, my 2 cents: 1. Nobody had the time to
     properly test them. I believe Tom's had like two or so days to run
     dozens of tests on two brand new processors, and still produce the
     article. Also, these are early BIOS and drivers. It could improve
     with time. 2. The 3700x reaches a max of 4.3, but the 3950x reaches
     4.6. Maybe it's inconsistent because of such a new process, after
     all, these are the first 7nm cpus produced, right? And lots of
     changes, some sillicon revisions could help a lot.
       May want to watch this link from world reknown overclocker de8auer:
       [MEDIA=youtube]WXbCdGENp5I[/MEDIA]
       View: https://www.youtube.com/watch?v=WXbCdGENp5I
       He has 10 CPU's (mix of 6/8/12 cores). Forget overclocking, he
       couldn't get any of his 8 or 12 core CPU's to hit AMD's advertised
       max clocks (4.5 for 3800x, 4.6 for 3900x) even with a custom water
       cooling loop. There were leaks before launch that these chips would
       hit 5GHz. De8auer says in this video to forget that, he has chips
       that wouldn't hit 5GHz using liquid nitrogen.
     *
   ingtar33 [X]
       [ ]

     Quote:

     I've seen reviews from 5 different sites and the conclusions bounce
     all over the place, which makes me think there is much to do on the
     software optimization side. :unsure: I was expecting gaming FPS to
     not change all that much with many of the titles being tested have
     partnered or optimized around Intel.
       Linus had a possible cause for this. He noted the same thing (with
       results all over the place) and realized it's a result of the
       windows scheduler bouncing heavy tasks from core to core without
       consideration for boost clocks. when they locked the application to
       the cores that were boosting the results for the AMD cpu shoot went
       up and became much more consistent. He believes once the scheduling
       issue in windows is worked out the Ryzen bench results will
       probably increase noticeably.
     *
   Ncogneto [X]
       [ ]

     Quote:

     Good effort by AMD, but still doesn't move the performance bar any
     for home users. For anyone who has been waiting 4 years to get a
     5960x for less than $500, you've now got your CPU. For anyone
     looking for something tangibly faster than Intel's Haswell
     generation CPU's, you're still waiting.
       One of these days you might actually understand what matters, and
       what doesn't. AMD delivered on performance that matters, not some
       silly gaming benchmark(s) that is Intel's last gasp as it hangs on
       by it's fingernails. Performance that you can only see if you
       couple either platform with a $1200 GPU no less, and even then, you
       can't actually notice a difference while playing your game (155fps
       vs 135 FPS, who gives a rats a**). Show me a game in which the
       Intel CPU is performing at a level that is visually noticeably
       better (without looking at some silly fps counter).
       On every other front, the Intel CPU gets stomped in the dirt, all
       while costing more and consuming more power, and needing an exotic
       cooling solution as well.
       Easy win for AMD, for anyone other than those only interested in
       nothing but pure fps bragging rights, which is so High School level
       silly.
       Spectre/meltdown/zombieload......disable HT, etc etc etc. Every
       month brings another Intel bug.
       They should be ashamed.
     *
   fr3sgnint [X]
       [ ]
       Shame we didn't see any CAD / Solidworks ETC type benchmarks on the
       productivity side.
     *
   acquinn [X]
       [ ]
       So impressive. AMD hasn't been on my radar since the Athlon days.
       Competition is good! I wonder how long it'll take Intel to get to
       7NM. Also, where's Intel's response hardware-wise? I mean isn't the
       9900K almost a year old at this point? And it's still beating AMD
       in a lot of areas.
     *
   kinggremlin [X]
       [ ]

     Quote:

     Shame we didn't see any CAD / Solidworks ETC type benchmarks on the
     productivity side.
       You don't have to. Both are single threaded applications with
       random addons that may support multithreading, so you know Intel is
       going to win, just like most stuff from Adobe. People like Ncogneto
       don't seem to grasp how much commonly used software doesn't benefit
       at all from increased core counts beyond a few.

   Display more comments

   Most Popular
    1. India's First CPUs Are Ready for App Development
    2. PCPartPicker Reveals AMD Ryzen 3000 CPU Packaging
    3. Silicon Lottery to Bin and Sell Ryzen 3000 CPUs

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate next

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * Graphics

     Review

  AMD Radeon RX 5700 XT and Radeon RX 5700 Review: New Prices Keep Navi In The
                                      Game

   by Chris Angelini July 7, 2019 at 7:00 AM

     *
     *
     *
     *
     *
     *

   22 Comments

   [ ] (*) ( ) ( ) ( ) ( ) ( ) ( )
   Page 1:AMD Radeon RX 5700 XT and Radeon RX 5700 Review
   Page 2:Performance Results: 2560 x 1440
   Page 3:Performance Results: 3840 x 2160
   Page 4:Power Consumption: Radeon RX 5700
   Page 5:Power Consumption: Radeon RX 5700 XT
   Page 6:Fan Speeds, Clock Rates, and Temperatures
   Page 7:Conclusion
     *

Page 1:AMD Radeon RX 5700 XT and Radeon RX 5700 Review
     *

Page 2:Performance Results: 2560 x 1440
     *

Page 3:Performance Results: 3840 x 2160
     *

Page 4:Power Consumption: Radeon RX 5700
     *

Page 5:Power Consumption: Radeon RX 5700 XT
     *

Page 6:Fan Speeds, Clock Rates, and Temperatures
     *

Page 7:Conclusion

   AMD’s propensity for slowly dribbling out information about upcoming
   products keeps our news desk buzzing but also serves to illustrate the
   company’s play book months in advance. That drawn-out tease definitely
   worked against AMD last generation. By the time Radeon RX Vega landed
   in our lab, expectations had boiled over beyond what the card could
   deliver, particularly at its cryptocurrency-affected prices.

   This time around, AMD used the Electronic Entertainment Expo in Los
   Angeles as the launch pad for a Navi deets, pouring out all the
   architectural details it was willing to divulge with less than 24 hours
   to write it up, then complicating matters by prohibiting audio or video
   recordings of the technical deep dives. But even rushing the
   particulars for reasons unknown couldn’t stop Nvidia from squeezing in
   a Turing refresh prior to Radeon RX 5700 and Radeon RX 5700 XT
   availability.

   The two Navi-based cards originally took aim at GeForce RTX 2070 and
   GeForce RTX 2060. Just days before reviews were scheduled to go live,
   however, AMD found itself staring down the barrel of GeForce RTX 2060,
   2060 Super, and 2070 Super. Its hardware was already baked, so the
   company turned another dial to stay competitive: it dropped the price
   of Radeon RX 5700 XT to $400, matching GeForce RTX 2060 Super, and
   lowered Radeon RX 5700 to $350, pulling up alongside GeForce RTX 2060.
   AMD is clearly feeling good enough about its performance story to go up
   against the GeForces at identical pricing. Is that confidence
   well-placed or is AMD underestimating the appeal of real-time ray
   tracing support?

   (*) ( ) [ ] [ ]
     *
   AMD Radeon RX 5700 XT
       [javascript]
       3.5/5
       Review
       $400Amazon
     *
   AMD Radeon RX 5700
       [javascript]
       4/5
       Review
       $350Amazon

     * AMD Radeon RX 5700 XT AMD Radeon RX 5700

A Navi Recap

   Check out AMD Announces Radeon RX 5700 XT and RX 5700: Navi Takes the
   Fight to GeForce RTX for a quick recap of Navi’s architectural basics,
   including its redesigned Compute Units and cache hierarchy changes.

Meet Radeon RX 5700 XT

   Both AMD Radeon RX 5700-series cards are based on the same Navi GPU.
   Manufactured on TSMC’s 7nm FinFET process and composed of 10.3 billion
   transistors, these chips occupy a scant 251 mm². Vega was much larger.
   Manufactured on GlobalFoundries’ 14nm LPP process, it packed 12.5
   billion transistors into a 495 mm² die. For some additional context,
   Nvidia’s competing GeForce RTX 2060-series cards employ TU106, a
   10.8-billion-transistor chip measuring 445 mm² and built using TSMC’s
   12nm FinFET process.

   We confirmed with AMD that Radeon RX 5700 XT employs a fully-enabled
   version of the Navi GPU—no part of the chip is turned off to improve
   yields or leave room for a more resource-rich model in the future. It
   exposes 40 RDNA Compute Units, each with 64 Stream processors, totaling
   2,560 ALUs across the processor. The CUs host four texture units, just
   as they did in AMD’s Graphics Core Next design, adding up to 160 in a
   complete Navi GPU. Four render back-ends per quadrant are capable of 16
   pixels per clock cycle, yielding 64 ROPs.

   That’s clearly a more compact configuration than Radeon RX Vega 64,
   which featured 64 CUs with 4,096 Stream processors and 256 texture
   units. And yet our benchmarks will show that Radeon RX 5700 XT averages
   15%-higher frame rates than Vega 64. Almost 60% of the architecture’s
   speed-up comes from performance per clock enhancements, according to
   AMD. Another 25% is attributable to gains enabled by 7nm manufacturing.
   The reminder falls under design frequency and power improvement, which
   includes more effective clock gating.

   The specifications for Radeon RX 5700 XT curiously define its base
   clock rate as “up to 1,605 MHz.” At first, we didn’t think anything of
   this. After subjecting the 5700 XT to synthetic workloads like FurMark,
   however, and observing frequencies as low as 1,575 MHz, it appears that
   the base can be violated under the right (or wrong) conditions,
   favoring a consistent acoustic experience over strict performance
   boundaries. AMD also specifies a Game GPU clock of “up to 1,755 MHz”
   and a Boost GPU clock of “up to 1,905 MHz.” As you might guess, both
   ratings are subject to certain conditions. In fact, we saw Boost
   frequencies well above 1,905 MHz at the start of many games. As the
   card warms up, though, expect to see clock rates closer to the Game GPU
   clock.

   Let’s just throw this out there: We’d prefer that AMD not create a
   third frequency rating. Because it is fleeting, it’s subject to abuse.
   In fact, AMD is already using that peak figure to calculate its 9.75
   TFLOPS FP32 performance figure. The more sustainable 1,755 MHz Game GPU
   clock translates to 9 TFLOPS, and that just doesn’t look as thunderous
   next to GeForce RTX 2060 Super’s 7.2 TFLOPS, right? Navi does carry
   over support for rapid-packed math, so AMD cites half-precision
   performance of up to 19.5 TFLOPS.

   An aggregate 256-bit pathway is populated by 8GB of GDDR6 operating at
   14 Gb/s. This gives Radeon RX 5700 XT up to 448 GBps of memory
   bandwidth—slightly less than Radeon RX Vega 64’s 484 GBps but
   significantly more than Radeon RX 590’s 256 GBps. AMD claims other
   notable improvements throughout Navi’s memory hierarchy, from reduced
   congestion in its 4MB L2 cache to a new 128KB L1 cache per quadrant
   that helps reduce latency.

   AMD considers PCIe 4.0 support one of its most noteworthy competitive
   advantages. The Radeon RX 5700 XT and standard 5700 both enable the
   latest standard’s 16 GT/s transfer rate on compatible platforms.
   However, we don’t have the requisite hardware to test for PCIe 4.0’s
   theoretical 32 GBps of throughput (two of our other labs received the
   X570-based setups for Ryzen testing). If AMD is going to say the time
   isn’t right for hardware-accelerated ray tracing, then surely the need
   for more bus bandwidth falls ever further down the priority list for
   gamers.

   Although Radeon RX 5700 XT hosts a much more sophisticated GPU than
   Radeon RX 590 and is indeed faster than Radeon RX Vega 64, its total
   board power rating is 225W. That’s the same TBP as RX 590. As we’ll see
   in our power analysis, the 5700 XT dutifully obeys this ceiling, too.

   AMD covers Navi 10 with a much more artistic shroud than we’ve seen the
   company use previously. Representatives claim the contour design helps
   optimize airflow and minimize acoustic output, and we can attest that
   the 5700 XT is one of the quietest AMD reference cards we've tested. An
   aluminum alloy shell wraps around this card’s top, front, and bottom,
   enveloping the entire cooler. One end is open, facilitating ambient air
   intake, yet is still decorated with red pinstripes, Radeon branding,
   and black-painted aluminum fins.

   The other end is loaded with slats for ventilation. Three DisplayPort
   1.4 connectors and one HDMI 2.0b interface run along the PCB’s edge.
   It’s worth noting that Navi is AMD’s first GPU with Display Stream
   Compression technology, supporting 4K monitors at 144 Hz through a
   single cable without resorting to chroma subsampling.

   Up top, eight- and six-pin auxiliary connectors feed the 5700 XT’s
   seven-phase power system. Another pair of pin stripes add a sporty
   accent, while that Radeon logo lights up red.

   Around back, an aluminum plate covers most of the PCA, protecting it
   from accidental drops.

   Ambient air is pulled in to the 70mm centrifugal fan and blown through
   an array of aluminum fins sitting on top of a vapor chamber cooler. The
   heated air is exhausted out the back of your chassis rather than
   recirculated. Of course, the downside of this design is more noise than
   many axial fan-based solutions and less airflow, resulting in higher
   GPU temperatures.

   AMD’s reference 5700 XT is quite a bit longer than Nvidia’s GeForce RTX
   2060 Super: it measures 10.75 inches from the expansion bracket to back
   edge. The Nvidia card is roughly 9 inches long in comparison. They’re
   close to the same height though, and the two cards similarly fit into a
   dual-slot form factor. It comes as no surprise that AMD’s vapor chamber
   adds notable heft. Whereas the reference GeForce RTX 2060 Super
   registers 2lb 2.2oz on our scale, Radeon RX 5700 XT weighs in at 2lb
   7.2oz.

Meet Radeon RX 5700

   Radeon RX 5700 is a close relative of the higher-end model. It’s based
   on the same graphics processor, sits on the same circuit board, and
   utilizes a similar thermal solution. The 5700’s appearance just isn’t
   as fancy. An aluminum shroud does wrap around the cooler, including its
   back edge. But instead of LED lighting, racing stripes, or textured
   ridges, it’s a plain shade of flat grey with a couple of red Radeon
   decals. That's fine by us; the clean colors and lines look good.

   A 185W board power rating justifies eight- and six-pin auxiliary
   connectors up top, along with the same 70mm blower-style fan and vapor
   chamber cooler found on Radeon RX 5700 XT. You don’t get a backplate
   this time around. Nevertheless, Radeon RX 5700 still weighs more than
   GeForce RTX 2060 Super. At 2lb 3.2oz, it carries an extra ounce around
   its hips.

   AMD brings similar display connectivity here, so you get three
   DisplayPort connectors and one HDMI port.

   Under the hood, Navi is trimmed down slightly. Thirty-six of the chip’s
   40 Compute Units remain active, cutting its Stream processor and
   texture unit count to 2,304 and 144, respectively. AMD also detunes
   Navi’s clock rates. The base frequency is “up to 1,465 MHz,” the Game
   clock is “up to 1,625 MHz,” and the so-called Boost rating is “up to
   1,725 MHz.” AMD uses those figures to claim a peak FP32 rate of 7.95
   TFLOPS, though a more practical specification would land between the
   Boost and Game frequencies based on our measurements.

   Aside from the four missing CUs, Navi remains otherwise intact for
   Radeon RX 5700. That means all its caches remain active, along with the
   256-bit memory bus hosting 8GB of 14 Gb/s GDDR6.

   Priced at $350, Radeon RX 5700 finds itself going up against GeForce
   RTX 2060.
                             Radeon RX 5700 XT
     GeForce RTX 2060 Super
         Radeon RX 5700
      GeForce RTX 2060 FE
       Architecture (GPU)
   RDNA (Navi 10)
   Turing (TU106)            RDNA (Navi 10)
   Turing (TU106)
              ALUs
   2560
   2176
   2304
   1920
   Peak FP32 Compute
   (Based on Typical Boost)
   9 TFLOPS
   7.2 TFLOPS
   7.5 TFLOPS
   6.45 TFLOPS
          Tensor Cores
   N/A
   272
   N/A
   240
            RT Cores
   N/A
   34
   N/A
   30
         Texture Units
   160
   136
   144
   120
        Base Clock Rate
   1605 MHz
   1470 MHz
   1465 MHz
   1365 MHz
   Nvidia Boost/AMD Game Rate
   1755 MHz
   1650 MHz
   1625 MHz
   1680 MHz
         AMD Boost Rate
   1905 MHz
   N/A
   1725 MHz
   N/A
        Memory Capacity
   8GB GDDR6                 8GB GDDR6
   8GB GDDR6                 6GB GDDR6
           Memory Bus
   256-bit
   256-bit
   256-bit
   192-bit
        Memory Bandwidth
   448 GB/s                  448 GB/s
   448 GB/s                  336 GB/s
              ROPs
   64
   64
   64
   48
            L2 Cache
   4MB                       4MB              4MB     3MB
              TDP
   225W
   175W
   185W
   160W
        Transistor Count
   10.3 billion              10.8 billion
   10.3 billion              10.8 billion
            Die Size
   251 mm²                   445 mm²          251 mm² 445 mm²

How We Tested Radeon RX 5700 XT and Radeon RX 5700

   AMD shipped us its Radeon RX 5700 and 5700 XT cards with plenty of time
   to test. But Nvidia cut in with the GeForce RTX 2060 Super and 2070
   Super, condensing our schedule considerably. Regardless, we still
   managed to test all four cards on a brand-new platform powered by
   Intel’s Core i7-8086K six-core CPU on a Z370 Aorus Ultra Gaming
   motherboard with 64GB of a Corsair CMK128GX4M8A2400OC14 kit. We’re
   still using a couple of 500GB Crucial MX200 SSDs for our gaming suite,
   along with Noctua’s NH-D15S heat sink/fan combo.

   Of course, this required building a new library of data with a limited
   amount of time to do it. We started with a selection of cards relevant
   to the new GeForces, and then added AMD’s Radeon RX 5700-series boards.
   From Nvidia, that includes GeForce RTX 2080, GeForce RTX 2070, GeForce
   RTX 2060, GeForce GTX 1080 Ti, GeForce GTX 1080, GeForce GTX 1070 Ti,
   and GeForce GTX 1070. All of those cards are represented by Nvidia’s
   own Founders Edition models except for the 1070 Ti, which is an MSI
   GeForce GTX 1070 Ti Gaming 8G. AMD’s own Radeon VII is part of the
   comparison as well, along with Sapphire’s Nitro+ Radeon RX Vega
   64 and Nitro+ Radeon RX Vega 56. Those partner cards ensure we don’t
   see the frequency/throttling issues encountered with our reference
   models.

   Loading...

   Our benchmark selection includes Battlefield V, Destiny 2, Far Cry 5,
   Final Fantasy XV, Forza Horizon 4, Grand Theft Auto V, Metro
   Exodus, Shadow of the Tomb Raider, Strange Brigade, Tom Clancy’s The
   Division 2, Tom Clancy’s Ghost Recon Wildlands, The Witcher
   3 and Wolfenstein II: The New Colossus.

   The testing methodology we're using comes from PresentMon: Performance
   In DirectX, OpenGL, And Vulkan. In short, these games are evaluated
   using a combination of OCAT and our own in-house GUI for PresentMon,
   with logging via GPU-Z.

   We’re using driver build 431.16 for Nvidia’s GeForce RTX 2060 and 2070
   Super and build 430.86 for all the other Nvidia cards. On AMD’s side,
   we’re using Adrenalin 2019 Edition 19.6.3 for all three existing cards,
   plus 19.7.1 for the Radeon RX 5700-series cards.

   MORE: Best Graphics Cards

   MORE: Desktop GPU Performance Hierarchy Table

   MORE: All Graphics Content

   IFRAME: https://content.jwplatform.com/players/SzkW6ASo.html

   Next

   Summary
    1. AMD Radeon RX 5700 XT and Radeon RX 5700 Review
    2. Performance Results: 2560 x 1440
    3. Performance Results: 3840 x 2160
    4. Power Consumption: Radeon RX 5700
    5. Power Consumption: Radeon RX 5700 XT
    6. Fan Speeds, Clock Rates, and Temperatures
    7. Conclusion

   About the author
   Chris Angelini @chris_angelini

   Chris Angelini is an Editor Emeritus at Tom's Hardware US. He edits
   hardware reviews and covers high-profile CPU and GPU launches.
   [javascript]
   Read more
     * Graphics
     * AMD
     * Components

   22 comments
   Comment from the forums
       Your comment
     *
   ICWiener [X]
       [ ]
       Honestly I'd pay the extra just for Nvidia's decent cooler, let
       alone power consumption, RTX, etc. Hard pass on another blower.
     *
   justin.m.beauvais [X]
       [ ]
       Dear AMD,
       Why? Wasn't your line GTX 1080 performance at RX 580 prices? points
       at Navi This is not that.
       Regrettably yours,
       Your Fans
     *
   alextheblue [X]
       [ ]

     Quote:

     Honestly I'd pay the extra just for Nvidia's decent cooler, let
     alone power consumption, RTX, etc. Hard pass on another blower.
       I read the review, don't know what you mean by "let alone power
       consumption". Efficiency is virtually the same. I personally prefer
       blowers as long as they're not crazy loud, and the review says
       they're not so I'm all for it. If you DON'T like blowers, I'm sure
       there will be third-party coolers that improve cooling performance
       and acoustics further (and dump heat into the chassis like crazy).

     Quote:

     Dear AMD, Why? Wasn't your line GTX 1080 performance at RX 580
     prices? points at Navi This is not that. Regrettably yours, Your
     Fans
       Who ever promised that? They are 10-11% faster than same-priced
       Nvidia models. They're not going to drop 5700 $100 bucks when
       they're already ahead by 11%, even though I would love lower
       prices, there's no incentive for them to do so.
     *
   face-plants [X]
       [ ]
       I'm pleasantly surprised by both the performance AND the
       last-minute drop in price. Personally I don't like blower style
       cards and will be on the lookout for third-party offerings with
       more traditional dual fan coolers. This is totally personal
       preference as I tend to build in bigger cases with plenty of
       ventilation to deal with the extra heat. If the prices don't get
       too out of line from the AIBs then I'll gladly give a few of these
       cards a go in upcoming builds. I'm also expecting a decent
       improvement in thermals so hopefully these FE cards aren't
       exemplary of the best cooling you can get on air. The prospect of
       building all AMD machines in the coming months has got me totally
       nerding out.
     *
   kinggremlin [X]
       [ ]

     Quote:

     I read the review, don't know what you mean by "let alone power
     consumption". Efficiency is virtually the same. I personally prefer
     blowers as long as they're not crazy loud, and the review says
     they're not so I'm all for it. If you DON'T like blowers, I'm sure
     there will be third-party coolers that improve cooling performance
     and acoustics further (and dump heat into the chassis like crazy).
     Who ever promised that? They are 10-11% faster than same-priced
     Nvidia models. They're not going to drop 5700 $100 bucks when
     they're already ahead by 11%, even though I would love lower prices,
     there's no incentive for them to do so.
       You didn't read the review. Ignoring the Furmark results which
       don't mirror any realworld scenario. The 5700xt is slower than the
       2070 Super while using more power and running over 10degrees C
       hotter in gaming.
     *
   alextheblue [X]
       [ ]

     Quote:

     You didn't read the review. Ignoring the Furmark results which don't
     mirror any realworld scenario. The 5700xt is slower than the 2070
     Super while using more power and running over 10degrees C hotter in
     gaming.
       Yes, I did, stop being obstinate. Ah, I get it, you must have read
       "efficiency" in my post as "power consumption", or something. Why
       are you comparing it to the 2070 Super? The 5700 is the same price
       as the 2060 and 11-12% faster on average (between TH and AT), and
       the 5700XT is the same price as the 2060 Super and roughly 10-11%
       (TH-AT) faster. Yes, they use more power, but they're faster. As a
       result the efficiency is pretty close. It varies based on workload
       (game title), but I have read the reviews here and AT (so far,
       haven't looked at a third review yet). Here:
       https://www.anandtech.com/show/14618/the-amd-radeon-rx-5700-xt-rx-5
       700-review/15
       Factor in the performance gain (in AT's suite it was 11% for the XT
       and 12 for vanilla 5700) and you'll see their power consumption is
       pretty good. Average that with TH's results in Metro: LL and the
       final efficiency is pretty neck and neck with their direct
       competitors.
       I didn't say they didn't run hot. For people that don't like
       blowers (as I already said) there will be cooler, quieter third
       party options.
     *
   digitalgriffin [X]
       [ ]

     Quote:

     I read the review, don't know what you mean by "let alone power
     consumption". Efficiency is virtually the same. I personally prefer
     blowers as long as they're not crazy loud, and the review says
     they're not so I'm all for it. If you DON'T like blowers, I'm sure
     there will be third-party coolers that improve cooling performance
     and acoustics further (and dump heat into the chassis like crazy).
     Who ever promised that? They are 10-11% faster than same-priced
     Nvidia models. They're not going to drop 5700 $100 bucks when
     they're already ahead by 11%, even though I would love lower prices,
     there's no incentive for them to do so.
       In all honesty, all these cards are expensive. $350 would have been
       the most I wanted to pay for a 5700XT. And the card does run hot.
       Pascal was a small move up in prices. Turing was just insane
       pricing wise.
       That being said I bought one today and said "F"-it. I just don't
       like NVIDIA's business ethics. It will get the job done for two to
       three years.
     *
   Axiss [X]
       [ ]
       any idea when the board partner cards come out?
     *
   randomizer [X]
       [ ]
       How many engineers looked at those fan curves in lab testing and
       thought they were good?
       I think this release is a bit underwhelming. Local pricing here
       makes the RX 5700 fairly unattractive compared to a 2060, but the
       XT is better positioned against the 2070. Not sure it's really
       worthwhile upgrading a 970 though.
     *
   daglesj [X]
       [ ]
       So for the many of us on a RX480?...
       Worth it? Could someone not dig out the previous AMD midrange value
       demon to test against? C'mon...
     *
   feelinfroggy777 [X]
       [ ]
       The last $200 card AMD has released was 2.5 years ago. Let that
       sink in for a minute.
     *
   jeremyj_83 [X]
       [ ]

     Quote:

     How many engineers looked at those fan curves in lab testing and
     thought they were good? I think this release is a bit underwhelming.
     Local pricing here makes the RX 5700 fairly unattractive compared to
     a 2060, but the XT is better positioned against the 2070. Not sure
     it's really worthwhile upgrading a 970 though.
       On average the 5700XT is about twice as fast as the GTX 980 and the
       980 is 10-15% faster on average than the 970. That means that going
       to the XT will double your framerate assuming your CPU can keep up.
       I would say that is a worthwhile upgrade.
       https://www.anandtech.com/bench/product/2522?vs=2529
     *
   randomizer [X]
       [ ]

     Quote:

     On average the 5700XT is about twice as fast as the GTX 980 and the
     980 is 10-15% faster on average than the 970. That means that going
     to the XT will double your framerate assuming your CPU can keep up.
     I would say that is a worthwhile upgrade.
     https://www.anandtech.com/bench/product/2522?vs=2529
       I usually like to triple my framerates while sticking to a similar
       price bracket :)
       Also I don't think I can afford to run that space heater in summer.
     *
   redgarl [X]
       [ ]
       For anyone that want to know what matter...
       https://static.techspot.com/articles-info/1870/bench/Cost.png
       https://static.techspot.com/articles-info/1870/bench/Cost1.png
       With the actual scoring, this review is a joke. Navi is disrupting
       pricing and almost match a 1080 TI for 400$... however at
       tomshardware it sux.
     *
   alextheblue [X]
       [ ]

     Quote:

     In all honesty, all these cards are expensive. $350 would have been
     the most I wanted to pay for a 5700XT. And the card does run hot.
     Pascal was a small move up in prices. Turing was just insane pricing
     wise. That being said I bought one today and said "F"-it. I just
     don't like NVIDIA's business ethics. It will get the job done for
     two to three years.
       They ARE expensive - both Nvidia and AMD. This is the new
       "mid-range" unfortunately. I'd like to see Xe undercut them both
       and force this tier back down to the ~$250 range. I still might end
       up getting one, we'll see.

     Quote:

     How many engineers looked at those fan curves in lab testing and
     thought they were good? I think this release is a bit underwhelming.
     Local pricing here makes the RX 5700 fairly unattractive compared to
     a 2060, but the XT is better positioned against the 2070. Not sure
     it's really worthwhile upgrading a 970 though.
       If you're talking about the erratic behavior with the speed
       dropping over time, that looks to me like a bug or an issue with
       that sample. Their XT didn't act that way. Pricing can suck
       depending where you are... in the US it certainly offers more bang
       for the buck than the 2060. Maybe pricing will be more reasonable
       when partner boards become widespread. Like your icon, BTW, big fan
       of new Genesis/MD titles - awaiting Xeno Crisis currently.
     *
   randomizer [X]
       [ ]

     Quote:

     If you're talking about the erratic behavior with the speed dropping
     over time, that looks to me like a bug or an issue with that sample.
     Their XT didn't act that way
       While not as bad, I wouldn't call the XT's fan curve good either.
       It's going to start roaring the moment you get to the main menu
       (and you'll probably notice the sudden change) and it won't stop
       until you're back at the desktop.

     Quote:

     Like your icon, BTW, big fan of new Genesis/MD titles - awaiting
     Xeno Crisis currently.
       It is actually from a Genesis/MD game, but one that was released
       last year, not in the 90s. :)
     *
   TJ Hooker [X]
       [ ]

     Quote:

     For anyone that want to know what matter...
     https://static.techspot.com/articles-info/1870/bench/Cost.png
     https://static.techspot.com/articles-info/1870/bench/Cost1.png With
     the actual scoring, this review is a joke. Navi is disrupting
     pricing and almost match a 1080 TI for 400$... however at
     tomshardware it sux.
       Those performance numbers aren't very different than what TH is
       reporting. And values will obviously vary depending on your test
       suite. This review reflects the fact that these otherwise great
       value cards are paired with a mediocre blower cooler, it hardly
       says the cards "suck". Which amounts to saying that we should wait
       for AIB cards, which is pretty common advice for every GPU launch.
     *
   AgentLozen [X]
       [ ]

     Quote:

     For anyone that want to know what matter...
     https://static.techspot.com/articles-info/1870/bench/Cost.png
     https://static.techspot.com/articles-info/1870/bench/Cost1.png With
     the actual scoring, this review is a joke. Navi is disrupting
     pricing and almost match a 1080 TI for 400$... however at
     tomshardware it sux.
       While I was reading this article I was thinking "please please
       please let redgarl post in the forums!!" My wish was granted and
       Redgarl definitely provided.
       Redgarl describes this review as a "joke". He claims that Navi is
       "disrupting pricing" and "almost match a 1080 TI". He provides
       links to the TechSpot 5700 review for "anyone that want to know
       what matter..." This made me genuinely really curious. Is today the
       day that redgarl proves Tomshardware is run by a bunch of hacks? I
       had to know for myself.
       First of all I looked at the test setups for each review. Techspot
       uses a 9900K Intel CPU and 32GB DDR4-3200. Tomshardware's build is
       different. They use a 8086K Intel CPU and 64GB of DDR4 2400.
       Tomshardware documents the drivers used as 431.16 Nvidia Driver for
       the 2060Super and 2070Super but the 430.86 for every other Nvidia
       card. For AMD cards, they use the 19.7.1 driver with the RX 5700
       cards and the 19.6.3 driver for everything else. By comparison,
       Techspot reports that they used the "latest drivers available at
       the time". Its not clear how that compares to Tomshardware. The
       test machine on each respective site is different and that may
       cause differences in benchmarks. I predict (jk. I've already seen
       the results) the Tech Spot benchmarks may be a little higher
       judging by the faster memory and the better CPU.
       I then compared Techspots benchmarks to those featured on
       Tomshardware. First I looked at the Assassin's Creed Odyssey
       results. At 1440p, Techspot reports the Radeon RX 5700XT hit an
       average of 70fps. Meanwhile, Tomshardware was showing... oh dear,
       Odyssey wasn't featured on Tom's. Let's just move on. The
       Techspot's Destiny 2 1440p results show that.. uh oh, it's
       happening again. Techspot didn't feature Destiny 2 benchmarks. This
       is a problem because you can't compare apples-to-apples when the
       same benchmarks aren't used.
       Assassin's Creed Odyssey, Destiny 2, DiRT Rally 2.0, Far Cry 5, Far
       Cry New Dawn, Final Fantasy XV, GTA V, Resident Evil 2, Strange
       Brigade, Tom Clancy's Ghost Recon, Tom Clancy's Rainbow Six Siege,
       World War Z, The Witcher 3, Wolfenstein II: The New Colossus are
       either benchmarked by Tom's or TechSpot but not both. The only
       overlapping games are Battlefield V, Forza Horizon 4, Metro Exodus,
       Shadow of the Tomb Raider, and Tom Clancy's The Division 2.
       Let's look at the Shadow of the Tomb Raider results. Techspot and
       Tom's ran the game at 1440p, highest quality.... oh no.
       Tomshardware used SMAAT2x in it's Shadow of the Tomb Raider
       benchmark. Techspot didn't report using that setting. Well that's
       not apples-to-apples either. A similar situation happens with
       Battlefield V where Tomshardware uses DX12 and Techspot uses DX11.
       For Tom Clancy's The Division 2, I double checked to make sure the
       software is the same between both Techspot and Tom's platforms.
       DX12, 1440p, Ultra quality. Check. Now here is something that
       appears to be genuinely inconsistent between reviews. Techspot's
       averages for the GeForce 2080, 2070 Super, 2060 Super, Radeon RX
       5700, and 5700XT are higher than the Tomshardware numbers. Well
       there you have it. Redgarl has proven that "With the actual
       scoring, this review is a joke." Tomshardware losses, redgarl wins.
       Except that the test systems between Tomshardware and TechSpot are
       different. Isn't this a roller coaster? TechSpot has a better CPU
       and faster ram. I looked carefully at the trends between both
       reviews and lo and behold they show the same thing. Both sites list
       the video card pecking order as: 2080, 2070 Super, Radeon VII, 1080
       Ti, 5700XT, 2070, 2060 Super, Vega 64, 5700, 1080, 2060, and so on
       and so forth. It's very plausible that the differences in specific
       numbers can be chalked up to differences in platforms or margin of
       error.
       The charts that redgarl cherry picked show the results between Toms
       and Techspot are totally different. If you only considered those
       charts then you might think Tomshardware has an incompetent staff
       the way redgarl wants you to believe. However, when you look at the
       whole picture, its obvious that you can't directly compare the
       benchmark results between both sites. A total of 19 games were
       reviewed by both sites. Only five overlap. I threw out Shadow of
       the Tomb Raider because we can't be sure both sites used the same
       settings. That leaves four. When we directly compare results, the
       numbers vary probably due to differences in test hardware, but the
       trends are consistent. In conclusion: there isn't enough evidence
       to prove that the Tomshardware review did anything wrong.
       To redgarl, this is just for fun my dude. No hard feelings.
     *
   alextheblue [X]
       [ ]

     Quote:

     It is actually from a Genesis/MD game, but one that was released
     last year, not in the 90s. :)
       Yeah I know, that's why I pointed it out. It's from Tanglewood.
       That's what I was talking about when I said I am a fan of new
       Genesis games. Xeno Crisis, for example, isn't even (quite) out
       yet.
     *
   randomizer [X]
       [ ]

     Quote:

     Yeah I know, that's why I pointed it out. It's from Tanglewood.
     That's what I was talking about when I said I am a fan of new
     Genesis games. Xeno Crisis, for example, isn't even (quite) out yet.
       Righto, I thought it was just the style that reminded you of
       Genesis games. I didn't expect to run into anyone else who had
       actually heard of this game :LOL:
     *
   treetops422 [X]
       [ ]
       "Across our benchmark suite, Radeon RX 5700 XT averages 9.9%-higher
       frame rates than the GeForce RTX 2060 Super at 2560 x 1440. Radeon
       RX 5700 averages 11%-higher frame rates than the GeForce RTX 2060
       at the same resolution. The GeForce RTX 2070 Super does serve up
       average frame rates 6.9% higher than Radeon RX 5700 XT, but it
       costs 25% more. "
       That's all I need to know. Who would RT and half their performance?
       Esp on a 2060
     *
   B-Real85 [X]
       [ ]

     Quote:

     Honestly I'd pay the extra just for Nvidia's decent cooler, let
     alone power consumption, RTX, etc. Hard pass on another blower.
          + Most consumers buy AIB models, so it's not a problem.
          + Power consumption? RX 5700 consumes about the same as the RTX
            2060 , therefore it has better performance/W ratio. The RX
            5700 XT is near the 2070's ratio.
          + RT: 3 games so far, in which only the RTX 2080 Ti can produce
            tolerable fps with about 40-45 minimums, and it's still on
            FHD. RTX 2060 produces 50-60 average and 30 lows... That's
            what you expect from a 350$ card? Don't think so.

     Quote:

     Dear AMD, Why? Wasn't your line GTX 1080 performance at RX 580
     prices? points at Navi This is not that. Regrettably yours, Your
     Fans
       No, sadly not. There was only a rumoured news on WCCFTech. There
       was 7 or 8 models with 200$ Vega 56 and so on. But AMD never spoke
       about such prices. That was only a rumour (sadly).

     Quote:

     You didn't read the review. Ignoring the Furmark results which don't
     mirror any realworld scenario. The 5700xt is slower than the 2070
     Super while using more power and running over 10degrees C hotter in
     gaming.
       And costs 100$ more. And, wow, really, 5W more than a 2070 Super,
       fantastic. AIB models help you. As 99% of the NV customers also buy
       AIB NV models.

     Quote:

     In all honesty, all these cards are expensive. $350 would have been
     the most I wanted to pay for a 5700XT. And the card does run hot.
     Pascal was a small move up in prices. Turing was just insane pricing
     wise. That being said I bought one today and said "F"-it. I just
     don't like NVIDIA's business ethics. It will get the job done for
     two to three years.
       You are 100% right, just mentioning: if someone needs performance
       over RT feature, RX5700 is absolutely better than the RTX 2060 and
       the RX5700XT is absolutely better than the RTX2060 Super and RTX
       2070.

   Most Popular
    1. Confirmed: AMD’s Navi RX 5700 Graphics Cards Will Be Cheaper Than
       We Thought
    2. Update: Nvidia VP Says Next-Gen GPUs Will Be Made by TSMC and
       Samsung
    3. AMD Radeon RX 5700 XT and RX 5700 Packaging Leak

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * Raspberry Pi

     Review

    Raspberry Pi 4 Review: The New Gold Standard for Single-Board Computing

   by Avram Piltch August 28, 2019 at 9:04 PM

     *
     *
     *
     *
     *
     *

   46 Comments

   Raspberry Pi has long been the gold standard for inexpensive
   single-board computing, powering everything from robots to smart home
   devices to digital kiosks. The long-anticipated Raspberry Pi 4 takes Pi
   to another level, with performance that’s good enough to use in a pinch
   as a desktop PC, plus the ability to output 4K video at 60 Hz or power
   dual monitors.

Raspberry Pi 4

   Editor's choice tom's Hardware
   [javascript]
   Pros
     * Much faster than prior Raspberry Pis
     * USB 3 Ports
     * Ability to output 4K video at 60 Hz
     * Dual-monitor support

   Cons
     * Key software doesn’t work at launch
     * Poor high-res video playback

   Verdict

   The best Raspberry Pi yet is faster and more functional than any of its
   predecessors, without raising the price.
   4.5/5
   $35CanaKit

   IFRAME: https://content.jwplatform.com/players/YdWWS5dA.html

   For the same $35 starting price as prior models, you get speeds that
   are two to four times faster, support for USB 3 and true Gigabit
   Ethernet. Perhaps more importantly, there is a $45 Raspberry Pi 4 with
   2GB of RAM and a $55 unit with 4GB, four times more than any previous
   Pi has had. Makers and hobbyists should add the Raspberry Pi 4 to their
   arsenals, and tech enthusiasts who’ve never used a Pi before now have
   even more reasons to buy one.

   Raspberry Pi 4 Model B. (Credit: Tom's Hardware) Raspberry Pi 4 Model
   B. (Credit: Tom's Hardware)

   We had an opportunity to get early access to the Raspberry Pi 4 B, the
   full name of the first Pi 4 model, and were able to test a board with
   the full 4GB of RAM. What we saw is a full-fledged mini computer that’s
   packed with potential. We’re particularly excited about the
   possibilities for inference, particularly object and sound detection.

  Backward Compatibility

   It’s important to note that, even a couple of months after launch, some
   important Raspberry Pi software doesn’t yet work on the Pi 4. To run Pi
   4, you’ll need to download a brand new build of the Raspbian
   OS, Raspbian Buster. And not everything runs in Buster yet. During
   testing, we found numerous Python libraries or other required packages
   that weren’t compatible with the new OS.

  Game Emulation or Lack Thereof

   Retropie, the very popular gaming emulator software, does not
   officially support the Raspberry Pi 4. We did find a workaround that
   lets you run Retropie on the Raspberry Pi 4, but right now it's a
   kludge that doesn't necessarily work well. You can also download a beta
   version of Lakka, another emulation platform, but it too is not a
   final, fully working build. Developers have said that they are working
   on a Pi 4-compatible version and will have one soon, but if you’re
   reading this today and need to build an arcade machine in the near
   future, you might want to get an older model.

  Key Differences

   The table below shows a key specs comparison between the Raspberry Pi 4
   B, the first an only Pi 4 model, and the Raspberry Pi 3B+, the fastest
   version of the Pi 3.

   Spec Raspberry Pi 4 B Raspberry Pi 3 B+
   CPU 1.5-GHz, Quad-Core Broadcom BCM2711B0 (Cortex A-72) 1.4-GHz, Quad
   Core Broadcom BCM2837B0  (Cortex A-53)
   RAM 1 - 4GB DDR4 1GB DDR2
   GPU 500 MHz VideoCore VI  400 MHz VideoCore IV
   Video Out dual micro HDMI ports single HDMI port
   Max resolution 4K 60 Hz + 1080p or 2x 4K 30 Hz 2560 x 1600
   USB Ports 2x USB 3.0 / 2x USB 2.0 4x USB 2.0
   Wired Networking Gigabit Ethernet 330 Mbps Ethernet
   Wireless 802.11ac (2.4 / 5 GHz), Bluetooth 5.0 802.11ac (2.4 / 5 GHz),
   Bluetooth 4.1
   Charging Port USB Type-C micro USB
   Power Requirement 3A, 5V 2.5A, 5V
   Size 3.5 x 2.3 x 0.76 inches (88 x 58 x 19.5mm)
   3.2 x 2.2 x 0.76 inches (82 x 56 x 19.5mm)
   Weight 0.1 pounds (46 grams) 0.11 pounds (50 grams)

   The most important new features are the faster processor and GPU, more
   and faster RAM, the addition of USB 3 ports, dual micro HDMI ports
   instead of a single HDMI connection and support for 4K output. The
   higher bus speed that enables USB 3 support also allows the on-board
   Ethernet port to support true Gigabit connections (125 MBps) where the
   last-gen models had a theoretical maximum of just 41 MBps. The microSD
   card slot is also twice as fast, offering a theoretical maximum of 50
   MBps versus 25 MBps on the 3B+.

   Because the new SoC needs more power, the Raspberry Pi 4 B charges
   over USB Type-C instead of micro USB. It also requires a power adapter
   that can deliver at least 3 amps of power and 5 volts, though you may
   be able to get away with 2.5 amps if you don’t attach many peripherals
   to the USB ports. Putting aside the power needs, USB Type-C connectors
   are reversible, which makes them much easier for kids (and adults) to
   plug in.
   After we first published this review, we found out that some USB Type-C
   cables don't work with the Raspberry Pi 4. Those cables that are
   "electronically marked" will see the Pi 4 as a USB audio device and not
   provide power. Only USB-C to USB-C cables can be electronically marked
   and you only find this on cables that operate at 5 Gbps or higher. In
   our tests, we found 11 low-cost USB-C cables that work with the
   Raspberry Pi 4 and only found a few cables, all USB 3.1, that did not.
   If you buy Raspberry Pi's official Pi 4 charger, which comes with a
   cable, you also won't have a problem.  The Pi foundation says it will
   fix the problem on future builds, but it can't be solved via a firmware
   update.

  Design

   (*) ( ) ( ) ( ) ( ) ( ) ( ) ( )
     * raspberrypi4-hero-2
       raspberrypi4-hero-2
     * raspberrypi4-hero
       raspberrypi4-hero
     * raspberrypi4-gpio
       raspberrypi4-gpio
     * raspberrypi4-hdmiusbav
       raspberrypi4-hdmiusbav
     * raspberrypi4-usbethernet
       raspberrypi4-usbethernet
     * raspberrypi4-ethernet
       raspberrypi4-ethernet
     * raspberrypi4-usbtypec
       raspberrypi4-usbtypec
     * raspberrypi4-box
       raspberrypi4-box

   (*) ( ) ( ) ( ) ( ) ( ) ( ) ( )
     * raspberrypi4-hero-2

raspberrypi4-hero-2
     * raspberrypi4-hero

raspberrypi4-hero
     * raspberrypi4-gpio

raspberrypi4-gpio
     * raspberrypi4-hdmiusbav

raspberrypi4-hdmiusbav
     * raspberrypi4-usbethernet

raspberrypi4-usbethernet
     * raspberrypi4-ethernet

raspberrypi4-ethernet
     * raspberrypi4-usbtypec

raspberrypi4-usbtypec
     * raspberrypi4-box

raspberrypi4-box

   At 3.5 x 2.3 x 0.76 inches (88 x 58 x 19.5 mm)  and 0.1 pounds (46
   grams), the Pi 4 is thin enough to fit in your pocket and light enough
   to carry anywhere. The board is durable enough to probably survive
   rolling around in your bag, but we recommend sticking it in something
   protective, mostly to protect the pins. However, during testing, I
   always used the board bare on my desk and I carried it back and forth
   between work and home many times by simply putting it in a cardboard
   box with no padding or static bag.

   Unfortunately, if you want a case, you can’t use one that’s been
   designed for any previous Raspberry Pi. The Raspberry Pi 3 B / 3 B+
   have almost the same dimensions, but the port layout has changed just
   enough to make the Pi 4 B incompatible. Where prior Pis had a single,
   full-size HDMI port, the dual micro HDMI connectors on the Pi 4 jut out
   more and so don’t line up with the holes on anything that was designed
   for the Pi 3 B. We really like the Pimoroni Pibow, a $10 / £8.50 case
   that looks really good and doesn't cover over the GPIO pins.

   The Raspberry Pi 4 covers more than just the basics when it comes to
   ports. The right side has four USB Type-A connections, two of which are
   USB 3.0. There’s also a full-size, Gigabit Ethernet port for wired
   connections there. The bottom edge has a 3.5mm audio jack, two micro
   HDMI ports and the USB Type-C charging port. On the left side, you’ll
   find the microSD card reader.
   And on the top surface of the board, you’ll see ribbon connectors for
   the Camera Serial Interface (CSI) and Display Serial Interface (DSI),
   which provide dedicated connections to Raspberry Pi’s own camera and
   screen (or compatible accessories). Of course, you can connect a camera
   to a USB port as well and there are a couple of more common ways,
   including the micro HDMI ports, to output to a screen.

  New CPU, RAM

   The Raspberry Pi 4 has similar design and dimensions to its
   predecessors, but it’s an all-new platform, powered by a new processor,
   the Broadcom BCM2711B0. Since the first Pi in 2012, all  Pis have used
   40nm SoCs, but this new chip is based on a 28nm process and, instead of
   the older Cortex-A53 microarchitecture, it uses Cortex-A72. The
   BCM2711B0 in the Raspberry Pi 4 has four cores and is clocked at 1.5
   GHz, which at first blush, doesn’t seem much quicker than the
   quad-core, 1.4 GHz BCM2837B0 in the Raspberry Pi 3B+.

   However, Cortex A72 has 15-instruction pipeline depth, compared to just
   8 on the older model, and it also provides out-of-order execution so
   it’s not waiting for the output of one process to start on another. So,
   even at the same clock speed (and the BCM2711B0 is based on a smaller
   process node), Cortex-A72 processors will be significantly faster and
   use more power than their A53-powered ancestors.

   For example, on the Linpack benchmark, which measures overall compute
   power, the Pi 4 absolutely whooped the Pi 3 B+ in all three tests. On
   the all-important single precision (SP) test, the Pi 4, scored 925 as
   compared to the 3 B+’s mark of 224, a boost of 413 percent.

   On the Sysbench CPU test, the Pi 4 B was capable of performing 394
   events per second as compared to 263 for the Pi 3 B+. That's a
   difference of 50 percent.

   The RAM is also quite a bit quicker, going from 1GB of DDR2 RAM
   operating on the Pi 3B+ to up to 4GB of DDR4 RAM. In addition to the
   increased bandwidth, having more memory is a huge deal, particularly
   for web surfing.

   The Pi 4’s RAM returned read and write rates of 4,130 and 4,427 Mbps,
   respectively. That’s 51 percent and 54 percent better than the 3 B+.

   Both the CPU and the RAM are implicated when you do file compression.
   When zipping a file in multithreaded mode, the Pi 4 B is 37 percent
   quicker than its predecessor, but it’s far stronger in single-threaded,
   eclipsing the 3 B+ by 60 percent.

  New GPU, Faster Graphics Performance

   The GPU is getting a nice boost too. It moves from a Broadcom VideoCore
   IV that operated at a core clock speed of 400 MHz to a VideoCore VI
   that’s set at 500 MHz. The new architecture allows it to output to a
   display at up to 4K resolution with a rate of 60 fps or to support dual
   monitors at up to 4K 30 Hz.

   While we wish we could have tried some of the more resource-intensive
   emulators in Retropie in time for this review, there wasn’t a Pi
   4-compatible version at launch. However, the OpenArena Benchmark, which
   measures frame rates in a game that’s a a clone of Quake III Arena, did
   run.

   At 720p resolution, the Pi 4 was the only Raspberry Pi capable of
   delivering smooth frame rates. Yes, you can play on the Pi 3, 3 A+ or 3
   B+, but all three deliver rates between 27 and 28 fps as compared to
   41.4 fps on  the Pi 4.

  Storage Performance

   No matter how fast your processor, RAM and GPU are, if your storage is
   slow, everyday tasks like opening apps and files will be laggy. Like
   all Raspberry Pis, the 4 B’s primary storage device is its microSD card
   reader, which is convenient but a bit constrained. According to the Pi
   Foundation, the 4 B has a top transfer rate of 50 MBps, which is double
   the speed of the reader on the 3 B+. There’s no known limit on
   capacity.

   Our benchmarks, which were conducted with a Samsung EVO Plus microSD XC
   Class 10 card, show less impressive rates than the theoretical
   maximums. The Pi 4 B returned sequential read / write rates of 45.7 and
   27.7 MBps, while the 3 B+ trailed at 22.8 and 17.5 MBps. Keep in mind
   that the card is rated for 100 MBps reads and 60 MBps writes.
   If you have a speedy USB Flash drive or an external SSD, you can get
   far better storage performance out of the Pi 4 B. The Pi 4 B is the
   first with USB 3 ports, which have a maximum, theoretical bandwidth of
   625 MBps. To find out how this works in real-life, we wrote a separate
   article where we attached an external SSD to a Raspberry Pi 4 B.
   You'll find full results in the article, but what we found was
   impressive.

   Using a Western Digital Blue SSD in a USB to M.2 enclosure, we saw
   transfer rates that were 2 to 13x times faster than the microSD card.
   And applications definitely opened a lot faster with the SSD attached.
   Unfortunately, a regular USB Flash drive, was often slower than the
   microSD card.

   However, it's important to note that, at present, the Pi 4 firmware
   doesn't allow you to boot off an external drive so the best that you
   can do is run all of your programs, including the majority of the OS,
   off of an SSD while leaving the boot partition on a microSD card. A
   firmware update should fix this in the next few weeks, but for now, we
   have an article explaining how to run your Raspberry Pi 4's software
   from an SSD.

   Fast USB 3 ports are about more than just storage. You can use other
   high-bandwidth peripherals like Google’s Coral USB Accelerator, which
   helps with artificial intelligence tasks.

  Networking Performance

   The Raspberry Pi 4 has the same 802.11ac Wi-Fi as its direct
   predecessor, but it throws in Bluetooth 5.0 support, an improvement
   over the Bluetooth 4 on prior models. More importantly, the Ethernet
   port now has more bandwidth, which allows it to offer a full gigabit of
   throughput where prior models could only achieve about 330 megabits.

   In testing, the PI 4 B’s Ethernet port achieved 943 Mbps, which blows
   away the other Raspberry Pis. In fact, in a throughput test, the Pi 4 B
   got 943 Mbps (close to the 1,000 Mbps maximum). That’s nearly five
   times as many as the Pi 3B+, which only got 237 Mbps.

   Both the old and new Raspberry Pi have 802.11ac Wi-Fi that can run on
   2.4 GHz or 5-GHz bands. So we didn’t expect to see much difference in
   performance here. But the 5-GHz throughput is noticeably higher for the
   Pi 4, returning a rate of 114 Mbps, compared to 97 Mbps on the Pi 3 B+,
   a decent 18 percent improvement.

  Power and Heat

   With a more power-hungry processor and the need for at least a 5-volt,
   3-amp power adapter, the Pi 4 should be expected to consume more power
   than its predecessors.

   At idle, the Pi 4 B draws 3.4 watts, which is just 17 percent more than
   the 3 B+. Under load, that number jumps to 7.6 watts, but that’s still
   only 19 percent more juice than its direct predecessor. If you want the
   lowest-power Pi, performance be damned, then go for the Pi Zero W,
   which consumes a mere 0.8 watts at idle and 1.6 watts under load.

   Yes, this board gets warm, warmer than its predecessor. Thermal images
   mirror what we experienced; the areas of the board near the CPU get
   really warm, not just the top of the processor itself. The Pi 4 board
   reaches a toasty 74.5 degrees Celsius (166 degrees Fahrenheit). That’s
   not enough for a serious burn, but kids especially should be sure to
   pick up the Pi by its sides only. The top surface of the Pi 3 B+ is
   much cooler, maxing out at 62.5 degrees Celsius (144.6 degrees
   Fahrenheit).

   Thermal image of Raspberry Pi 3 B+. (Credit: Gareth Halfacree) Thermal
   image of Raspberry Pi 3 B+. (Credit: Gareth Halfacree)

   Thermal image of Raspberry Pi 4 B. (Credit: Gareth Halfacree) Thermal
   image of Raspberry Pi 4 B. (Credit: Gareth Halfacree)

   As with any modern computer,  if you push the system too hard and the
   CPU or GPU get too hot, the computer will throttle down to avoid
   damage.

   While running a CPU-intensive workload for 10 minutes, the processor
   hit 81 degrees and began throttling down from 1.5 to 1 GHz after 3
   minutes. However, the system kept bringing itself back to the full 1.5
   GHz when it dipped down to around 80 degrees, but then it would get
   warm again and go down to 1 GHz. If you want to have better sustained
   performance under load, consider getting an active cooler for the
   Raspberry Pi 4 or, at the very least, attach a passive heat sink.

  GPIO Pins

   The real star of the show on any Raspberry Pi is its set of 40 GPIO
   (General Input / Output) pins. The pin count and layout remains
   unchanged from prior models, going back to the Raspberry Pi 2, so any
   “hats,” sensors or LED screens that were made to attach to a Pi 2 or 3
   will be compatible.

   (Image Credit: Gareth Halfacree) (Image Credit: Gareth Halfacree)

   However, the Raspberry Pi 4 has added a few new capabilities to some of
   the pins. For hardcore makers who are wiring up a variety of
   peripherals, the GPIO pins now support four additional I2C, SPI and
   UART connections. So, if your sensors or peripherals require any of
   these interfaces, you now have a lot more of them.
   Below, you'll find a new GPIO pinout, with the added capabilities of
   the Pi 4. To learn more about what each pin does, checkout our
   Raspberry Pi 4 GPIO pinout article.

   Raspberry Pi 4 GPIO Pinout. Image Credit: Les Pounder Raspberry Pi 4
   GPIO Pinout. Image Credit: Les Pounder

   The speed and responsiveness of the GPIO pins is also much faster on
   the Raspberry Pi 4, likely due to its faster processor. Our test uses
   the gpiozero Python library to toggle pins on and off continuously and
   measures the rate at which they switch. The Pi 4 achieved a speed of
   50.8 KHz, compared to just 16.1 on the Pi 3 B+. That’s an improvement
   of 215%.

  Using the Raspberry Pi 4 as a PC

   One of the goals of the Raspberry Pi 4 is to be a capable PC that
   anyone can use for surfing the web, doing light productivity work or
   even playing very basic games. In order to test this use case, I spent
   several hours doing my everyday work on the device and I even wrote
   portions of this review using it.

   I really liked being able to output to dual monitors, something I do
   everyday at both work and home. And, since much of my daily work
   routine these days takes place in a web browser, I had no problem
   writing, editing and researching articles using Chromium. Even with 15
   tabs open, switching between them was smooth and I had not maxed out
   the 4GB of on-board RAM.

   And while I wouldn’t want to use it every day, GIMP provides a decent
   way to edit still images. If I wanted to crunch spreadsheets or compose
   documents outside of Google Docs, Libre Office more than fits the bill.

   My biggest problems involved video playback. If I wanted to watch a
   YouTube video, I had to keep it in a window, because even in 480p
   resolution, it was jerky at full screen. The other task I’d like to
   perform is playing retro games, but as of this writing, the Retropie
   package of emulators doesn’t work with Pi 4. I was able to install and
   play Quake Arena, however.
   Keep in mind that the Raspberry Pi 4 works with a few different
   operating systems, but the best-supported one is Raspbian, a flavor of
   Linux that has a small learning curve for newbies. Users who are only
   looking for a low-cost web-surfing PC, without doing any tinkering, can
   find a Chromebook or low-end Windows laptop for $150 to $200.

  4K Output, Video Playback and Transcoding

   One of the downsides of prior Raspberry Pi computers is that they can
   only natively output to one screen at a time, but if you like
   multitasking and want to use a Pi for productivity, you really want
   that second screen. The Raspberry Pi 4 has dual micro HDMI ports that
   can each connect to a separate monitor or TV and can operate at up to
   4K (3840 x 2160) resolution. If you have multiple 4K monitors, you have
   a choice: you can either run each screen at a somewhat-sluggish 30 Hz
   or, you can enable 4K mode in the settings menu, which jacks up the
   voltage a little so you can run one monitor at 4K 60 Hz and another at
   up to 1080p.

   During extensive hands-on testing, I found that, while the 4K at 30 Hz
   is tolerable, little things like the movement of the mouse pointer are
   a bit sluggish. If you have a 4K screen, you’re definitely better off
   going for the 60 Hz mode, but note that the added voltage may also
   cause your CPU to get hot and throttle more easily.

   While surfing the web, looking at still images and just enjoying all
   the extra screen real estate of 4K is great, video playback is the
   Raspberry Pi 4’s Achille’s heel, at least as of this writing. Whether
   we were attempting to stream a 4K video or use a downloaded file, we
   never got a smooth, workable 4K experience, either in Raspbian Buster
   or LibreElec, an OS that runs the Kodi media player. Several H.264
   encoded videos, including Tears of Steel, did not play at all or showed
   as a jumble of colors. Even the sample jelly fish videos that the folks
   at Kodi recommended for my testing appeared as still pictures with no
   movement. Clearly, there’s a lot of optimization that still needs to be
   done both on the OS and software side to make the Raspberry Pi 4
   capable of playing 4K video.

   Unfortunately, even streaming 1080p YouTube videos is a challenge at
   this point. Running at 1080p resolution, full screen video trailer for
   Stranger Things showed obvious jerkiness. However, the playback was
   smooth when I watched the same clip in a smaller window. The same
   problem occurred, even when I dropped the stream’s resolution down to
   480p.

   Playing offline 1080p videos works well, provided your screen is at
   1920 x 1080 or lower resolution. A downloaded trailer of Avenger’s
   Endgame was perfectly smooth when I watched it using the VLC player.

   The Raspberry Pi 4 won’t replace anyone’s MacBook Pro or Dell XPS 13
   creative workstation, but it can transcode videos for you, if you’re
   patient. It took the Raspberry Pi 4 48 seconds to transcode a very
   short H.264 encoded clip to NTSC DV format using FFmpeg. That’s much
   less time than the Pi 3 B+, which finished in 108 seconds, but if
   you’re converting a whole movie, you’ll probably need to walk away from
   your Pi for a while and then come back.

  Web Surfing

   The web surfing experience on the Raspberry Pi 4 is noticeably much
   smoother than on any of its predecessors. The faster processor helps,
   but so does having more than 1GB of RAM. Keeping my eye on Gnome System
   Monitor, I noticed that, even with just one or two tabs open, I was
   using more than 1GB of RAM. However, on the Pi 4 with 4GB of RAM, I had
   no problem running over 15 tabs at once, switching back and forth
   between them.

   While web pages don’t render as quickly as on my modern Core i7-powered
   laptop with Windows 10, the Pi 4 provides a very solid web browsing
   experience. I had no problems using my Google suite apps, including
   Gmail, Google Sheets and Google docs.

   On Jetstream 1.1, a synthetic browsing benchmark that measures
   Javasript processing and page rendering, the Pi 4 trounced the Pi 3B+,
   42.5 to 17.1  That’s an improvement of 148%, but the Pi still isn’t
   quite as powerful as a low-end, Intel-powered Chromebook like the
   Samsung Chromebook 3, which scored 49.7. However, there are PC laptops
   that fared worse, including the Dell Inspiron 14 3000, which hit just
   35.9.

   The Speedometer 2.0 benchmark measures overall responsiveness by
   loading dummy web apps and then simulating a user interacting with
   them. A higher score on this test, in terms of runs per minute, shows
   that when you’re actually using a web tool such as Google docs or
   Gmail, you should get less lag. As on Jetstream and in real-world
   scenarios, the Pi 4 came out comfortably ahead of its predecessor. In
   this case, it was 98 percent faster.

   Just forget about using web sites with webGL animation on them, because
   they are slideshow like, at least with the current software. When I
   launched the webGL aquarium demo, which shows 50 fish swimming, I got a
   rate of just 2 fps on the Raspberry Pi 4 and a mere 1 fps on the Pi 3
   B+. I guess that makes the Pi 4 twice as fast, but 2 fps is still
   useless.

  Web Hosting

   It's very easy to use set up a Raspberry Pi web server and this is one
   of the most popular use cases for the computer. In fact, at Tom’s
   Hardware, we use a Raspberry Pi 3 B as a server on our local network
   that we use to host our laptop battery test. Raspberry Pi 4 promises
   even more robust web surfing thanks to its faster processor, greater
   amount of RAM and better network connectivity.

   Using the Phoronix Apache Test, the Raspberry Pi 4 handled 3,983
   requests per second versus 2,850 for the Pi 3 B+. That’s an improvement
   of 40 percent, which means that you can deliver heavier web pages or
   serve more visitors at the same time, without lag.

   Many web applications use the PHP server-side scripting language so
   faster processing of PHP can help a lot. On PHPBench, which measures
   PHP performance, the Raspberry Pi 4 B scored 101,540, more than double
   the Pi 3 B+'s mark of 41,351.

  A.I., Inference and Machine Learning

   Perhaps the most exciting new use case for the Raspberry Pi 4 is for
   inference and machine learning. With the earlier Pis, you could already
   use a camera to do simple object detection at low frame rates, but the
   added performance and I/O from this new model should open up a whole
   new world of use cases.

   To see how well the Pi 4 handles object detection, we followed the
   steps in this tutorial, which uses a combination of Google’s TensorFlow
   machine learning platform and OpenCV, a programming library that’s good
   for computer vision. After spending a good three hours compiling and
   installing all the software, I got the app running and watched as the
   webcam identified a few -- very few -- objects in my office, including
   sensing that I was a “person” and my chair was a “chair” with great
   confidence. It operated at a sluggish rate of 1.7 fps, but that’s 70
   percent better than the 1 fps I got when running it on the Pi 3 B+.

   However, with a more optimized framework, the Pi 4 should be able to do
   real-time facial and object recognition. And, because it has USB 3, an
   accelerator like the Google Coral TPU USB dongle should have much more
   bandwidth to send data back to the SoC. Imagine building a home
   companion robot that recognizes every member of your household by face
   or one that helps a farmer sort cucumbers by type. Some of these
   workloads are possible on earlier Raspberry Pi computers, but the Pi 4
   B should make them fast and accurate enough to use on a regular basis.
   We can’t wait to see what developers and what makers do with Pi 4 and
   A.I.

   Scikit-learn is a popular python module that enables machine learning.
   Performing a task in Scikit-learn was more than twice as quick on the
   Pi 4 B.

  Compiling Code

   With Linux, you sometimes have to compile programs you want to install.
   Several times during our testing, we had to compile software packages,
   including when we wanted to get an object recognition demo going.

   A speedier processor and better RAM help the Raspberry Pi 4 B compile
   code much faster than its predecessor.  When we ran a test which
   compiles a Linux Kernel, the 4 B finished 33 percent faster. So,
   whether you're a developer who is writing your own software or just a
   user who wants a program that's not available as a direct download, the
   Pi 4 will save you time.

  Overclocking the Pi 4

   We’ve explained how to overclock the Raspberry Pi 4 and what kind of
   results you get in a separate article. However, the top line is that
   you can easily get the 1.5 GHz CPU up to 2 GHz and increase the
   frequency of the GPU from 500 to 600 MHz without missing a beat. Just
   make sure that you have a fan like the Pimoroni Fan Shim.

  How Much Raspberry Pi 4 RAM Do You Need?

   The Raspberry Pi 4 B comes in three configurations, which are identical
   but for the amount of RAM. The $35 entry-level model has 1GB of RAM,
   the $45 unit has 2GB and the $55 SKU goes all the way to 4GB. One of
   the great advantages of all Raspberry Pis is that they are affordable
   enough to use in anything, so you need to choose wisely. If you are
   building a robot or other iOT device that just deals with motors and
   sensors, 1GB should be enough, because you aren’t running a slew of
   apps and you don’t even need the GUI.

   We recommend 2GB if you’re doing very light web surfing, setting up a
   kiosk or deploying a limited-use web server. The 4GB model is ideal for
   using your Pi as a PC or for more complex tasks such as A.I.

  Bottom Line

   The Raspberry Pi 4 represents a giant leap forward, not only for the
   Raspberry Pi, but for single-board computing. For the first time, it’s
   realistic to use your Pi as a secondary or backup PC (or perhaps a
   kids’ first PC). However, the larger real benefit will come not from
   folks who use Raspberry Pi 4s in lieu of x86 PCs, but from all the
   innovators who harness the system’s enhanced performance, I/O and
   graphics to create new iOT devices, media servers and robots. Kids
   building Pi projects in school will also have a world of new learning
   possibilities.

   If you need a Raspberry Pi computer today, though, you’ll have to live
   with some issues that are likely to get resolved in the near future via
   software updates. Key apps like Retropie don't yet run Raspberry Pi 4
   and video playback performance is disappointing. While it’s certain
   that major applications will be ported to the new computer, we still
   don’t know exactly how good video playback will get once the operating
   system is refined over time.

   Despite these small issues , the Pi 4 stands head and shoulders above
   its predecessors and all the other inexpensive single board computers
   on the market. The main question isn’t: what can the Pi 4 do for you
   out of the box, but what can you make with it?

   Editor’s Note: Some of the benchmarks in this article were performed by
   co-author Gareth Halfacree, who has posted his own, detailed analysis
   of Raspberry Pi 4 performance on Medium.

   Image Credits: Tom's Hardware, Gareth Halfacree

   MORE: Raspberry Pi GPIO Pinout: What Each Pin Does

   MORE: How to Use Raspberry Pi as a VPN Gateway

   MORE: Raspberry Pi Tutorials

   [javascript]
   Raspberry Pi 4
   $35CanaKit

   About the author
   Avram Piltch & Gareth Halfacree @geekinchief

   Avram Piltch is Tom's Hardware's editor-in-chief. When he's not playing
   with the latest gadgets at work or putting on VR helmets at trade
   shows, you'll find him rooting his phone, taking apart his PC or coding
   plugins. With his technical knowledge and passion for testing, Avram
   developed many real-world benchmarks, including our laptop battery
   test.
   Read more
     * Raspberry Pi

   46 comments
   Comment from the forums
       Your comment
     *
   TerryLaze [X]
       [ ]
       Any plans on comparing the atomic PI to the raspberry?
       It's based on x86 so it would be much more compatible with what
       people need to run.
       https://dlidirect.com/products/atomic-pi
     *
   bit_user [X]
       [ ]
       I'm pretty sure your table has the GPU specs swapped. It says the
       Pi 3 has a 500 MHz VidCore VI and the Pi 4 has a 400 MHz VidCore
       IV.
       Speaking of the GPU, I'm really interested in support for OpenCL,
       Vulkan, and which OpenGL version it supports. Any other specs on
       the GPU would also be welcome!
       I should add that I'm actually a bit disappointed by the OpenArena
       benchmark. Given how old and slow the earlier Pis' GPU was (yes,
       even the Pi 3 used the same GPU as the original model that launched
       in 2012, just clocked a bit higher), I fully expected a much bigger
       jump. Think about how far desktop GPUs have come in that time -
       since Nvidia's GeForce 600 series and AMD's HD 7000 series.
     *
   bit_user [X]
       [ ]

     Quote:

     Any plans on comparing the atomic PI to the raspberry? It's based on
     x86 so it would be much more compatible with what people need to
     run. https://dlidirect.com/products/atomic-pi
       Overall, I think the Pi 4 would definitely win. The Atomic's SoC
       was designed for cell phones, that were none too popular. The
       Cortex-A72 cores in the Pi 4 are a bit newer and more efficient.
       Also, the Atomic Pi uses single-channel DDR3L, while the Pi 4 uses
       DDR4L.
       Of course, the Pi 4's biggest weakness remains storage. So, for I/O
       intensive tasks, the Atomic would still pull out some wins.
     *
   bit_user [X]
       [ ]
       In terms of specs, the best (sub-$100, at least) is still the
       ODROID N2:
       https://www.hardkernel.com/shop/odroid-n2-with-4gbyte-ram/
     *
   LordConrad [X]
       [ ]
       It'll be a nice change to have the support of the Raspberry Pi
       community AND a fast SBC to go with it. I never bought any of their
       previous SBCs because, for an extra $10-20, their competitors were
       always much better.
     *
   AllanGH [X]
       [ ]

     Quote:

     Any plans on comparing the atomic PI to the raspberry
       The Atomic Pi was dead before it was put up for sale....just
       somebody flipping old stock and system pulls from a mini-robot
       product. You won't be seeing support or new iterations of that
       platform--not from the purported manufacturer, at any rate.
     *
   punkncat [X]
       [ ]

     Quote:

     The Atomic Pi was dead before it was put up for sale....just
     somebody flipping old stock and system pulls from a mini-robot
     product. You won't be seeing support or new iterations of that
     platform--not from the purported manufacturer, at any rate.
       Agreed.
       I was taken in at first by all the promise and claims being made.
       Really glad I waited to purchase one until more detail came out.
       It seems that in addition to a fairly high fail rate, there are
       significant problems with audio, the well known poorly dealt with
       power delivery, as well as issues running "heavy" OS like it was
       purported to be capable of.
       The lack of possible future support was the final straw for me.
       This is a once it's gone, it's gone kind of thing. The whole thing
       was made for a different purpose and has connectors and
       functionality that, as of yet are (mostly) unknown and unable to be
       utilized.
       Many of the reviewers stated that even though the audio and OS
       issues were mostly resolvable that the time spent doing so
       outweighed the value. By and large the consensus among many of the
       purchasers was to go RPi for the community, the support, and the
       I/O functionality.
     *
   TJ Hooker [X]
       [ ]
       The 'Everything We Know about the Pi 4" article (from ~4 months
       ago) didn't age well :P
       "Release Date: Not Coming in 2019"
       https://www.tomshardware.com/news/raspberry-pi-4-everything-we-know
       ,38539.html
     *
   technome79 [X]
       [ ]
       The article mentions "true Gigabit Ethernet ". What is true gigabit
       ethernet? And what makes other gigabit ethernet false?
     *
   TJ Hooker [X]
       [ ]

     Quote:

     The article mentions "true Gigabit Ethernet ". What is true gigabit
     ethernet? And what makes other gigabit ethernet false?
       The 'gigabit' ethernet on previous models was fed by a USB 2
       connection, which is not capable of providing actual gigabit
       speeds. The "true gigabit ethernet" now allows for actual Gb/s
       throughput.
     *
   AllanGH [X]
       [ ]
       I never really did a drill-down into the specs before pre-ordering
       2 of the model 4s, and it now occurs to me that I really am hoping
       that RPF decided to add a lucid soft power switch facility to the
       board.
       I suppose I could actually look to see, but that would spoil the
       surprise. Nevertheless, if they didn't, the old method of adding a
       soft-power button will likely still work.
     *
   AlistairAB [X]
       [ ]
       Odroid N2 looks nice. Anyone have a Snapdragon 845 single board
       computer for a low cost? $450 for the Thundercomm one... ouch...
       The new Pi4 might beat/equal the nVidia nano now in CPU
       performance, wild.
     *
   AllanGH [X]
       [ ]
       I'm wondering about the performance, too.....
       My dad needed a computer on his desk at the local Senior's Center,
       where he is their VP, but the lady who is President is a bit of a
       snit (total control freak, actually) about things on the desks in
       the office.
       She outlined an area on his desk that was no larger than an LCD
       monitor, and said that's all he was allocated to use, because
       anything larger would be "ugly". SMH
       I screwed a nicely cased Pi-B3+ onto the back of an LCD monitor,
       hooked it up and gave him a wireless keyboard and mouse combo that
       he can lock in his desk drawer, and he's been deliriously happy
       about it ever since. I'm sure the fact that the President is green
       around the gills over it, doesn't hurt, either. ;)
       I've used it a few times (maintenance tasks, for the most part) and
       do see a bit of a lack in performance with a few things, but I'm
       used to much more horse power out of a desktop system.
       I'm now looking forward to swapping-out the old board for the new
       one, and seeing what sort of difference it makes in terms of
       performance.
     *
   R_1 [X]
       [ ]
       how about putting the price of the unit reviewed rather than the
       price of the cheapest variant?
       you tested and reviewed the 55 dollar 4GB unit, but list the price
       as 35 which is the 1GB unit.
       the link to CanaKit goes to the 1GB unit NOT the unit reviewed.
       if you review model A link to model A and reference model A
     *
   AllanGH [X]
       [ ]

     Quote:

     the link to CanaKit goes to the 1GB unit NOT the unit reviewed
       REALLY?
       [MEDIA=imgur]a/yiksYNH[/MEDIA]
       View: https://imgur.com/a/yiksYNH
       Clickity-click....
     *
   evilpaul [X]
       [ ]
       If you're interested in the Atomic Pi, see if you can get somebody
       to pay for part of the cost of a Compute Stick, cheap NUC, Latte
       Panda, etc for you, and buy one of those instead. It'll be just
       like an Atomic Pi, except functional without soldering crap to it,
       and there will be faster similar products produced to replace it in
       the future. And you can buy 30,001 of them if you want, because
       there's companies manufacturing them rather than pulling them out
       of talking Roombas that never made it to market.
       [USER=96206]@op[/USER], did you try tinkering with chrome://flags
       and seeing if you could force hardware video decoding in Chromium?
       The Youtube channel Explaining Computers's guy seemed to get
       streaming video to work OK in his review. Also, I'm pretty sure
       that ~40 FPS in Quake III Arena was what my Voodoo Banshee managed
       to pull off. Or that might have been the used Voodoo 3 that
       replaced it. Details are fuzzy, for some reason.
     *
   g-unit1111 [X]
       [ ]
       I think this might be the time I actually take the plunge and buy a
       Raspberry Pi. Does anyone know what kind of OS it can support?
       Could I try loading my spare Windows 7 license on it? Will Pi 4
       support memory cards larger than 32GB?
     *
   AllanGH [X]
       [ ]
       OS support is, AFAIK:
       NOOBS - no idea what that is, because I never tried it or read
       anything about it.
       and
       Debian Stretch, ported to the Pi...which works quite well, as long
       as you can ignore all the cutesy icons. o_O
       and
       Debian Jessie and Jessie-Lite, ported to the Pi. Same caveat about
       icons for the full-Jessie release.
       and
       For the Model 4, you'll need to download Debian Buster (still in
       testing), ported to the Pi....and Buster is backward compatible
       with all the Pi boards.
       #### EDIT ####
       I also just ran across this review of the Buster iteration of the
       OS:
       https://www.raspberrypi.org/magpi/raspberry-pi-4-raspbian-buster/
       ############
       There are also several other CLI-only (no-GUI) releases that I've
       tinkered with, but Jessie-Lite was quite a bit better than what I
       saw.
       I'm sure that there are other OS options available for the ARM
       Cortex-A53, and people have spoken of windows on the Pi (win10
       ARM64 or win10 IoT Core), but I've never been interested enough to
       bother looking.
     *
   AllanGH [X]
       [ ]
       Oh....the platform will support up to 128GB MicroSD or SD cards
       (depending on the specific board you have).
       You can get some answers to potential questions here.
       ################
       And I just ran across some third-party OS offerings:
       https://www.raspberrypi.org/downloads/
     *
   shivansps [X]
       [ ]
       Looks fine, the main bottleneck on the PI3 was the i/o, with USB3
       that is gone.
       The PI4 should the able to run Win10 ARM OK if a IGP driver is ever
       released.
       Freespace 2 Demo would have been a much better game to test here...
       IF OpenGL is working in the distro.
       https://www.raspberrypi.org/forums/viewtopic.php?t=230316
     *
   bit_user [X]
       [ ]

     Quote:

     The new Pi4 might beat/equal the nVidia nano now in CPU performance,
     wild.
       Yeah, but the point of the Nvidia platforms wasn't really CPU
       performance.
       Plus, their Nano is just a re-purposed Tegra X1, from 4 years ago.
       I guess they also disabled half of the CUDA cores (probably for
       power & yield reasons). Anyway, surpassing its CPU performance
       isn't really saying much...
     *
   bit_user [X]
       [ ]

     Quote:

     I screwed a nicely cased Pi-B3+ onto the back of an LCD monitor,
       Why not a NUC? ...unless it'd cost too much. AFAIK, they include a
       VESA mounting bracket.
     *
   bit_user [X]
       [ ]

     Quote:

     If you're interested in the Atomic Pi, see if you can get somebody
     to pay for part of the cost of a Compute Stick, cheap NUC, Latte
     Panda, etc for you, and buy one of those instead.
       Or an ODROID-H2, which looks to be an excellent implementation of
       the latest and greatest of Intel's low-power SoCs:
       https://www.hardkernel.com/shop/odroid-h2/
       (BTW, they have US distributors - I think you needn't order it
       direct form SK)
     *
   bit_user [X]
       [ ]

     Quote:

     I think this might be the time I actually take the plunge and buy a
     Raspberry Pi.
       Do it.

     Quote:

     Does anyone know what kind of OS it can support? Could I try loading
     my spare Windows 7 license on it?
       About running Windows on it, here's what their FAQ says:

     Quote:

     As of summer 2015, a version of Windows 10 is available for use on
     the Raspberry Pi 2 and 3. This is an entirely new version of the
     operating system designed exclusively for embedded use, dubbed the
     Windows 10 Internet of Things (IoT) Core. It does not include the
     user interface (shell) or the desktop operating system.
       https://www.raspberrypi.org/documentation/faqs/#pi-software

   Display more comments

   Most Popular
    1. Where to Buy the Raspberry Pi 4, Including the 4GB Model
    2. Modular 'Pi-Tops' Raise Safety Concerns After Student 'Burns'
       Finger
    3. How to Create Custom Keyboard Shortcuts for Raspberry Pi

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate next

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * Reference

             About Tom’s Hardware: Our Staff, Ratings and History

   by The Staff of Tom's Hardware September 9, 2019 at 9:42 AM

     *
     *
     *
     *
     *
     *

   [ ] (*) ( ) ( ) ( ) ( ) ( )
   Page 1:Our Mission
   Page 2:Our Team
   Page 3:Our Contributors
   Page 4:How We Test and Rate Products
   Page 5:22 Years of Tom's Hardware History
   Page 6:Europe, Sister Sites & Contact
     *

Page 1:Our Mission
     *

Page 2:Our Team
     *

Page 3:Our Contributors
     *

Page 4:How We Test and Rate Products
     *

Page 5:22 Years of Tom's Hardware History
     *

Page 6:Europe, Sister Sites & Contact

Our Mission

   Tom's Hardware is the leading destination for tech enthusiasts of all
   skill levels. Whether you're building a PC, buying a laptop or learning
   how to create robots with your kids, we've got comprehensive editorial
   resources and a vibrant expert community to help you on your journey.

Our Team

Avram Piltch (@geekinchief)

   Avram Piltch Avram PiltchAvram's been in love with PCs since he played
   original Castle Wolfenstein on an Apple II+.  Before joining Tom's
   Hardware, for 10 years, he served as Online Editorial Director for
   sister sites Tom's Guide and Laptop Mag, where he programmed the CMS
   and many of the benchmarks. When he's not editing, writing or stumbling
   around trade show halls, you'll find him building Arduino robots with
   his son and watching every single superhero show on the CW.

   Contact Avram: Email | Twitter

Matt Safford, Managing Editor (@mattsafford)

   Matt Safford Matt SaffordMatt began piling up computer experience as a
   child with his Mattel Aquarius. He built his first PC in the late 1990s
   and ventured into mild PC modding in the early 2000s. He’s spent the
   last decade covering emerging technology for Smithsonian, Popular
   Science, and Consumer Reports, while testing components and PCs for
   Computer Shopper and Digital Trends. When not writing about tech, he’s
   often walking—through the streets of New York, over the sheep-dotted
   hills of Scotland, or just at his treadmill desk at home in front of
   the 50-inch 4K HDR TV that serves as his PC monitor.

   Contact Matt: Email | Twitter

Anj Bryant, Assistant Managing Editor (@anjbryant)

   Anj Bryant Anj Bryant

   Anj provides content layout and development support, and coordinates
   editorial initiatives for the talented group of authors and editors at
   Tom's Hardware. She enjoys putting her love for technology and her past
   IT experience to good use. With a background in Enterprise software
   that started with Cybermedia she eventually caught the hardware bug and
   hasn't looked back. Outside of Tom's, she's mom to two tech-savvy girls
   who keep her busy with questions about Minecraft modding.

   Contact Anj: Email | Twitter

Scharon Harding, Senior Editor (@ScharHar)

   Scharon Harding Scharon HardingScharon helps out with any and all
   articles on Tom’s Hardware with a special affinity for laptops and
   desktops, virtual reality and monitors. She previously covered business
   technology, including hardware, software, cyber security and other IT
   happenings, at Channelnomics. When she’s not exploring all things PC,
   Scharon is usually outdoors searching for sunshine, trees and music, or
   watching movies she should’ve seen ages ago (sorry, Infinity War).

   Contact Scharon: Email | Twitter

Thomas Soderstrom, Senior Editor

   Thomas Soderstrom Thomas SoderstromThomas oversees the motherboard,
   cooling and case review teams while handling most of our DRAM reviews
   himself. After starting with hardware mod guides in our community
   forums, he freelanced for other sites before returning to Tom’s
   Hardware as motherboard editor. After he puts the mouse down for the
   day, he enjoys hiking, biking, and practical applications of skilled
   trades, often while spending time with his family.

   Contact Thomas: Email | Twitter

Paul Alcorn, Senior Editor (@PaulyAlcorn)

   Paul Alcorn Paul AlcornAs a teenager, Paul scraped up enough money to
   buy a 486-powered PC with a turbo button (yes, a turbo button). Back
   when floppies were still popular he was already chasing after the
   fastest spinners for his personal computer, which led him down the long
   and winding storage road, covering enterprise storage. His current
   focus is on consumer processors, though he still keeps a close eye on
   the latest storage news. In his spare time, you’ll find Paul hanging
   out with his kids or indulging his love of the Kansas City Chiefs and
   Royals.

   Contact Paul: Email | Twitter

Andrew E. Freedman, Editor (@FreedmanAE)

   Andrew Freedman Andrew FreedmanAndrew oversees laptop and desktop
   coverage and keeps up with the latest news in tech and gaming. His work
   has been published in Kotaku, PCMag, Complex, Tom’s Guide and Laptop
   Mag, among others. He fondly remembers his first computer: a Gateway
   that still lives in a spare room in his parents' home, albeit without
   an internet connection. When he’s not writing about tech, you can find
   him playing video games, checking social media and waiting for the next
   Marvel movie.

   Contact Andrew: Email | Twitter

Kenneth Butler, Social Media Editor (@KRichButler)

   Kenneth Butler Kenneth ButlerKenneth digs into the world of enthusiast
   PC and tech culture to help tell stories that get readers looking,
   voting, sharing, thinking and laughing on social media platforms. He’s
   worked as a fact checker, staff writer, and production director for
   Laptop Mag and Tom’s Guide. Off hours, his hobbies include early
   morning runs, writing comedy, obsessing over details in the Marvel
   Cinematic Universe and planning his ultimate Halloween costume, Major
   Payne.

   Contact Kenneth:Email | Twitter

Joe Pishgar, Community Director (@Pishgar)

   Joe Pishgar Joe PishgarJoe is our head of community, managing an
   international team of administrators, moderators, and content
   specialists who keep our communities civil, safe and helpful. Joe
   previously led community efforts in game development for Disney, Sony
   and Microsoft as a subject matter expert on user-generated content,
   engagement, and retention. When not managing the community, Joe is
   gaming, writing or hiking.

   Contact Joe: Email | Twitter

Josh Simenhoff, Community Manager

   Josh Simenhoff Josh SimenhoffJosh helps Joe manage the forums and serve
   the millions of Tom’s Hardware members across the globe. In this role,
   he helps our moderators maintain a supportive and engaging community.
   Josh also assists editorial on topics such as gaming and
   cryptocurrency. In his free time, Josh enjoys wargaming, boardgaming or
   writing.

   Contact Josh: Email

Our Contributors

     * Christian Eberle, Contributing Writer (Monitors)
     * Aris Mpitziopoulos, Contributing Writer (Power Supplies)
     * Nathaniel Mott, Contributing Writer (News)
     * Zhiye Liu, Contributing Writer (News)
     * Amy Oztan, Contributing Writer (Digital Parenting)
     * Lucian Armasu, Contributing Writer (News)
     * Kevin Carbotte, Contributing Writer (Graphics, News & VR)
     * Adam Darling, Contributing Page Setter (Layout)
     * Sean Webster, Contributing Writer (Storage)
     * Jacob Terkelsen, Contributing Writer (Motherboards)
     * Garrett Carver, Contributing Writer (Coolers)
     * Steven Lynch, Contributing Writer (Cases)
     * Matthew Connatser, Contributing Writer (News)
     * Arne Verheyde, Contributing Writer (News)
     * Allen 'Splave' Golibersuch, Contributing Writer (Overclocking)
     * Joe Shields, Contributing Writer (Motherboards)
     * LowSpecGamer Alex, Contributing Writer (Gaming)

How We Test and Rate Products

   Tom’s Hardware is renowned for its benchmark testing. We subject every
   product we review to a rigorous set of quantifiable tests based on a
   combination of homegrown, Tom’s Hardware-only benchmarks, and industry
   standard benchmarks where applicable.

   As of May 2018, all new product reviews are rated on a scale of 1 to 5,
   with 5 being the best. Each product may also receive an Editor's Choice
   badge, which designates it as the best within its niche. The ratings
   mean the following:

   5 = Practically perfect

   4.5 = Superior

   4 = Totally worth it

   3.5 = Very good

   3 = Worth considering

   2.5 = Meh

   2 = Not worth the money

   1.5 = Buy for an enemy

   1 = Fails horribly

   0.5 = Laughably bad

22 Years of Tom's Hardware History

   Tom’s Hardware has its name and roots in Dr. Thomas Pabst, who was one
   of the first people to bring technology journalism to the internet, as
   early as 1996. Back in these early days, the site was still called
   “Tom’s Hardware and Performance Guide” and its domain was
   sysdoc.pair.com, pair.com being a Pittsburgh-based hosting company.

   One of Tom’s Hardware’s journalistic milestones was Tom’s findings
   regarding the Intel Pentium III 1.13 GHz processor, which forced the
   chip company to postpone its launch by months. Since then, Tom’s
   Hardware has kept up the tradition with unrivaled scrutiny of
   technology.

   The current domain, tomshardware.com, was added on September 11, 1997,
   followed by additional language versions, including the French, German
   and Italian sites, all of which are run by independent teams. Pabst
   moved on to other pursuits in 2008, Tom's Hardware and sister
   site, Tom's Guide, became part of the Purch company in 2013 and Purch
   was purchased by Future Plc in 2018.

   Today's Tom's Hardware is more than just a site for PC builders. While
   we've maintained our rich tradition of thorough component testing and
   reviews, we've expanded our coverage to meet a broader swath of
   enthusiasts with different needs and levels of experience. If you'd
   rather buy a laptop or desktop, you're on your first PC build or you
   want to share your love of tech with your family, we're there to
   empower you with accessible editorial and a helpful, supportive
   community.

Europe, Sister Sites & Contact

Sister Sites

     * Tom's Guide: Our similarly-named sister site is designed to serve a
       slightly broader audience than Tom's Hardware. Where we serve
       enthusiasts, Tom's Guide focuses on mainstream consumers.
     * Laptop Mag: The leading destination for laptop reviews and buying
       advice.

International Tom's Hardware Sites

     * Tom's Hardware France
     * Tom's Hardware Italy
     * Tom's Hardware UK

Contact Info

   Tom's Hardware
   Future Plc
   11 West 42nd Street, 15th Floor
   New York, NY 10036

   Advertise With Us

   About the author
   The Staff of Tom's Hardware

   Most Popular
    1. Laptop Prices Will Jump 19% Due to Tariffs, Report Claims
    2. Net Neutrality Fights Back With the Save the Internet Act of 2019
    3. Microsoft Makes Gaming More Accessible With the Xbox Adaptive
       Controller (Updated)

     * Latest in
          +
        News

Laptop Prices Will Jump 19% Due to Tariffs, Report Claims
            Laptop Prices Will Jump 19% Due to Tariffs, Report Claims
          +
        Net Neutrality Fights Back With the Save the Internet Act of 2019
        News

Net Neutrality Fights Back With the Save the Internet Act of 2019
            by Nathaniel Mott Mar 6, 2019, 10:42 AM
            Microsoft Makes Gaming More Accessible With the Xbox Adaptive
            Controller (Updated)
        News

Microsoft Makes Gaming More Accessible With the Xbox Adaptive Controller
(Updated)
            by Kevin Carbotte Jul 25, 2018, 7:50 AM
     *

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

Sign Up for e-mail newsletters

   Oops, something went wrong, please try again later.

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate next

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * CPUs

     Review

      AMD Ryzen 9 3900X and Ryzen 7 3700X Review: Zen 2 and 7nm Unleashed

   by Paul Alcorn July 7, 2019 at 7:07 AM

     *
     *
     *
     *
     *
     *

   180 Comments

   [ ] (*) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
   Page 1:Into the 7nm Era
   Page 2:7nm Process, Zen 2, and the X570 Chipset
   Page 3:Ryzen 3000 IPC Measurements and Power Consumption
   Page 4:Overclocking and Ryzen Master
   Page 5:Updated Test Metholodgy and Setup
   Page 6:VRmark, 3DMark and AotS: Escalation
   Page 7:Civilization VI Graphics and AI, Dawn of War III
   Page 8:Far Cry 5 and Final Fantasy XV
   Page 9:GTA: V and Hitman 2
   Page 10:Project Cars 2, The Division 2, and World of Tanks enCore
   Page 11:Office, Web Browser, and Productivity
   Page 12:Rendering, Encoding, Compression, Encryption
   Page 13:Conclusion
     *

Page 1:Into the 7nm Era
     *

Page 2:7nm Process, Zen 2, and the X570 Chipset
     *

Page 3:Ryzen 3000 IPC Measurements and Power Consumption
     *

Page 4:Overclocking and Ryzen Master
     *

Page 5:Updated Test Metholodgy and Setup
     *

Page 6:VRmark, 3DMark and AotS: Escalation
     *

Page 7:Civilization VI Graphics and AI, Dawn of War III
     *

Page 8:Far Cry 5 and Final Fantasy XV
     *

Page 9:GTA: V and Hitman 2
     *

Page 10:Project Cars 2, The Division 2, and World of Tanks enCore
     *

Page 11:Office, Web Browser, and Productivity
     *

Page 12:Rendering, Encoding, Compression, Encryption
     *

Page 13:Conclusion

   AMD's launch of the Ryzen 3000 series processors marks an occasion that
   was nearly unthinkable a few short years ago: AMD has taken the process
   lead over Intel by fielding new 7nm processors that contain smaller and
   more densely-packed transistors than Intel's competing 14nm chips. The
   advantages of increased density come in the form of higher performance,
   better power efficiency, more cores, and more cache packed into a
   smaller area than the first-gen Ryzen models, all of which makes
   third-gen Ryzen a potent adversary for Intel both on the desktop and in
   the data center.

   AMD paved the way for the 'Matisse' Ryzen 3000 series several years ago
   when it unveiled the revolutionary chiplet-based Zen microarchitecture.
   At the time, AMD laid out a roadmap that included a steady cadence of
   tick-tock-like updates interspersed with new revisions of the scalable
   microarchitecture. After the company's sophomore effort with the
   second-gen Ryzen processors, which featured a faster process paired
   with the same first-gen Zen design, the company is plowing forward with
   its Zen 2 architecture that AMD says offers up to 15% more instructions
   per cycle (IPC). Paired with the advantages of the 7nm process and more
   cores, not to mention AMD's trailblazing of the PCIe 4.0 interface on
   desktop platforms, the Ryzen 3000 chips promise an explosive step
   forward in performance.

   AMD's first chips to come packing TSMC's 7nm process span the entire
   range of the mainstream desktop stack, but push core counts up from
   eight cores to 12 cores and 24 threads with the Ryzen 9 3900X we have
   in the lab today, upsetting the status quo and bringing mainstream
   platforms into what used to be the realm of the pricey high end
   desktop. If you're looking for something even beefier, AMD also has the
   16-core 32-thread Ryzen 9 3950X coming in September.
                               SEP (USD)
   Cores / Threads
   TDP (Watts)
   Base / Boost Frequency (GHz)
   L3 Cache (MB)
   PCIe 4.0 Lanes
   Ryzen 9 3950X
   $749
   16 / 32
   105W
   3.5 / 4.7
   64
   24
   Ryzen 9 3900X
   $499
   12 / 24
   105W
   3.8 / 4.6
   64
   24
   Ryzen 7 3800X
   $399
   8 / 16
   105W
   3.9 / 4.5
   32
   24
   Ryzen 7 3700X
   $329
   8 / 16
   65W
   3.6 / 4.4
   32
   24
   Ryzen 5 3600X
   $249
   6 / 12
   95W
   3.8 / 4.4
   32
   24
   Ryzen 5 3600
   $199
   6 / 12
   65W
   3.6 / 4.2
   32
   24

   Aside from those halo parts, AMD also has plenty of models that address
   the bulk of casual users, gamers, and enthusiasts, like the eight-core
   16-thread Ryzen 7 3700X we also have in the lab, and a lineup of
   six-core 12-thread Ryzen 5 models.

   AMD is staying true to its enthusiast-friendly roots: Although you can
   pair the Ryzen 3000 chips with the new X570 chipset, they are also
   backward compatible with most AM4 socket motherboards. All of the
   models also come with beefy stock coolers, solder thermal interface
   material between the heat spreader and die to improve thermal transfer,
   and unlocked multipliers for easy overclocking. AMD even added support
   for auto-overclocking for mainstream processors. Pair that with the
   lower per-core pricing and the debut of the PCIe 4.0 interface for the
   desktop, and the Ryzen 3000 series appears to be a potent force.

   AMD's ability to deliver on its optimistic roadmap in the waning light
   of Moore's Law is truly impressive, especially as we have become
   accustomed to never-ending cadences of incremental updates. But at the
   end of the day it all boils down to real-world performance. Let's see
   what the Ryzen 3000 series has in store.

Ryzen 9 3900X

                         Process
   SEP / RCP (USD)
   Cores / Threads
   TDP (Watts)
   Base Frequency (GHz)
   L3 Cache (MB)
   PCIe Lanes
   Memory Support
   iGPU
   Price Per Thread
   Intel Core i9-9920X
   14nm
   $1199
   12 / 24
   165W
   3.5 / 4.4
   19.25
   16 Gen3
   Quad-Channel DDR4-2666
   No
   $49.95
   Ryzen 9 3900X
   7nm
   $499
   12 / 24
   105W
   3.8 / 4.6
   64
   24 Gen4
   Dual-Channel DDR4-3200
   No
   $20.79
   Threadripper 2920X
   12nm
   $625
   12 / 24
   180W
   3.5 / 4.3
   32
   64 Gen3
   Quad-Channel DDR4-2933
   No
   $26.04
   Core i9-9900K
   14nm
   $488
   8 / 16
   95W
   3.6 / 5.0
   16
   16 Gen3
   Dual-Channel DDR4-2666
   Yes
   $61

   Make no mistake - from a core count perspective, the $500 12-core
   24-thread Ryzen 9 3900X really has no comparison on the mainstream
   desktop. We have to reach up to Intel's high end desktop (HEDT)
   platform to find a fair comparison based on core counts. Intel's Core
   i9-9920X slots in with 12 cores and 24-threads for $1,199, a $700
   premium over AMD's Ryzen 9 3900X.

   There's no doubt the 3900X also blurs the line between the AMD's own
   HEDT Threadripper platform and the mainstream desktop: The Threadripper
   1920X is AMD's only core-comparable processor. That processors has its
   own advantages, like access to 64 lanes of PCIe 3.0, and like the
   -9920X, it supports quad-channel memory. But both company's HEDT chips
   are much more expensive than the 3900X and require pricey HEDT
   motherboards.

   Back in the familiar realm of the mainstream desktop, Intel's $488 Core
   i9-9900K serves as the 3900X's primary competitor. The -9900K comes
   with four fewer cores and eight fewer threads than the 3900X, marking a
   distinct difference in the price you pay per thread, but the -9900K
   does hold the clock speed advantage. AMD hopes to offset that advantage
   with its increased IPC throughput and the 3900X also supports the PCIe
   4.0 interface with twice the bandwidth of the -9900K's PCIe 3.0
   interface. You'll also notice the Core i9-9900K, known for its high
   power consumption and intense heat generation, has a lower 95W TDP than
   the 3900X's 105W rating. We can chalk that up to different measurement
   techniques. We'll provide extensive power and efficiency testing on the
   following pages to get a more accurate picture of actual power
   consumption.

   (*) ( ) ( )
     * 09
       09
     * 07
       07
     * 02
       02

   (*) ( ) ( )
     * 09

09
     * 07

07
     * 02

02

   As pictured here, the 39000X comes packing AMD's Zen 2
   microarchitecture spread across two small 7nm eight-core compute
   chiplets tied together with the Infinity Fabric interconnect via a
   larger 12nm I/O die (IOD). Each small 3900X compute chiplet, referred
   to as a CCD (Core Chiplet Die), comes with eight physical cores spread
   across two four-core Core Complexes (CCXes). Each CCX has 16MB of
   shared L3 cache, totaling 32MB of L3 cache per CCD, and 64MB of total
   cache for the entire chip. AMD disables two cores per CCD to create the
   12-core 3900X.

   Each 7nm CCD measures ~74mm^2 and has 3.9 billion transistors, while
   the 12nm IOD is ~125mm^2 and has 2.09 billion transistors. That means
   the 3900X comes with ~273mm^2 of silicon that sports ~9.89 billion
   transistors.

   The 3900X's larger cache comes courtesy of the denser 7nm manufacturing
   process, but it does have a slightly higher latency (on the order of
   "five or six" clocks) than the 16MB of L3 cache found on
   previous-generation models. However, the increased capacity allows the
   processor to store more data closer to the execution cores, thus
   increasing cache hit rates that ultimately yield more performance. AMD
   also decreased the size of its L1 instruction cache from 64KB with the
   first-gen Zen processors to 32KB for Zen 2 chips. This allowed the
   company to expand its microop cache, and paired with changing the L1
   instruction cache from 4-way to 8-way associativity, AMD feels this
   provides a more balanced approach to its cache subsystem.

   The -9900K's 16MB of L3 cache pales in comparison from a capacity
   standpoint, but cache bandwidth and latency are more important metrics.
   We'll put hard numbers behind the differences on the following pages.

   As a sidenote, AMD now calls its combined L2+L3 cache "GameCache" to
   highlight to casual consumers the importance of cache to gaming
   performance, but we'll stick with the established terms.

Ryzen 7 3700X

   The eight-core 16-thread Ryzen 7 3700X slots in at $329 and comes with
   a 65W TDP rating, which is significantly lower than the competing Core
   i7-9700K's 95W rating. You'll notice that AMD has maintained similar
   price points for the new models compared to the previous-gen Ryzen 7's,
   but we caution that pricing is a moving target for the last-gen chips.
                       Process
   SEP / RCP (USD)
   Cores / Threads
   TDP (Watts)
   Base Frequency (GHz)
   Total Cache (MB)
   PCIe Lanes
   iGPU
   Price Per Thread
   Core i9-9900K
   14nm
   $488
   8 / 16
   95w
   3.6 / 5.0
   16
   16 Gen3
   Yes
   $30.05
   Ryzen 7 3800X
   7nm
   $399
   8 / 16
   105W
   3.9 / 4.5
   32
   24 Gen4
   No
   $24.94
   Core i9-9700K
   14nm
   $374
   8 / 8
   95W
   3.6 / 4.9
   12
   16 Gen3
   Yes
   $46.75
   Ryzen 7 2700X
   12nm
   $329
   8 / 16
   105W
   3.7 / 4.3
   16
   20 Gen3
   No
   $20.56
   Ryzen 7 3700X
   7nm
   $329
   8 / 16
   65W
   3.6 / 4.4
   32
   24 Gen4
   No
   $20.56
   Core i7-9700
   14nm
   $323
   8 / 8
   95W
   3.6 / 4.9
   12
   16 Gen3
   Yes
   $40.38

   Although third-gen Ryzen pricing is close to the current-gen processors
   on sale, this is far lower than the per-core pricing at the launch of
   the previous gen. Normalize the numbers to price-per-thread, and its
   clear AMD maintains a pricing advantage over Intel's lineup. But
   performance varies based on architecture, so the price-to-performance
   ratio is where the rubber meets the road.

   (*) ( )
     * 10
       10
     * 01
       01

   (*) ( )
     * 10

10
     * 01

01

   The Ryzen 7 3700X features a single CCD with all eight active cores
   connected to the I/O die, highlighting that the company's Zen 2
   architecture is inherently scalable. Threadripper processors also come
   with varying numbers of compute dies, but substitute in 'dummy' dies to
   ensure structural rigidity and prevent crushing the integrated heat
   spreader (IHS) when you tighten down your cooler. The smaller surface
   area of the 2700X's IHS doesn't require a dummy die, so this pad is
   simply left unoccupied.

   AMD hasn't sampled the Ryzen 7 3800X yet, which features a higher 105W
   rating and 3.9 / 4.5 GHz base/boost clocks, which is higher than the
   Ryzen 7 3700X's 3.6 / 4.4 GHz base/boost frequency. It also looks like
   a compelling part, so look to these pages for a review soon.

   Credit: AMD Credit: AMD

   Both the Ryzen 9 3900X and the Ryzen 7 3700X come with the bundled
   Wraith Prism RGB cooler that features four direct-contact copper heat
   pipes, three independent RGB zones, switchable fan profiles, and a 39
   dB(A) noise rating. The cooler is rated to dissipate 116W of waste heat
   in "L" mode (2800 RPM) and 124W in "H" mode (3600 RPM). Cooler Master
   manufactures the heat sink/fan, while AMD provides software for
   controlling the lighting and fan profiles. Company representatives
   claim the cooler represents a roughly $43 value, and that it also
   allows for some overclocking headroom. Intel's K-series models, in
   contrast, don't come with a bundled cooler.

Memory Subsystem and Overclocking, Infinity Fabric

   Ryzen 3000 chips support dual-channel DDR4-3200, a step up from the
   previous-gen's support for DDR4-2966. That should boost performance
   significantly because the Zen 2 microarchitecture, like its
   predecessor, benefits heavily from increased memory performance
   (particularly in gaming). Credit: AMD Credit: AMD

   AMD's new Zen 2 microarchitecture uses a centralized memory controller
   on the I/O die, which helps ensure consistent memory latency in the
   multi-die Ryzen 9 models. It also improves cache access latency. AMD
   has also overhauled the Infinity Fabric, doubling its throughput by
   increasing the previous-gen 256-bit interconnect to 512-bit, which
   facilitates access to memory and enables the PCIe 4.0 interface. AMD
   also instituted more fine-grained Infinity Fabric quality of service
   controls and claims to have reduced the amount of energy required to
   transfer a bit by 27%.  Credit: AMD Credit: AMD

   AMD has improved memory overclocking substantially, partly due to
   decoupling the Infinity Fabric from the memory clock. AMD's first-gen
   Ryzen processors had plenty of difficulties with memory overclocking
   when they first launched, but AMD has addressed those concerns with the
   second-gen products and has even demoed an air-cooled Ryzen platform
   running at DDR4-5100. We also didn't encounter any issues during our
   testing.

   As with previous-gen Ryzen, memory overclocking confers big performance
   speedups for gaming. To sidestep the Infinity Fabric's maximum
   frequency of 2,000 MHz, which effectively constrains memory
   overclocking, AMD allows users to separate the memory and Infinity
   Fabric clock dependencies. The domains remain tied together at a 1:1
   ratio up to DDR4-3600, but run at a 2:1 ratio beyond that transfer
   rate. This setting, which is also user-adjustable in the BIOS, improves
   memory bandwidth but comes with a latency penalty (~9ns). Tuners can
   also adjust the Infinity Fabric clock (fclk) in 33Mhz increments to get
   an extra kicker during overclocking. AMD says that the
   price/performance sweetspot will be around DDR4-3600.

   As before, AMD supports up to 128GB of RAM and enables ECC support, but
   AMD leaves qualification and enablement of the feature up to
   motherboard vendors.
   DIMM Config
   Memory Ranks
   Official Supported Transfer Rate (MT/s)
   2 of 2
   Single
                                          DDR4-3200
                                          2 of 4
   DDR4-3200
   4 of 4
   DDR4-2933
   2 of 2
   Dual
                                          DDR4-3200
                                          2 of 4
   DDR4-3200
   4 of 4
   DDR4-2667

   As seen with the first-gen Zen chips, AMD's official supported memory
   data transfer rates vary based on the type of DIMM (single rank or dual
   rank) and the number of populated channels, as outlined above.

PCIe 4.0 Comes to the Desktop

   Ryzen 3000 processors support the PCIe interface on X570 motherboards,
   and while the chips will drop into some previous-gen AM4 motherboards,
   the processor will downshift into PCIe 3.0 on those platforms. AMD has
   also infused the new technology into its "Navi" Radeon 5000 series GPUs
   and worked with storage vendors to assure a supply of speedy new PCIe
   4.0 SSDs. We recently had the opportunity to take an early look at PCIe
   4.0 SSD performance, which you can see here.
               Bandwidth
   Gigatransfer
   Frequency
   Encoding
   PCIe 3.0
   32 GB/s
   8 GT/s
   8.0 GHz
   128b/130b
   PCIe 4.0
   128 GB/s
   16 GT/s
   32.0 GHz
   128b/130b

   PCIe 4.0 provides yet another advantage for performance seekers,
   particularly in the content creation realm, over Intel's platform, but
   it doesn't materially impact gaming performance (at least not yet). The
   new interface also comes at the cost of higher pricing for
   X570-equipped motherboards due to tighter signalling requirements.
   Those prices could recede over time as the pricing of the PCIe 4.0
   component ecosystem, like switches and redrivers, benefit from
   economies of scale, but AMD has wisely encouraged its partners to
   continue offering the current-gen X470 motherboards that will now serve
   as a lower tier of motherboards.

   AMD's new Ryzen 3000 series lineup is fully compatible with existing
   X470 motherboards and will operate at their full performance on the
   previous-gen boards, albeit at the loss of PCIe 4.0 connectivity. That
   shouldn't be too much of a concern for users without PCIe 4.0 devices
   or SSD RAID storage arrays that hang off the chipset. Fast storage
   arrays will certainly benefit from the faster PCIe 4.0 connection
   between the chipset and processor, though.

Ryzen-Specific Windows 10 Scheduler Updates

   AMD worked with Microsoft to deliver on a much needed feature: A
   Ryzen-aware scheduler. The new scheduler arrived with the Windows 10
   May update and benefits both current-gen and previous-gen Ryzen models
   (Threadripper and Ryzen 3000 processors).

   (*) ( )
     * David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-009
       David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-009
     * David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-008
       David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-008

   (*) ( )
     * David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-009

David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-009
     * David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-008

David_McAfee-Next_Horizon_Gaming-3rd_Gen_Ryzen_06092019-page-008

   The new scheduler pins threads within a single CCX (the four-core
   clusters inside each CCD) before scheduling threads to other CCXes.
   This approach reduces latency during thread synchronizations or
   frequent cache accesses, thus improving performance for all existing
   Ryzen processors. AMD says the feature doesn't benefit all
   applications, but can result in significant performance improvements in
   those that do.

   AMD also introduced its Collaborative Power Performance Control 2
   (CPPC2) feature, which is a software feature that manipulates Ryzen
   3000's power states from within the operating system. This is similar
   to Intel's Speed Shift technology and reduces power state transition
   latency from 30ns to 1ns, which ultimately saves power and boosts
   efficiency. The feature comes enabled in the latest AMD chipset drivers
   and the Windows 10 May update (and newer).

   As before, these mainstream models don't come with integrated graphics,
   meaning you'll need a discrete GPU.

   MORE: Best CPUs

   MORE: Intel & AMD Processor Hierarchy

   MORE: All CPUs Content

   IFRAME: https://content.jwplatform.com/players/zYBgfFoA.html

   Next

   Summary
    1. Into the 7nm Era
    2. 7nm Process, Zen 2, and the X570 Chipset
    3. Ryzen 3000 IPC Measurements and Power Consumption
    4. Overclocking and Ryzen Master
    5. Updated Test Metholodgy and Setup
    6. VRmark, 3DMark and AotS: Escalation
    7. Civilization VI Graphics and AI, Dawn of War III
    8. Far Cry 5 and Final Fantasy XV
    9. GTA: V and Hitman 2
   10. Project Cars 2, The Division 2, and World of Tanks enCore
   11. Office, Web Browser, and Productivity
   12. Rendering, Encoding, Compression, Encryption
   13. Conclusion

   About the author
   Paul Alcorn @PaulyAlcorn

   Paul Alcorn is a Senior Editor for Tom's Hardware US. He writes news
   and reviews on CPUs, storage and enterprise hardware.
   [javascript]
   Read more
     * CPUs
     * AMD
     * Components

   180 comments
   Comment from the forums
       Your comment
     *
   Isokolon [X]
       [ ]
       too bad there wasn't a 3800X included, would be interesting to see
       if the price tag for the 3800X over the 3700X is indeed worth it
     *
   feelinfroggy777 [X]
       [ ]
       Nice for worksation task, but disappointing it basically ties Intel
       in gaming if not just a tad behind.
     *
   salgado18 [X]
       [ ]

     Quote:

     Nice for worksation task, but disappointing it basically ties Intel
     in gaming if not just a tad behind.
       It tied to Intel for less money, with a great bundled cooler, and a
       cheaper platform (edit: and less power too). Also, unless you use a
       144Hz monitor, the diference is purely synthetic. Did you expect it
       to be way faster than a 5 GHz Intel magically?
     *
   Phaaze88 [X]
       [ ]

     Quote:

     Thank you so much and to be honest the only reason why I bought the
     750tx Corsair psu is because it was 30$, can I ask you if the gtx770
     2gb will run battlefield 1 on high?

     Quote:

     too bad there wasn't a 3800X included, would be interesting to see
     if the price tag for the 3800X over the 3700X is indeed worth it
       I can't imagine it would be.
       If looking at the 3800x as a binned 3700x - that's basically what
       it would be - grab one if it goes on sale closer to the 3700x's
       price.
       These chips don't overclock any better than their predecessors,
       which wasn't good to begin with, so whatever extra clocks you get
       with a 3800x will hardly be noticeable and won't be worth a $50+
       price increase over 3700x.
     *
   velocityg4 [X]
       [ ]
       The overclocking results were disappointing. 4.1Ghz max on all
       cores. Given that the 3950x does a 4.7Ghz single core turbo boost
       and the 3900x does 4.6Hz single core turbo boost. I'd have assumed
       any of the Ryzen 3000 would OC to 4.6/4.7Ghz on all cores with
       decent air/water cooling.
       Power consumption: AIDA 64 seems to punish AMD a lot more than
       Intel. When you were using Prime95 Intel was punished a lot more.
       It seems the switch from Prime95 to AIDA 64 gives Intel an unfair
       advantage in the stress test power consumption test. While Prime95
       gave AMD an unfair advantage. I'd suggest using both in reviews or
       find another torture test that will fully punish both AMD and Intel
       for a max load test. With such wild variation. I can't see how
       either is an accurate measure of a CPU under full load.
       Example Review:
       https://www.tomshardware.com/reviews/intel-core-i9-9900k-9th-gen-cp
       u,5847-11.html
       The Intel i9-9900K hit 204.6W in your old reviews stress test. This
       time it is only 113W.
       The AMD Ryzen 2700x hit 104.7W in your old review. Now it is 133W.
     *
   feelinfroggy777 [X]
       [ ]

     Quote:

     It tied to Intel for less money, with a great bundled cooler, and a
     cheaper platform (edit: and less power too). Also, unless you use a
     144Hz monitor, the diference is purely synthetic. Did you expect it
     to be way faster than a 5 GHz Intel magically?
       It did tie Intel in gaming. It tied basically the same Intel CPUs
       that have been on the market since 2015 with Skylake. We are in the
       back half of 2019 and we see the same gaming performance that we
       had in 2015 mainstream CPUs.
       We know AMD is cheaper and comparing clockspeeds against different
       architectures between Intel and AMD is silly. But it would be nice
       to see some tangible improvement regarding fps with CPUs. The GPU
       still remains king when it comes to a quality gaming build.
     *
   delaro [X]
       [ ]
       I've seen reviews from 5 different sites and the conclusions bounce
       all over the place, which makes me think there is much to do on the
       software optimization side. :unsure: I was expecting gaming FPS to
       not change all that much with many of the titles being tested have
       partnered or optimized around Intel.
     *
   jimmysmitty [X]
       [ ]

     Quote:

     Nice for worksation task, but disappointing it basically ties Intel
     in gaming if not just a tad behind.
       Minus the ability to overclock yes tied. Most people who buy the
       9900K will not be buying it to leave it stock.

     Quote:

     The overclocking results were disappointing. 4.1Ghz max on all
     cores. Given that the 3950x does a 4.7Ghz single core turbo boost
     and the 3900x does 4.6Hz single core turbo boost. I'd have assumed
     any of the Ryzen 3000 would OC to 4.6/4.7Ghz on all cores with
     decent air/water cooling. Power consumption: AIDA 64 seems to punish
     AMD a lot more than Intel. When you were using Prime95 Intel was
     punished a lot more. It seems the switch from Prime95 to AIDA 64
     gives Intel an unfair advantage in the stress test power consumption
     test. While Prime95 gave AMD an unfair advantage. I'd suggest using
     both in reviews or find another torture test that will fully punish
     both AMD and Intel for a max load test. With such wild variation. I
     can't see how either is an accurate measure of a CPU under full
     load. Example Review:
     https://www.tomshardware.com/reviews/intel-core-i9-9900k-9th-gen-cpu
     ,5847-11.html The Intel i9-9900K hit 204.6W in your old reviews
     stress test. This time it is only 113W. The AMD Ryzen 2700x hit
     104.7W in your old review. Now it is 133W.
       Its what I wanted to know. Ryzen has always been pushed to the
       limit in terms of clock speed and Zen 2 is no different it seems.
       Little to no headroom. AnandTech was able to get it to 4.3GHz all
       core but with manual OCing it seems to disable boost clocking which
       in turn cuts 300MHz from single core performance.
       As for the power consumption, the differences are probably what
       they prioritize. I know Prime 95 heavily uses AVX which is a power
       hog. Not as sure on AIDA 64 since I never used it. I always use
       Prime 95 and IBT for stability.

     Quote:

     It did tie Intel in gaming. It tied basically the same Intel CPUs
     that have been on the market since 2015 with Skylake. We are in the
     back half of 2019 and we see the same gaming performance that we had
     in 2015 mainstream CPUs. We know AMD is cheaper and comparing
     clockspeeds against different architectures between Intel and AMD is
     silly. But it would be nice to see some tangible improvement
     regarding fps with CPUs. The GPU still remains king when it comes to
     a quality gaming build.
       Its not silly to compare clock speeds as those can be advantages.
       Intel still clearly has a clock speed advantage and that advantage
       will keep them priced higher. We might see some drops but I doubt
       we will see enough to make it feel like Athlon 64 again.
       As much crap as people give Intel for getting stuck at 14nm I have
       to give them props for having a 5 year old process tech beat modern
       process tech, especially one that's supposed to be "half" the size.
       I know its not quite as most 7nms out there are still less dense
       than Intels initial 10nm plans but still it goes to show that the
       nm part has become pointless and a marketing gimmick more than
       anything.
       The only thing a CPU matters gaming wise is how long it will last
       before it will bottleneck the GPU. While its still early the clock
       speed and overclocking advantage Intel has might make their CPUs
       last longer in gaming than Zen 2. Only time will tell but maybe AMD
       will get a better process tech in a few years and finally compete
       like the old days.
     *
   martinch [X]
       [ ]

     Quote:

     Its not silly to compare clock speeds [between AMD and Intel] as
     those can be advantages.
       Unless you're trying to give an indication of "performance-per-MHz"
       of varying architectures, yes, comparing clock speeds between
       differing architectures is a fundamentally invalid comparison (it's
       also not exactly an accurate predictor of per-core performance).
     *
   feelinfroggy777 [X]
       [ ]

     Quote:

     Its not silly to compare clock speeds as those can be advantages.
     Intel still clearly has a clock speed advantage and that advantage
     will keep them priced higher. We might see some drops but I doubt we
     will see enough to make it feel like Athlon 64 again.
       Clockspeeds between AMD and Intel are not apples to apples.
       Bulldozer hit 5ghz and it was a terrible CPU. Just because it could
       hit 5ghz, did not make a good chip.
     *
   InvalidError [X]
       [ ]

     Quote:

     Unless you're trying to give an indication of "performance-per-MHz"
     of varying architectures, yes, comparing clock speeds between
     differing architectures is a fundamentally invalid comparison (it's
     also not exactly an accurate predictor of per-core performance).
       Even real-world aren't reliable indicators unless you can find
       benchmarks of the specific software and task within that software
       you are interested in. Anything else is a general indicator at
       best.
       So, I predicted all-core overclocks on par with XFR boost at best
       and it seems Tom's review samples get nowhere near that on
       practical cooling. Looks like AMD is doing a pretty good job of
       leaving little to nothing for practical overclockers to do with the
       X-chips already squeezing everything they are worth out of
       themselves with little more than a few checkboxes.
     *
   feelinfroggy777 [X]
       [ ]

     Quote:

     Even real-world aren't reliable indicators unless you can find
     benchmarks of the specific software and task within that software
     you are interested in. Anything else is a general indicator at best.
     So, I predicted all-core overclocks on par with XFR boost at best
     and it seems Tom's review samples get nowhere near that on practical
     cooling. Looks like AMD is doing a pretty good job of leaving little
     to nothing for practical overclockers to do with the X-chips already
     squeezing everything they are worth out of themselves with little
     more than a few checkboxes.
       They pretty much overclock themselves these days.
     *
   yeti_yeti [X]
       [ ]
       I think this Ryzen lineup is great and delivers awesome
       performance, especially in professional applications. Some people
       seem to be disappointed, that Ryzen 7/9 didn't beat their Intel
       counterparts in gaming, which was something a lot of
       people(myself included) didn't think was going to happen anyway.
       However, I do think that AMD could have been a bit more transparent
       and modest, when showcasing their own benchmarks of their products
       beating or matching the competing Intel chips, which would result
       in less people being let down.
       Other than that, I think AMD did great in both GPU and CPU
       department and look forward to more products from them.
     *
   kinggremlin [X]
       [ ]
       Good effort by AMD, but still doesn't move the performance bar any
       for home users. For anyone who has been waiting 4 years to get a
       5960x for less than $500, you've now got your CPU. For anyone
       looking for something tangibly faster than Intel's Haswell
       generation CPU's, you're still waiting.
     *
   lxtbell2 [X]
       [ ]

     Quote:

     Good effort by AMD, but still doesn't move the performance bar any
     for home users. For anyone who has been waiting 4 years to get a
     5960x for less than $500, you've now got your CPU. For anyone
     looking for something tangibly faster than Intel's Haswell
     generation CPU's, you're still waiting.
       Zen 2 seems to have some 30% better power efficiency than Intel
       processors, and that's huge. Power efficiency is everything for
       SFF, laptops, HTPC, fanless builds etc etc. So I would say it can
       potentially move the performance bar a lot for those.
       Regarding absolute performance, the 12-core destroys all Haswell
       desktop CPUs in productivity, and I can't care less about "tangibly
       faster" in gaming as long as my graphics card is the bottleneck in
       1440p+.
     *
   InvalidError [X]
       [ ]

     Quote:

     Zen 2 seems to have some 30% better power efficiency than Intel
     processors, and that's huge. Power efficiency is everything for SFF,
     laptops, HTPC, fanless builds etc etc. So I would say it can
     potentially move the performance bar a lot for those.
       For the lower-power segments, Intel has Icelake which is more than
       a match for Zen 2 CPU-wise.
     *
   salgado18 [X]
       [ ]
       About bad overclock numbers, my 2 cents:
       1. Nobody had the time to properly test them. I believe Tom's had
       like two or so days to run dozens of tests on two brand new
       processors, and still produce the article. Also, these are early
       BIOS and drivers. It could improve with time.
       2. The 3700x reaches a max of 4.3, but the 3950x reaches 4.6. Maybe
       it's inconsistent because of such a new process, after all, these
       are the first 7nm cpus produced, right? And lots of changes, some
       sillicon revisions could help a lot.
     *
   kinggremlin [X]
       [ ]

     Quote:

     Regarding absolute performance, the 12-core destroys all Haswell
     desktop CPUs in productivity, and I can't care less about "tangibly
     faster" in gaming as long as my graphics card is the bottleneck in
     1440p+.
       A 12 core CPU will not be any faster than an 8 core for home users.
       The software used does not scale with that many cores. If it was so
       easy to make typical software scale with more cores, programmers
       would have already done it. Most mainstream software is not capable
       of being highly parallelized like a graphics workload, so this
       problem isn't going to get fixed without a completely different
       type of computing platform. Adding more cores doesn't make a PC
       faster when what you really need is faster cores.
     *
   kinggremlin [X]
       [ ]

     Quote:

     About bad overclock numbers, my 2 cents: 1. Nobody had the time to
     properly test them. I believe Tom's had like two or so days to run
     dozens of tests on two brand new processors, and still produce the
     article. Also, these are early BIOS and drivers. It could improve
     with time. 2. The 3700x reaches a max of 4.3, but the 3950x reaches
     4.6. Maybe it's inconsistent because of such a new process, after
     all, these are the first 7nm cpus produced, right? And lots of
     changes, some sillicon revisions could help a lot.
       May want to watch this link from world reknown overclocker de8auer:
       [MEDIA=youtube]WXbCdGENp5I[/MEDIA]
       View: https://www.youtube.com/watch?v=WXbCdGENp5I
       He has 10 CPU's (mix of 6/8/12 cores). Forget overclocking, he
       couldn't get any of his 8 or 12 core CPU's to hit AMD's advertised
       max clocks (4.5 for 3800x, 4.6 for 3900x) even with a custom water
       cooling loop. There were leaks before launch that these chips would
       hit 5GHz. De8auer says in this video to forget that, he has chips
       that wouldn't hit 5GHz using liquid nitrogen.
     *
   ingtar33 [X]
       [ ]

     Quote:

     I've seen reviews from 5 different sites and the conclusions bounce
     all over the place, which makes me think there is much to do on the
     software optimization side. :unsure: I was expecting gaming FPS to
     not change all that much with many of the titles being tested have
     partnered or optimized around Intel.
       Linus had a possible cause for this. He noted the same thing (with
       results all over the place) and realized it's a result of the
       windows scheduler bouncing heavy tasks from core to core without
       consideration for boost clocks. when they locked the application to
       the cores that were boosting the results for the AMD cpu shoot went
       up and became much more consistent. He believes once the scheduling
       issue in windows is worked out the Ryzen bench results will
       probably increase noticeably.
     *
   Ncogneto [X]
       [ ]

     Quote:

     Good effort by AMD, but still doesn't move the performance bar any
     for home users. For anyone who has been waiting 4 years to get a
     5960x for less than $500, you've now got your CPU. For anyone
     looking for something tangibly faster than Intel's Haswell
     generation CPU's, you're still waiting.
       One of these days you might actually understand what matters, and
       what doesn't. AMD delivered on performance that matters, not some
       silly gaming benchmark(s) that is Intel's last gasp as it hangs on
       by it's fingernails. Performance that you can only see if you
       couple either platform with a $1200 GPU no less, and even then, you
       can't actually notice a difference while playing your game (155fps
       vs 135 FPS, who gives a rats a**). Show me a game in which the
       Intel CPU is performing at a level that is visually noticeably
       better (without looking at some silly fps counter).
       On every other front, the Intel CPU gets stomped in the dirt, all
       while costing more and consuming more power, and needing an exotic
       cooling solution as well.
       Easy win for AMD, for anyone other than those only interested in
       nothing but pure fps bragging rights, which is so High School level
       silly.
       Spectre/meltdown/zombieload......disable HT, etc etc etc. Every
       month brings another Intel bug.
       They should be ashamed.
     *
   fr3sgnint [X]
       [ ]
       Shame we didn't see any CAD / Solidworks ETC type benchmarks on the
       productivity side.
     *
   acquinn [X]
       [ ]
       So impressive. AMD hasn't been on my radar since the Athlon days.
       Competition is good! I wonder how long it'll take Intel to get to
       7NM. Also, where's Intel's response hardware-wise? I mean isn't the
       9900K almost a year old at this point? And it's still beating AMD
       in a lot of areas.
     *
   kinggremlin [X]
       [ ]

     Quote:

     Shame we didn't see any CAD / Solidworks ETC type benchmarks on the
     productivity side.
       You don't have to. Both are single threaded applications with
       random addons that may support multithreading, so you know Intel is
       going to win, just like most stuff from Adobe. People like Ncogneto
       don't seem to grasp how much commonly used software doesn't benefit
       at all from increased core counts beyond a few.

   Display more comments

   Most Popular
    1. India's First CPUs Are Ready for App Development
    2. PCPartPicker Reveals AMD Ryzen 3000 CPU Packaging
    3. Silicon Lottery to Bin and Sell Ryzen 3000 CPUs

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate next

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * Graphics

     Review

  AMD Radeon RX 5700 XT and Radeon RX 5700 Review: New Prices Keep Navi In The
                                      Game

   by Chris Angelini July 7, 2019 at 7:00 AM

     *
     *
     *
     *
     *
     *

   22 Comments

   [ ] (*) ( ) ( ) ( ) ( ) ( ) ( )
   Page 1:AMD Radeon RX 5700 XT and Radeon RX 5700 Review
   Page 2:Performance Results: 2560 x 1440
   Page 3:Performance Results: 3840 x 2160
   Page 4:Power Consumption: Radeon RX 5700
   Page 5:Power Consumption: Radeon RX 5700 XT
   Page 6:Fan Speeds, Clock Rates, and Temperatures
   Page 7:Conclusion
     *

Page 1:AMD Radeon RX 5700 XT and Radeon RX 5700 Review
     *

Page 2:Performance Results: 2560 x 1440
     *

Page 3:Performance Results: 3840 x 2160
     *

Page 4:Power Consumption: Radeon RX 5700
     *

Page 5:Power Consumption: Radeon RX 5700 XT
     *

Page 6:Fan Speeds, Clock Rates, and Temperatures
     *

Page 7:Conclusion

   AMD’s propensity for slowly dribbling out information about upcoming
   products keeps our news desk buzzing but also serves to illustrate the
   company’s play book months in advance. That drawn-out tease definitely
   worked against AMD last generation. By the time Radeon RX Vega landed
   in our lab, expectations had boiled over beyond what the card could
   deliver, particularly at its cryptocurrency-affected prices.

   This time around, AMD used the Electronic Entertainment Expo in Los
   Angeles as the launch pad for a Navi deets, pouring out all the
   architectural details it was willing to divulge with less than 24 hours
   to write it up, then complicating matters by prohibiting audio or video
   recordings of the technical deep dives. But even rushing the
   particulars for reasons unknown couldn’t stop Nvidia from squeezing in
   a Turing refresh prior to Radeon RX 5700 and Radeon RX 5700 XT
   availability.

   The two Navi-based cards originally took aim at GeForce RTX 2070 and
   GeForce RTX 2060. Just days before reviews were scheduled to go live,
   however, AMD found itself staring down the barrel of GeForce RTX 2060,
   2060 Super, and 2070 Super. Its hardware was already baked, so the
   company turned another dial to stay competitive: it dropped the price
   of Radeon RX 5700 XT to $400, matching GeForce RTX 2060 Super, and
   lowered Radeon RX 5700 to $350, pulling up alongside GeForce RTX 2060.
   AMD is clearly feeling good enough about its performance story to go up
   against the GeForces at identical pricing. Is that confidence
   well-placed or is AMD underestimating the appeal of real-time ray
   tracing support?

   (*) ( ) [ ] [ ]
     *
   AMD Radeon RX 5700 XT
       [javascript]
       3.5/5
       Review
       $400Amazon
     *
   AMD Radeon RX 5700
       [javascript]
       4/5
       Review
       $350Amazon

     * AMD Radeon RX 5700 XT AMD Radeon RX 5700

A Navi Recap

   Check out AMD Announces Radeon RX 5700 XT and RX 5700: Navi Takes the
   Fight to GeForce RTX for a quick recap of Navi’s architectural basics,
   including its redesigned Compute Units and cache hierarchy changes.

Meet Radeon RX 5700 XT

   Both AMD Radeon RX 5700-series cards are based on the same Navi GPU.
   Manufactured on TSMC’s 7nm FinFET process and composed of 10.3 billion
   transistors, these chips occupy a scant 251 mm². Vega was much larger.
   Manufactured on GlobalFoundries’ 14nm LPP process, it packed 12.5
   billion transistors into a 495 mm² die. For some additional context,
   Nvidia’s competing GeForce RTX 2060-series cards employ TU106, a
   10.8-billion-transistor chip measuring 445 mm² and built using TSMC’s
   12nm FinFET process.

   We confirmed with AMD that Radeon RX 5700 XT employs a fully-enabled
   version of the Navi GPU—no part of the chip is turned off to improve
   yields or leave room for a more resource-rich model in the future. It
   exposes 40 RDNA Compute Units, each with 64 Stream processors, totaling
   2,560 ALUs across the processor. The CUs host four texture units, just
   as they did in AMD’s Graphics Core Next design, adding up to 160 in a
   complete Navi GPU. Four render back-ends per quadrant are capable of 16
   pixels per clock cycle, yielding 64 ROPs.

   That’s clearly a more compact configuration than Radeon RX Vega 64,
   which featured 64 CUs with 4,096 Stream processors and 256 texture
   units. And yet our benchmarks will show that Radeon RX 5700 XT averages
   15%-higher frame rates than Vega 64. Almost 60% of the architecture’s
   speed-up comes from performance per clock enhancements, according to
   AMD. Another 25% is attributable to gains enabled by 7nm manufacturing.
   The reminder falls under design frequency and power improvement, which
   includes more effective clock gating.

   The specifications for Radeon RX 5700 XT curiously define its base
   clock rate as “up to 1,605 MHz.” At first, we didn’t think anything of
   this. After subjecting the 5700 XT to synthetic workloads like FurMark,
   however, and observing frequencies as low as 1,575 MHz, it appears that
   the base can be violated under the right (or wrong) conditions,
   favoring a consistent acoustic experience over strict performance
   boundaries. AMD also specifies a Game GPU clock of “up to 1,755 MHz”
   and a Boost GPU clock of “up to 1,905 MHz.” As you might guess, both
   ratings are subject to certain conditions. In fact, we saw Boost
   frequencies well above 1,905 MHz at the start of many games. As the
   card warms up, though, expect to see clock rates closer to the Game GPU
   clock.

   Let’s just throw this out there: We’d prefer that AMD not create a
   third frequency rating. Because it is fleeting, it’s subject to abuse.
   In fact, AMD is already using that peak figure to calculate its 9.75
   TFLOPS FP32 performance figure. The more sustainable 1,755 MHz Game GPU
   clock translates to 9 TFLOPS, and that just doesn’t look as thunderous
   next to GeForce RTX 2060 Super’s 7.2 TFLOPS, right? Navi does carry
   over support for rapid-packed math, so AMD cites half-precision
   performance of up to 19.5 TFLOPS.

   An aggregate 256-bit pathway is populated by 8GB of GDDR6 operating at
   14 Gb/s. This gives Radeon RX 5700 XT up to 448 GBps of memory
   bandwidth—slightly less than Radeon RX Vega 64’s 484 GBps but
   significantly more than Radeon RX 590’s 256 GBps. AMD claims other
   notable improvements throughout Navi’s memory hierarchy, from reduced
   congestion in its 4MB L2 cache to a new 128KB L1 cache per quadrant
   that helps reduce latency.

   AMD considers PCIe 4.0 support one of its most noteworthy competitive
   advantages. The Radeon RX 5700 XT and standard 5700 both enable the
   latest standard’s 16 GT/s transfer rate on compatible platforms.
   However, we don’t have the requisite hardware to test for PCIe 4.0’s
   theoretical 32 GBps of throughput (two of our other labs received the
   X570-based setups for Ryzen testing). If AMD is going to say the time
   isn’t right for hardware-accelerated ray tracing, then surely the need
   for more bus bandwidth falls ever further down the priority list for
   gamers.

   Although Radeon RX 5700 XT hosts a much more sophisticated GPU than
   Radeon RX 590 and is indeed faster than Radeon RX Vega 64, its total
   board power rating is 225W. That’s the same TBP as RX 590. As we’ll see
   in our power analysis, the 5700 XT dutifully obeys this ceiling, too.

   AMD covers Navi 10 with a much more artistic shroud than we’ve seen the
   company use previously. Representatives claim the contour design helps
   optimize airflow and minimize acoustic output, and we can attest that
   the 5700 XT is one of the quietest AMD reference cards we've tested. An
   aluminum alloy shell wraps around this card’s top, front, and bottom,
   enveloping the entire cooler. One end is open, facilitating ambient air
   intake, yet is still decorated with red pinstripes, Radeon branding,
   and black-painted aluminum fins.

   The other end is loaded with slats for ventilation. Three DisplayPort
   1.4 connectors and one HDMI 2.0b interface run along the PCB’s edge.
   It’s worth noting that Navi is AMD’s first GPU with Display Stream
   Compression technology, supporting 4K monitors at 144 Hz through a
   single cable without resorting to chroma subsampling.

   Up top, eight- and six-pin auxiliary connectors feed the 5700 XT’s
   seven-phase power system. Another pair of pin stripes add a sporty
   accent, while that Radeon logo lights up red.

   Around back, an aluminum plate covers most of the PCA, protecting it
   from accidental drops.

   Ambient air is pulled in to the 70mm centrifugal fan and blown through
   an array of aluminum fins sitting on top of a vapor chamber cooler. The
   heated air is exhausted out the back of your chassis rather than
   recirculated. Of course, the downside of this design is more noise than
   many axial fan-based solutions and less airflow, resulting in higher
   GPU temperatures.

   AMD’s reference 5700 XT is quite a bit longer than Nvidia’s GeForce RTX
   2060 Super: it measures 10.75 inches from the expansion bracket to back
   edge. The Nvidia card is roughly 9 inches long in comparison. They’re
   close to the same height though, and the two cards similarly fit into a
   dual-slot form factor. It comes as no surprise that AMD’s vapor chamber
   adds notable heft. Whereas the reference GeForce RTX 2060 Super
   registers 2lb 2.2oz on our scale, Radeon RX 5700 XT weighs in at 2lb
   7.2oz.

Meet Radeon RX 5700

   Radeon RX 5700 is a close relative of the higher-end model. It’s based
   on the same graphics processor, sits on the same circuit board, and
   utilizes a similar thermal solution. The 5700’s appearance just isn’t
   as fancy. An aluminum shroud does wrap around the cooler, including its
   back edge. But instead of LED lighting, racing stripes, or textured
   ridges, it’s a plain shade of flat grey with a couple of red Radeon
   decals. That's fine by us; the clean colors and lines look good.

   A 185W board power rating justifies eight- and six-pin auxiliary
   connectors up top, along with the same 70mm blower-style fan and vapor
   chamber cooler found on Radeon RX 5700 XT. You don’t get a backplate
   this time around. Nevertheless, Radeon RX 5700 still weighs more than
   GeForce RTX 2060 Super. At 2lb 3.2oz, it carries an extra ounce around
   its hips.

   AMD brings similar display connectivity here, so you get three
   DisplayPort connectors and one HDMI port.

   Under the hood, Navi is trimmed down slightly. Thirty-six of the chip’s
   40 Compute Units remain active, cutting its Stream processor and
   texture unit count to 2,304 and 144, respectively. AMD also detunes
   Navi’s clock rates. The base frequency is “up to 1,465 MHz,” the Game
   clock is “up to 1,625 MHz,” and the so-called Boost rating is “up to
   1,725 MHz.” AMD uses those figures to claim a peak FP32 rate of 7.95
   TFLOPS, though a more practical specification would land between the
   Boost and Game frequencies based on our measurements.

   Aside from the four missing CUs, Navi remains otherwise intact for
   Radeon RX 5700. That means all its caches remain active, along with the
   256-bit memory bus hosting 8GB of 14 Gb/s GDDR6.

   Priced at $350, Radeon RX 5700 finds itself going up against GeForce
   RTX 2060.
                             Radeon RX 5700 XT
     GeForce RTX 2060 Super
         Radeon RX 5700
      GeForce RTX 2060 FE
       Architecture (GPU)
   RDNA (Navi 10)
   Turing (TU106)            RDNA (Navi 10)
   Turing (TU106)
              ALUs
   2560
   2176
   2304
   1920
   Peak FP32 Compute
   (Based on Typical Boost)
   9 TFLOPS
   7.2 TFLOPS
   7.5 TFLOPS
   6.45 TFLOPS
          Tensor Cores
   N/A
   272
   N/A
   240
            RT Cores
   N/A
   34
   N/A
   30
         Texture Units
   160
   136
   144
   120
        Base Clock Rate
   1605 MHz
   1470 MHz
   1465 MHz
   1365 MHz
   Nvidia Boost/AMD Game Rate
   1755 MHz
   1650 MHz
   1625 MHz
   1680 MHz
         AMD Boost Rate
   1905 MHz
   N/A
   1725 MHz
   N/A
        Memory Capacity
   8GB GDDR6                 8GB GDDR6
   8GB GDDR6                 6GB GDDR6
           Memory Bus
   256-bit
   256-bit
   256-bit
   192-bit
        Memory Bandwidth
   448 GB/s                  448 GB/s
   448 GB/s                  336 GB/s
              ROPs
   64
   64
   64
   48
            L2 Cache
   4MB                       4MB              4MB     3MB
              TDP
   225W
   175W
   185W
   160W
        Transistor Count
   10.3 billion              10.8 billion
   10.3 billion              10.8 billion
            Die Size
   251 mm²                   445 mm²          251 mm² 445 mm²

How We Tested Radeon RX 5700 XT and Radeon RX 5700

   AMD shipped us its Radeon RX 5700 and 5700 XT cards with plenty of time
   to test. But Nvidia cut in with the GeForce RTX 2060 Super and 2070
   Super, condensing our schedule considerably. Regardless, we still
   managed to test all four cards on a brand-new platform powered by
   Intel’s Core i7-8086K six-core CPU on a Z370 Aorus Ultra Gaming
   motherboard with 64GB of a Corsair CMK128GX4M8A2400OC14 kit. We’re
   still using a couple of 500GB Crucial MX200 SSDs for our gaming suite,
   along with Noctua’s NH-D15S heat sink/fan combo.

   Of course, this required building a new library of data with a limited
   amount of time to do it. We started with a selection of cards relevant
   to the new GeForces, and then added AMD’s Radeon RX 5700-series boards.
   From Nvidia, that includes GeForce RTX 2080, GeForce RTX 2070, GeForce
   RTX 2060, GeForce GTX 1080 Ti, GeForce GTX 1080, GeForce GTX 1070 Ti,
   and GeForce GTX 1070. All of those cards are represented by Nvidia’s
   own Founders Edition models except for the 1070 Ti, which is an MSI
   GeForce GTX 1070 Ti Gaming 8G. AMD’s own Radeon VII is part of the
   comparison as well, along with Sapphire’s Nitro+ Radeon RX Vega
   64 and Nitro+ Radeon RX Vega 56. Those partner cards ensure we don’t
   see the frequency/throttling issues encountered with our reference
   models.

   Loading...

   Our benchmark selection includes Battlefield V, Destiny 2, Far Cry 5,
   Final Fantasy XV, Forza Horizon 4, Grand Theft Auto V, Metro
   Exodus, Shadow of the Tomb Raider, Strange Brigade, Tom Clancy’s The
   Division 2, Tom Clancy’s Ghost Recon Wildlands, The Witcher
   3 and Wolfenstein II: The New Colossus.

   The testing methodology we're using comes from PresentMon: Performance
   In DirectX, OpenGL, And Vulkan. In short, these games are evaluated
   using a combination of OCAT and our own in-house GUI for PresentMon,
   with logging via GPU-Z.

   We’re using driver build 431.16 for Nvidia’s GeForce RTX 2060 and 2070
   Super and build 430.86 for all the other Nvidia cards. On AMD’s side,
   we’re using Adrenalin 2019 Edition 19.6.3 for all three existing cards,
   plus 19.7.1 for the Radeon RX 5700-series cards.

   MORE: Best Graphics Cards

   MORE: Desktop GPU Performance Hierarchy Table

   MORE: All Graphics Content

   IFRAME: https://content.jwplatform.com/players/SzkW6ASo.html

   Next

   Summary
    1. AMD Radeon RX 5700 XT and Radeon RX 5700 Review
    2. Performance Results: 2560 x 1440
    3. Performance Results: 3840 x 2160
    4. Power Consumption: Radeon RX 5700
    5. Power Consumption: Radeon RX 5700 XT
    6. Fan Speeds, Clock Rates, and Temperatures
    7. Conclusion

   About the author
   Chris Angelini @chris_angelini

   Chris Angelini is an Editor Emeritus at Tom's Hardware US. He edits
   hardware reviews and covers high-profile CPU and GPU launches.
   [javascript]
   Read more
     * Graphics
     * AMD
     * Components

   22 comments
   Comment from the forums
       Your comment
     *
   ICWiener [X]
       [ ]
       Honestly I'd pay the extra just for Nvidia's decent cooler, let
       alone power consumption, RTX, etc. Hard pass on another blower.
     *
   justin.m.beauvais [X]
       [ ]
       Dear AMD,
       Why? Wasn't your line GTX 1080 performance at RX 580 prices? points
       at Navi This is not that.
       Regrettably yours,
       Your Fans
     *
   alextheblue [X]
       [ ]

     Quote:

     Honestly I'd pay the extra just for Nvidia's decent cooler, let
     alone power consumption, RTX, etc. Hard pass on another blower.
       I read the review, don't know what you mean by "let alone power
       consumption". Efficiency is virtually the same. I personally prefer
       blowers as long as they're not crazy loud, and the review says
       they're not so I'm all for it. If you DON'T like blowers, I'm sure
       there will be third-party coolers that improve cooling performance
       and acoustics further (and dump heat into the chassis like crazy).

     Quote:

     Dear AMD, Why? Wasn't your line GTX 1080 performance at RX 580
     prices? points at Navi This is not that. Regrettably yours, Your
     Fans
       Who ever promised that? They are 10-11% faster than same-priced
       Nvidia models. They're not going to drop 5700 $100 bucks when
       they're already ahead by 11%, even though I would love lower
       prices, there's no incentive for them to do so.
     *
   face-plants [X]
       [ ]
       I'm pleasantly surprised by both the performance AND the
       last-minute drop in price. Personally I don't like blower style
       cards and will be on the lookout for third-party offerings with
       more traditional dual fan coolers. This is totally personal
       preference as I tend to build in bigger cases with plenty of
       ventilation to deal with the extra heat. If the prices don't get
       too out of line from the AIBs then I'll gladly give a few of these
       cards a go in upcoming builds. I'm also expecting a decent
       improvement in thermals so hopefully these FE cards aren't
       exemplary of the best cooling you can get on air. The prospect of
       building all AMD machines in the coming months has got me totally
       nerding out.
     *
   kinggremlin [X]
       [ ]

     Quote:

     I read the review, don't know what you mean by "let alone power
     consumption". Efficiency is virtually the same. I personally prefer
     blowers as long as they're not crazy loud, and the review says
     they're not so I'm all for it. If you DON'T like blowers, I'm sure
     there will be third-party coolers that improve cooling performance
     and acoustics further (and dump heat into the chassis like crazy).
     Who ever promised that? They are 10-11% faster than same-priced
     Nvidia models. They're not going to drop 5700 $100 bucks when
     they're already ahead by 11%, even though I would love lower prices,
     there's no incentive for them to do so.
       You didn't read the review. Ignoring the Furmark results which
       don't mirror any realworld scenario. The 5700xt is slower than the
       2070 Super while using more power and running over 10degrees C
       hotter in gaming.
     *
   alextheblue [X]
       [ ]

     Quote:

     You didn't read the review. Ignoring the Furmark results which don't
     mirror any realworld scenario. The 5700xt is slower than the 2070
     Super while using more power and running over 10degrees C hotter in
     gaming.
       Yes, I did, stop being obstinate. Ah, I get it, you must have read
       "efficiency" in my post as "power consumption", or something. Why
       are you comparing it to the 2070 Super? The 5700 is the same price
       as the 2060 and 11-12% faster on average (between TH and AT), and
       the 5700XT is the same price as the 2060 Super and roughly 10-11%
       (TH-AT) faster. Yes, they use more power, but they're faster. As a
       result the efficiency is pretty close. It varies based on workload
       (game title), but I have read the reviews here and AT (so far,
       haven't looked at a third review yet). Here:
       https://www.anandtech.com/show/14618/the-amd-radeon-rx-5700-xt-rx-5
       700-review/15
       Factor in the performance gain (in AT's suite it was 11% for the XT
       and 12 for vanilla 5700) and you'll see their power consumption is
       pretty good. Average that with TH's results in Metro: LL and the
       final efficiency is pretty neck and neck with their direct
       competitors.
       I didn't say they didn't run hot. For people that don't like
       blowers (as I already said) there will be cooler, quieter third
       party options.
     *
   digitalgriffin [X]
       [ ]

     Quote:

     I read the review, don't know what you mean by "let alone power
     consumption". Efficiency is virtually the same. I personally prefer
     blowers as long as they're not crazy loud, and the review says
     they're not so I'm all for it. If you DON'T like blowers, I'm sure
     there will be third-party coolers that improve cooling performance
     and acoustics further (and dump heat into the chassis like crazy).
     Who ever promised that? They are 10-11% faster than same-priced
     Nvidia models. They're not going to drop 5700 $100 bucks when
     they're already ahead by 11%, even though I would love lower prices,
     there's no incentive for them to do so.
       In all honesty, all these cards are expensive. $350 would have been
       the most I wanted to pay for a 5700XT. And the card does run hot.
       Pascal was a small move up in prices. Turing was just insane
       pricing wise.
       That being said I bought one today and said "F"-it. I just don't
       like NVIDIA's business ethics. It will get the job done for two to
       three years.
     *
   Axiss [X]
       [ ]
       any idea when the board partner cards come out?
     *
   randomizer [X]
       [ ]
       How many engineers looked at those fan curves in lab testing and
       thought they were good?
       I think this release is a bit underwhelming. Local pricing here
       makes the RX 5700 fairly unattractive compared to a 2060, but the
       XT is better positioned against the 2070. Not sure it's really
       worthwhile upgrading a 970 though.
     *
   daglesj [X]
       [ ]
       So for the many of us on a RX480?...
       Worth it? Could someone not dig out the previous AMD midrange value
       demon to test against? C'mon...
     *
   feelinfroggy777 [X]
       [ ]
       The last $200 card AMD has released was 2.5 years ago. Let that
       sink in for a minute.
     *
   jeremyj_83 [X]
       [ ]

     Quote:

     How many engineers looked at those fan curves in lab testing and
     thought they were good? I think this release is a bit underwhelming.
     Local pricing here makes the RX 5700 fairly unattractive compared to
     a 2060, but the XT is better positioned against the 2070. Not sure
     it's really worthwhile upgrading a 970 though.
       On average the 5700XT is about twice as fast as the GTX 980 and the
       980 is 10-15% faster on average than the 970. That means that going
       to the XT will double your framerate assuming your CPU can keep up.
       I would say that is a worthwhile upgrade.
       https://www.anandtech.com/bench/product/2522?vs=2529
     *
   randomizer [X]
       [ ]

     Quote:

     On average the 5700XT is about twice as fast as the GTX 980 and the
     980 is 10-15% faster on average than the 970. That means that going
     to the XT will double your framerate assuming your CPU can keep up.
     I would say that is a worthwhile upgrade.
     https://www.anandtech.com/bench/product/2522?vs=2529
       I usually like to triple my framerates while sticking to a similar
       price bracket :)
       Also I don't think I can afford to run that space heater in summer.
     *
   redgarl [X]
       [ ]
       For anyone that want to know what matter...
       https://static.techspot.com/articles-info/1870/bench/Cost.png
       https://static.techspot.com/articles-info/1870/bench/Cost1.png
       With the actual scoring, this review is a joke. Navi is disrupting
       pricing and almost match a 1080 TI for 400$... however at
       tomshardware it sux.
     *
   alextheblue [X]
       [ ]

     Quote:

     In all honesty, all these cards are expensive. $350 would have been
     the most I wanted to pay for a 5700XT. And the card does run hot.
     Pascal was a small move up in prices. Turing was just insane pricing
     wise. That being said I bought one today and said "F"-it. I just
     don't like NVIDIA's business ethics. It will get the job done for
     two to three years.
       They ARE expensive - both Nvidia and AMD. This is the new
       "mid-range" unfortunately. I'd like to see Xe undercut them both
       and force this tier back down to the ~$250 range. I still might end
       up getting one, we'll see.

     Quote:

     How many engineers looked at those fan curves in lab testing and
     thought they were good? I think this release is a bit underwhelming.
     Local pricing here makes the RX 5700 fairly unattractive compared to
     a 2060, but the XT is better positioned against the 2070. Not sure
     it's really worthwhile upgrading a 970 though.
       If you're talking about the erratic behavior with the speed
       dropping over time, that looks to me like a bug or an issue with
       that sample. Their XT didn't act that way. Pricing can suck
       depending where you are... in the US it certainly offers more bang
       for the buck than the 2060. Maybe pricing will be more reasonable
       when partner boards become widespread. Like your icon, BTW, big fan
       of new Genesis/MD titles - awaiting Xeno Crisis currently.
     *
   randomizer [X]
       [ ]

     Quote:

     If you're talking about the erratic behavior with the speed dropping
     over time, that looks to me like a bug or an issue with that sample.
     Their XT didn't act that way
       While not as bad, I wouldn't call the XT's fan curve good either.
       It's going to start roaring the moment you get to the main menu
       (and you'll probably notice the sudden change) and it won't stop
       until you're back at the desktop.

     Quote:

     Like your icon, BTW, big fan of new Genesis/MD titles - awaiting
     Xeno Crisis currently.
       It is actually from a Genesis/MD game, but one that was released
       last year, not in the 90s. :)
     *
   TJ Hooker [X]
       [ ]

     Quote:

     For anyone that want to know what matter...
     https://static.techspot.com/articles-info/1870/bench/Cost.png
     https://static.techspot.com/articles-info/1870/bench/Cost1.png With
     the actual scoring, this review is a joke. Navi is disrupting
     pricing and almost match a 1080 TI for 400$... however at
     tomshardware it sux.
       Those performance numbers aren't very different than what TH is
       reporting. And values will obviously vary depending on your test
       suite. This review reflects the fact that these otherwise great
       value cards are paired with a mediocre blower cooler, it hardly
       says the cards "suck". Which amounts to saying that we should wait
       for AIB cards, which is pretty common advice for every GPU launch.
     *
   AgentLozen [X]
       [ ]

     Quote:

     For anyone that want to know what matter...
     https://static.techspot.com/articles-info/1870/bench/Cost.png
     https://static.techspot.com/articles-info/1870/bench/Cost1.png With
     the actual scoring, this review is a joke. Navi is disrupting
     pricing and almost match a 1080 TI for 400$... however at
     tomshardware it sux.
       While I was reading this article I was thinking "please please
       please let redgarl post in the forums!!" My wish was granted and
       Redgarl definitely provided.
       Redgarl describes this review as a "joke". He claims that Navi is
       "disrupting pricing" and "almost match a 1080 TI". He provides
       links to the TechSpot 5700 review for "anyone that want to know
       what matter..." This made me genuinely really curious. Is today the
       day that redgarl proves Tomshardware is run by a bunch of hacks? I
       had to know for myself.
       First of all I looked at the test setups for each review. Techspot
       uses a 9900K Intel CPU and 32GB DDR4-3200. Tomshardware's build is
       different. They use a 8086K Intel CPU and 64GB of DDR4 2400.
       Tomshardware documents the drivers used as 431.16 Nvidia Driver for
       the 2060Super and 2070Super but the 430.86 for every other Nvidia
       card. For AMD cards, they use the 19.7.1 driver with the RX 5700
       cards and the 19.6.3 driver for everything else. By comparison,
       Techspot reports that they used the "latest drivers available at
       the time". Its not clear how that compares to Tomshardware. The
       test machine on each respective site is different and that may
       cause differences in benchmarks. I predict (jk. I've already seen
       the results) the Tech Spot benchmarks may be a little higher
       judging by the faster memory and the better CPU.
       I then compared Techspots benchmarks to those featured on
       Tomshardware. First I looked at the Assassin's Creed Odyssey
       results. At 1440p, Techspot reports the Radeon RX 5700XT hit an
       average of 70fps. Meanwhile, Tomshardware was showing... oh dear,
       Odyssey wasn't featured on Tom's. Let's just move on. The
       Techspot's Destiny 2 1440p results show that.. uh oh, it's
       happening again. Techspot didn't feature Destiny 2 benchmarks. This
       is a problem because you can't compare apples-to-apples when the
       same benchmarks aren't used.
       Assassin's Creed Odyssey, Destiny 2, DiRT Rally 2.0, Far Cry 5, Far
       Cry New Dawn, Final Fantasy XV, GTA V, Resident Evil 2, Strange
       Brigade, Tom Clancy's Ghost Recon, Tom Clancy's Rainbow Six Siege,
       World War Z, The Witcher 3, Wolfenstein II: The New Colossus are
       either benchmarked by Tom's or TechSpot but not both. The only
       overlapping games are Battlefield V, Forza Horizon 4, Metro Exodus,
       Shadow of the Tomb Raider, and Tom Clancy's The Division 2.
       Let's look at the Shadow of the Tomb Raider results. Techspot and
       Tom's ran the game at 1440p, highest quality.... oh no.
       Tomshardware used SMAAT2x in it's Shadow of the Tomb Raider
       benchmark. Techspot didn't report using that setting. Well that's
       not apples-to-apples either. A similar situation happens with
       Battlefield V where Tomshardware uses DX12 and Techspot uses DX11.
       For Tom Clancy's The Division 2, I double checked to make sure the
       software is the same between both Techspot and Tom's platforms.
       DX12, 1440p, Ultra quality. Check. Now here is something that
       appears to be genuinely inconsistent between reviews. Techspot's
       averages for the GeForce 2080, 2070 Super, 2060 Super, Radeon RX
       5700, and 5700XT are higher than the Tomshardware numbers. Well
       there you have it. Redgarl has proven that "With the actual
       scoring, this review is a joke." Tomshardware losses, redgarl wins.
       Except that the test systems between Tomshardware and TechSpot are
       different. Isn't this a roller coaster? TechSpot has a better CPU
       and faster ram. I looked carefully at the trends between both
       reviews and lo and behold they show the same thing. Both sites list
       the video card pecking order as: 2080, 2070 Super, Radeon VII, 1080
       Ti, 5700XT, 2070, 2060 Super, Vega 64, 5700, 1080, 2060, and so on
       and so forth. It's very plausible that the differences in specific
       numbers can be chalked up to differences in platforms or margin of
       error.
       The charts that redgarl cherry picked show the results between Toms
       and Techspot are totally different. If you only considered those
       charts then you might think Tomshardware has an incompetent staff
       the way redgarl wants you to believe. However, when you look at the
       whole picture, its obvious that you can't directly compare the
       benchmark results between both sites. A total of 19 games were
       reviewed by both sites. Only five overlap. I threw out Shadow of
       the Tomb Raider because we can't be sure both sites used the same
       settings. That leaves four. When we directly compare results, the
       numbers vary probably due to differences in test hardware, but the
       trends are consistent. In conclusion: there isn't enough evidence
       to prove that the Tomshardware review did anything wrong.
       To redgarl, this is just for fun my dude. No hard feelings.
     *
   alextheblue [X]
       [ ]

     Quote:

     It is actually from a Genesis/MD game, but one that was released
     last year, not in the 90s. :)
       Yeah I know, that's why I pointed it out. It's from Tanglewood.
       That's what I was talking about when I said I am a fan of new
       Genesis games. Xeno Crisis, for example, isn't even (quite) out
       yet.
     *
   randomizer [X]
       [ ]

     Quote:

     Yeah I know, that's why I pointed it out. It's from Tanglewood.
     That's what I was talking about when I said I am a fan of new
     Genesis games. Xeno Crisis, for example, isn't even (quite) out yet.
       Righto, I thought it was just the style that reminded you of
       Genesis games. I didn't expect to run into anyone else who had
       actually heard of this game :LOL:
     *
   treetops422 [X]
       [ ]
       "Across our benchmark suite, Radeon RX 5700 XT averages 9.9%-higher
       frame rates than the GeForce RTX 2060 Super at 2560 x 1440. Radeon
       RX 5700 averages 11%-higher frame rates than the GeForce RTX 2060
       at the same resolution. The GeForce RTX 2070 Super does serve up
       average frame rates 6.9% higher than Radeon RX 5700 XT, but it
       costs 25% more. "
       That's all I need to know. Who would RT and half their performance?
       Esp on a 2060
     *
   B-Real85 [X]
       [ ]

     Quote:

     Honestly I'd pay the extra just for Nvidia's decent cooler, let
     alone power consumption, RTX, etc. Hard pass on another blower.
          + Most consumers buy AIB models, so it's not a problem.
          + Power consumption? RX 5700 consumes about the same as the RTX
            2060 , therefore it has better performance/W ratio. The RX
            5700 XT is near the 2070's ratio.
          + RT: 3 games so far, in which only the RTX 2080 Ti can produce
            tolerable fps with about 40-45 minimums, and it's still on
            FHD. RTX 2060 produces 50-60 average and 30 lows... That's
            what you expect from a 350$ card? Don't think so.

     Quote:

     Dear AMD, Why? Wasn't your line GTX 1080 performance at RX 580
     prices? points at Navi This is not that. Regrettably yours, Your
     Fans
       No, sadly not. There was only a rumoured news on WCCFTech. There
       was 7 or 8 models with 200$ Vega 56 and so on. But AMD never spoke
       about such prices. That was only a rumour (sadly).

     Quote:

     You didn't read the review. Ignoring the Furmark results which don't
     mirror any realworld scenario. The 5700xt is slower than the 2070
     Super while using more power and running over 10degrees C hotter in
     gaming.
       And costs 100$ more. And, wow, really, 5W more than a 2070 Super,
       fantastic. AIB models help you. As 99% of the NV customers also buy
       AIB NV models.

     Quote:

     In all honesty, all these cards are expensive. $350 would have been
     the most I wanted to pay for a 5700XT. And the card does run hot.
     Pascal was a small move up in prices. Turing was just insane pricing
     wise. That being said I bought one today and said "F"-it. I just
     don't like NVIDIA's business ethics. It will get the job done for
     two to three years.
       You are 100% right, just mentioning: if someone needs performance
       over RT feature, RX5700 is absolutely better than the RTX 2060 and
       the RX5700XT is absolutely better than the RTX2060 Super and RTX
       2070.

   Most Popular
    1. Confirmed: AMD’s Navi RX 5700 Graphics Cards Will Be Cheaper Than
       We Thought
    2. Update: Nvidia VP Says Next-Gen GPUs Will Be Made by TSMC and
       Samsung
    3. AMD Radeon RX 5700 XT and RX 5700 Packaging Leak

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * Raspberry Pi

     Review

    Raspberry Pi 4 Review: The New Gold Standard for Single-Board Computing

   by Avram Piltch August 28, 2019 at 9:04 PM

     *
     *
     *
     *
     *
     *

   46 Comments

   Raspberry Pi has long been the gold standard for inexpensive
   single-board computing, powering everything from robots to smart home
   devices to digital kiosks. The long-anticipated Raspberry Pi 4 takes Pi
   to another level, with performance that’s good enough to use in a pinch
   as a desktop PC, plus the ability to output 4K video at 60 Hz or power
   dual monitors.

Raspberry Pi 4

   Editor's choice tom's Hardware
   [javascript]
   Pros
     * Much faster than prior Raspberry Pis
     * USB 3 Ports
     * Ability to output 4K video at 60 Hz
     * Dual-monitor support

   Cons
     * Key software doesn’t work at launch
     * Poor high-res video playback

   Verdict

   The best Raspberry Pi yet is faster and more functional than any of its
   predecessors, without raising the price.
   4.5/5
   $35CanaKit

   IFRAME: https://content.jwplatform.com/players/YdWWS5dA.html

   For the same $35 starting price as prior models, you get speeds that
   are two to four times faster, support for USB 3 and true Gigabit
   Ethernet. Perhaps more importantly, there is a $45 Raspberry Pi 4 with
   2GB of RAM and a $55 unit with 4GB, four times more than any previous
   Pi has had. Makers and hobbyists should add the Raspberry Pi 4 to their
   arsenals, and tech enthusiasts who’ve never used a Pi before now have
   even more reasons to buy one.

   Raspberry Pi 4 Model B. (Credit: Tom's Hardware) Raspberry Pi 4 Model
   B. (Credit: Tom's Hardware)

   We had an opportunity to get early access to the Raspberry Pi 4 B, the
   full name of the first Pi 4 model, and were able to test a board with
   the full 4GB of RAM. What we saw is a full-fledged mini computer that’s
   packed with potential. We’re particularly excited about the
   possibilities for inference, particularly object and sound detection.

  Backward Compatibility

   It’s important to note that, even a couple of months after launch, some
   important Raspberry Pi software doesn’t yet work on the Pi 4. To run Pi
   4, you’ll need to download a brand new build of the Raspbian
   OS, Raspbian Buster. And not everything runs in Buster yet. During
   testing, we found numerous Python libraries or other required packages
   that weren’t compatible with the new OS.

  Game Emulation or Lack Thereof

   Retropie, the very popular gaming emulator software, does not
   officially support the Raspberry Pi 4. We did find a workaround that
   lets you run Retropie on the Raspberry Pi 4, but right now it's a
   kludge that doesn't necessarily work well. You can also download a beta
   version of Lakka, another emulation platform, but it too is not a
   final, fully working build. Developers have said that they are working
   on a Pi 4-compatible version and will have one soon, but if you’re
   reading this today and need to build an arcade machine in the near
   future, you might want to get an older model.

  Key Differences

   The table below shows a key specs comparison between the Raspberry Pi 4
   B, the first an only Pi 4 model, and the Raspberry Pi 3B+, the fastest
   version of the Pi 3.

   Spec Raspberry Pi 4 B Raspberry Pi 3 B+
   CPU 1.5-GHz, Quad-Core Broadcom BCM2711B0 (Cortex A-72) 1.4-GHz, Quad
   Core Broadcom BCM2837B0  (Cortex A-53)
   RAM 1 - 4GB DDR4 1GB DDR2
   GPU 500 MHz VideoCore VI  400 MHz VideoCore IV
   Video Out dual micro HDMI ports single HDMI port
   Max resolution 4K 60 Hz + 1080p or 2x 4K 30 Hz 2560 x 1600
   USB Ports 2x USB 3.0 / 2x USB 2.0 4x USB 2.0
   Wired Networking Gigabit Ethernet 330 Mbps Ethernet
   Wireless 802.11ac (2.4 / 5 GHz), Bluetooth 5.0 802.11ac (2.4 / 5 GHz),
   Bluetooth 4.1
   Charging Port USB Type-C micro USB
   Power Requirement 3A, 5V 2.5A, 5V
   Size 3.5 x 2.3 x 0.76 inches (88 x 58 x 19.5mm)
   3.2 x 2.2 x 0.76 inches (82 x 56 x 19.5mm)
   Weight 0.1 pounds (46 grams) 0.11 pounds (50 grams)

   The most important new features are the faster processor and GPU, more
   and faster RAM, the addition of USB 3 ports, dual micro HDMI ports
   instead of a single HDMI connection and support for 4K output. The
   higher bus speed that enables USB 3 support also allows the on-board
   Ethernet port to support true Gigabit connections (125 MBps) where the
   last-gen models had a theoretical maximum of just 41 MBps. The microSD
   card slot is also twice as fast, offering a theoretical maximum of 50
   MBps versus 25 MBps on the 3B+.

   Because the new SoC needs more power, the Raspberry Pi 4 B charges
   over USB Type-C instead of micro USB. It also requires a power adapter
   that can deliver at least 3 amps of power and 5 volts, though you may
   be able to get away with 2.5 amps if you don’t attach many peripherals
   to the USB ports. Putting aside the power needs, USB Type-C connectors
   are reversible, which makes them much easier for kids (and adults) to
   plug in.
   After we first published this review, we found out that some USB Type-C
   cables don't work with the Raspberry Pi 4. Those cables that are
   "electronically marked" will see the Pi 4 as a USB audio device and not
   provide power. Only USB-C to USB-C cables can be electronically marked
   and you only find this on cables that operate at 5 Gbps or higher. In
   our tests, we found 11 low-cost USB-C cables that work with the
   Raspberry Pi 4 and only found a few cables, all USB 3.1, that did not.
   If you buy Raspberry Pi's official Pi 4 charger, which comes with a
   cable, you also won't have a problem.  The Pi foundation says it will
   fix the problem on future builds, but it can't be solved via a firmware
   update.

  Design

   (*) ( ) ( ) ( ) ( ) ( ) ( ) ( )
     * raspberrypi4-hero-2
       raspberrypi4-hero-2
     * raspberrypi4-hero
       raspberrypi4-hero
     * raspberrypi4-gpio
       raspberrypi4-gpio
     * raspberrypi4-hdmiusbav
       raspberrypi4-hdmiusbav
     * raspberrypi4-usbethernet
       raspberrypi4-usbethernet
     * raspberrypi4-ethernet
       raspberrypi4-ethernet
     * raspberrypi4-usbtypec
       raspberrypi4-usbtypec
     * raspberrypi4-box
       raspberrypi4-box

   (*) ( ) ( ) ( ) ( ) ( ) ( ) ( )
     * raspberrypi4-hero-2

raspberrypi4-hero-2
     * raspberrypi4-hero

raspberrypi4-hero
     * raspberrypi4-gpio

raspberrypi4-gpio
     * raspberrypi4-hdmiusbav

raspberrypi4-hdmiusbav
     * raspberrypi4-usbethernet

raspberrypi4-usbethernet
     * raspberrypi4-ethernet

raspberrypi4-ethernet
     * raspberrypi4-usbtypec

raspberrypi4-usbtypec
     * raspberrypi4-box

raspberrypi4-box

   At 3.5 x 2.3 x 0.76 inches (88 x 58 x 19.5 mm)  and 0.1 pounds (46
   grams), the Pi 4 is thin enough to fit in your pocket and light enough
   to carry anywhere. The board is durable enough to probably survive
   rolling around in your bag, but we recommend sticking it in something
   protective, mostly to protect the pins. However, during testing, I
   always used the board bare on my desk and I carried it back and forth
   between work and home many times by simply putting it in a cardboard
   box with no padding or static bag.

   Unfortunately, if you want a case, you can’t use one that’s been
   designed for any previous Raspberry Pi. The Raspberry Pi 3 B / 3 B+
   have almost the same dimensions, but the port layout has changed just
   enough to make the Pi 4 B incompatible. Where prior Pis had a single,
   full-size HDMI port, the dual micro HDMI connectors on the Pi 4 jut out
   more and so don’t line up with the holes on anything that was designed
   for the Pi 3 B. We really like the Pimoroni Pibow, a $10 / £8.50 case
   that looks really good and doesn't cover over the GPIO pins.

   The Raspberry Pi 4 covers more than just the basics when it comes to
   ports. The right side has four USB Type-A connections, two of which are
   USB 3.0. There’s also a full-size, Gigabit Ethernet port for wired
   connections there. The bottom edge has a 3.5mm audio jack, two micro
   HDMI ports and the USB Type-C charging port. On the left side, you’ll
   find the microSD card reader.
   And on the top surface of the board, you’ll see ribbon connectors for
   the Camera Serial Interface (CSI) and Display Serial Interface (DSI),
   which provide dedicated connections to Raspberry Pi’s own camera and
   screen (or compatible accessories). Of course, you can connect a camera
   to a USB port as well and there are a couple of more common ways,
   including the micro HDMI ports, to output to a screen.

  New CPU, RAM

   The Raspberry Pi 4 has similar design and dimensions to its
   predecessors, but it’s an all-new platform, powered by a new processor,
   the Broadcom BCM2711B0. Since the first Pi in 2012, all  Pis have used
   40nm SoCs, but this new chip is based on a 28nm process and, instead of
   the older Cortex-A53 microarchitecture, it uses Cortex-A72. The
   BCM2711B0 in the Raspberry Pi 4 has four cores and is clocked at 1.5
   GHz, which at first blush, doesn’t seem much quicker than the
   quad-core, 1.4 GHz BCM2837B0 in the Raspberry Pi 3B+.

   However, Cortex A72 has 15-instruction pipeline depth, compared to just
   8 on the older model, and it also provides out-of-order execution so
   it’s not waiting for the output of one process to start on another. So,
   even at the same clock speed (and the BCM2711B0 is based on a smaller
   process node), Cortex-A72 processors will be significantly faster and
   use more power than their A53-powered ancestors.

   For example, on the Linpack benchmark, which measures overall compute
   power, the Pi 4 absolutely whooped the Pi 3 B+ in all three tests. On
   the all-important single precision (SP) test, the Pi 4, scored 925 as
   compared to the 3 B+’s mark of 224, a boost of 413 percent.

   On the Sysbench CPU test, the Pi 4 B was capable of performing 394
   events per second as compared to 263 for the Pi 3 B+. That's a
   difference of 50 percent.

   The RAM is also quite a bit quicker, going from 1GB of DDR2 RAM
   operating on the Pi 3B+ to up to 4GB of DDR4 RAM. In addition to the
   increased bandwidth, having more memory is a huge deal, particularly
   for web surfing.

   The Pi 4’s RAM returned read and write rates of 4,130 and 4,427 Mbps,
   respectively. That’s 51 percent and 54 percent better than the 3 B+.

   Both the CPU and the RAM are implicated when you do file compression.
   When zipping a file in multithreaded mode, the Pi 4 B is 37 percent
   quicker than its predecessor, but it’s far stronger in single-threaded,
   eclipsing the 3 B+ by 60 percent.

  New GPU, Faster Graphics Performance

   The GPU is getting a nice boost too. It moves from a Broadcom VideoCore
   IV that operated at a core clock speed of 400 MHz to a VideoCore VI
   that’s set at 500 MHz. The new architecture allows it to output to a
   display at up to 4K resolution with a rate of 60 fps or to support dual
   monitors at up to 4K 30 Hz.

   While we wish we could have tried some of the more resource-intensive
   emulators in Retropie in time for this review, there wasn’t a Pi
   4-compatible version at launch. However, the OpenArena Benchmark, which
   measures frame rates in a game that’s a a clone of Quake III Arena, did
   run.

   At 720p resolution, the Pi 4 was the only Raspberry Pi capable of
   delivering smooth frame rates. Yes, you can play on the Pi 3, 3 A+ or 3
   B+, but all three deliver rates between 27 and 28 fps as compared to
   41.4 fps on  the Pi 4.

  Storage Performance

   No matter how fast your processor, RAM and GPU are, if your storage is
   slow, everyday tasks like opening apps and files will be laggy. Like
   all Raspberry Pis, the 4 B’s primary storage device is its microSD card
   reader, which is convenient but a bit constrained. According to the Pi
   Foundation, the 4 B has a top transfer rate of 50 MBps, which is double
   the speed of the reader on the 3 B+. There’s no known limit on
   capacity.

   Our benchmarks, which were conducted with a Samsung EVO Plus microSD XC
   Class 10 card, show less impressive rates than the theoretical
   maximums. The Pi 4 B returned sequential read / write rates of 45.7 and
   27.7 MBps, while the 3 B+ trailed at 22.8 and 17.5 MBps. Keep in mind
   that the card is rated for 100 MBps reads and 60 MBps writes.
   If you have a speedy USB Flash drive or an external SSD, you can get
   far better storage performance out of the Pi 4 B. The Pi 4 B is the
   first with USB 3 ports, which have a maximum, theoretical bandwidth of
   625 MBps. To find out how this works in real-life, we wrote a separate
   article where we attached an external SSD to a Raspberry Pi 4 B.
   You'll find full results in the article, but what we found was
   impressive.

   Using a Western Digital Blue SSD in a USB to M.2 enclosure, we saw
   transfer rates that were 2 to 13x times faster than the microSD card.
   And applications definitely opened a lot faster with the SSD attached.
   Unfortunately, a regular USB Flash drive, was often slower than the
   microSD card.

   However, it's important to note that, at present, the Pi 4 firmware
   doesn't allow you to boot off an external drive so the best that you
   can do is run all of your programs, including the majority of the OS,
   off of an SSD while leaving the boot partition on a microSD card. A
   firmware update should fix this in the next few weeks, but for now, we
   have an article explaining how to run your Raspberry Pi 4's software
   from an SSD.

   Fast USB 3 ports are about more than just storage. You can use other
   high-bandwidth peripherals like Google’s Coral USB Accelerator, which
   helps with artificial intelligence tasks.

  Networking Performance

   The Raspberry Pi 4 has the same 802.11ac Wi-Fi as its direct
   predecessor, but it throws in Bluetooth 5.0 support, an improvement
   over the Bluetooth 4 on prior models. More importantly, the Ethernet
   port now has more bandwidth, which allows it to offer a full gigabit of
   throughput where prior models could only achieve about 330 megabits.

   In testing, the PI 4 B’s Ethernet port achieved 943 Mbps, which blows
   away the other Raspberry Pis. In fact, in a throughput test, the Pi 4 B
   got 943 Mbps (close to the 1,000 Mbps maximum). That’s nearly five
   times as many as the Pi 3B+, which only got 237 Mbps.

   Both the old and new Raspberry Pi have 802.11ac Wi-Fi that can run on
   2.4 GHz or 5-GHz bands. So we didn’t expect to see much difference in
   performance here. But the 5-GHz throughput is noticeably higher for the
   Pi 4, returning a rate of 114 Mbps, compared to 97 Mbps on the Pi 3 B+,
   a decent 18 percent improvement.

  Power and Heat

   With a more power-hungry processor and the need for at least a 5-volt,
   3-amp power adapter, the Pi 4 should be expected to consume more power
   than its predecessors.

   At idle, the Pi 4 B draws 3.4 watts, which is just 17 percent more than
   the 3 B+. Under load, that number jumps to 7.6 watts, but that’s still
   only 19 percent more juice than its direct predecessor. If you want the
   lowest-power Pi, performance be damned, then go for the Pi Zero W,
   which consumes a mere 0.8 watts at idle and 1.6 watts under load.

   Yes, this board gets warm, warmer than its predecessor. Thermal images
   mirror what we experienced; the areas of the board near the CPU get
   really warm, not just the top of the processor itself. The Pi 4 board
   reaches a toasty 74.5 degrees Celsius (166 degrees Fahrenheit). That’s
   not enough for a serious burn, but kids especially should be sure to
   pick up the Pi by its sides only. The top surface of the Pi 3 B+ is
   much cooler, maxing out at 62.5 degrees Celsius (144.6 degrees
   Fahrenheit).

   Thermal image of Raspberry Pi 3 B+. (Credit: Gareth Halfacree) Thermal
   image of Raspberry Pi 3 B+. (Credit: Gareth Halfacree)

   Thermal image of Raspberry Pi 4 B. (Credit: Gareth Halfacree) Thermal
   image of Raspberry Pi 4 B. (Credit: Gareth Halfacree)

   As with any modern computer,  if you push the system too hard and the
   CPU or GPU get too hot, the computer will throttle down to avoid
   damage.

   While running a CPU-intensive workload for 10 minutes, the processor
   hit 81 degrees and began throttling down from 1.5 to 1 GHz after 3
   minutes. However, the system kept bringing itself back to the full 1.5
   GHz when it dipped down to around 80 degrees, but then it would get
   warm again and go down to 1 GHz. If you want to have better sustained
   performance under load, consider getting an active cooler for the
   Raspberry Pi 4 or, at the very least, attach a passive heat sink.

  GPIO Pins

   The real star of the show on any Raspberry Pi is its set of 40 GPIO
   (General Input / Output) pins. The pin count and layout remains
   unchanged from prior models, going back to the Raspberry Pi 2, so any
   “hats,” sensors or LED screens that were made to attach to a Pi 2 or 3
   will be compatible.

   (Image Credit: Gareth Halfacree) (Image Credit: Gareth Halfacree)

   However, the Raspberry Pi 4 has added a few new capabilities to some of
   the pins. For hardcore makers who are wiring up a variety of
   peripherals, the GPIO pins now support four additional I2C, SPI and
   UART connections. So, if your sensors or peripherals require any of
   these interfaces, you now have a lot more of them.
   Below, you'll find a new GPIO pinout, with the added capabilities of
   the Pi 4. To learn more about what each pin does, checkout our
   Raspberry Pi 4 GPIO pinout article.

   Raspberry Pi 4 GPIO Pinout. Image Credit: Les Pounder Raspberry Pi 4
   GPIO Pinout. Image Credit: Les Pounder

   The speed and responsiveness of the GPIO pins is also much faster on
   the Raspberry Pi 4, likely due to its faster processor. Our test uses
   the gpiozero Python library to toggle pins on and off continuously and
   measures the rate at which they switch. The Pi 4 achieved a speed of
   50.8 KHz, compared to just 16.1 on the Pi 3 B+. That’s an improvement
   of 215%.

  Using the Raspberry Pi 4 as a PC

   One of the goals of the Raspberry Pi 4 is to be a capable PC that
   anyone can use for surfing the web, doing light productivity work or
   even playing very basic games. In order to test this use case, I spent
   several hours doing my everyday work on the device and I even wrote
   portions of this review using it.

   I really liked being able to output to dual monitors, something I do
   everyday at both work and home. And, since much of my daily work
   routine these days takes place in a web browser, I had no problem
   writing, editing and researching articles using Chromium. Even with 15
   tabs open, switching between them was smooth and I had not maxed out
   the 4GB of on-board RAM.

   And while I wouldn’t want to use it every day, GIMP provides a decent
   way to edit still images. If I wanted to crunch spreadsheets or compose
   documents outside of Google Docs, Libre Office more than fits the bill.

   My biggest problems involved video playback. If I wanted to watch a
   YouTube video, I had to keep it in a window, because even in 480p
   resolution, it was jerky at full screen. The other task I’d like to
   perform is playing retro games, but as of this writing, the Retropie
   package of emulators doesn’t work with Pi 4. I was able to install and
   play Quake Arena, however.
   Keep in mind that the Raspberry Pi 4 works with a few different
   operating systems, but the best-supported one is Raspbian, a flavor of
   Linux that has a small learning curve for newbies. Users who are only
   looking for a low-cost web-surfing PC, without doing any tinkering, can
   find a Chromebook or low-end Windows laptop for $150 to $200.

  4K Output, Video Playback and Transcoding

   One of the downsides of prior Raspberry Pi computers is that they can
   only natively output to one screen at a time, but if you like
   multitasking and want to use a Pi for productivity, you really want
   that second screen. The Raspberry Pi 4 has dual micro HDMI ports that
   can each connect to a separate monitor or TV and can operate at up to
   4K (3840 x 2160) resolution. If you have multiple 4K monitors, you have
   a choice: you can either run each screen at a somewhat-sluggish 30 Hz
   or, you can enable 4K mode in the settings menu, which jacks up the
   voltage a little so you can run one monitor at 4K 60 Hz and another at
   up to 1080p.

   During extensive hands-on testing, I found that, while the 4K at 30 Hz
   is tolerable, little things like the movement of the mouse pointer are
   a bit sluggish. If you have a 4K screen, you’re definitely better off
   going for the 60 Hz mode, but note that the added voltage may also
   cause your CPU to get hot and throttle more easily.

   While surfing the web, looking at still images and just enjoying all
   the extra screen real estate of 4K is great, video playback is the
   Raspberry Pi 4’s Achille’s heel, at least as of this writing. Whether
   we were attempting to stream a 4K video or use a downloaded file, we
   never got a smooth, workable 4K experience, either in Raspbian Buster
   or LibreElec, an OS that runs the Kodi media player. Several H.264
   encoded videos, including Tears of Steel, did not play at all or showed
   as a jumble of colors. Even the sample jelly fish videos that the folks
   at Kodi recommended for my testing appeared as still pictures with no
   movement. Clearly, there’s a lot of optimization that still needs to be
   done both on the OS and software side to make the Raspberry Pi 4
   capable of playing 4K video.

   Unfortunately, even streaming 1080p YouTube videos is a challenge at
   this point. Running at 1080p resolution, full screen video trailer for
   Stranger Things showed obvious jerkiness. However, the playback was
   smooth when I watched the same clip in a smaller window. The same
   problem occurred, even when I dropped the stream’s resolution down to
   480p.

   Playing offline 1080p videos works well, provided your screen is at
   1920 x 1080 or lower resolution. A downloaded trailer of Avenger’s
   Endgame was perfectly smooth when I watched it using the VLC player.

   The Raspberry Pi 4 won’t replace anyone’s MacBook Pro or Dell XPS 13
   creative workstation, but it can transcode videos for you, if you’re
   patient. It took the Raspberry Pi 4 48 seconds to transcode a very
   short H.264 encoded clip to NTSC DV format using FFmpeg. That’s much
   less time than the Pi 3 B+, which finished in 108 seconds, but if
   you’re converting a whole movie, you’ll probably need to walk away from
   your Pi for a while and then come back.

  Web Surfing

   The web surfing experience on the Raspberry Pi 4 is noticeably much
   smoother than on any of its predecessors. The faster processor helps,
   but so does having more than 1GB of RAM. Keeping my eye on Gnome System
   Monitor, I noticed that, even with just one or two tabs open, I was
   using more than 1GB of RAM. However, on the Pi 4 with 4GB of RAM, I had
   no problem running over 15 tabs at once, switching back and forth
   between them.

   While web pages don’t render as quickly as on my modern Core i7-powered
   laptop with Windows 10, the Pi 4 provides a very solid web browsing
   experience. I had no problems using my Google suite apps, including
   Gmail, Google Sheets and Google docs.

   On Jetstream 1.1, a synthetic browsing benchmark that measures
   Javasript processing and page rendering, the Pi 4 trounced the Pi 3B+,
   42.5 to 17.1  That’s an improvement of 148%, but the Pi still isn’t
   quite as powerful as a low-end, Intel-powered Chromebook like the
   Samsung Chromebook 3, which scored 49.7. However, there are PC laptops
   that fared worse, including the Dell Inspiron 14 3000, which hit just
   35.9.

   The Speedometer 2.0 benchmark measures overall responsiveness by
   loading dummy web apps and then simulating a user interacting with
   them. A higher score on this test, in terms of runs per minute, shows
   that when you’re actually using a web tool such as Google docs or
   Gmail, you should get less lag. As on Jetstream and in real-world
   scenarios, the Pi 4 came out comfortably ahead of its predecessor. In
   this case, it was 98 percent faster.

   Just forget about using web sites with webGL animation on them, because
   they are slideshow like, at least with the current software. When I
   launched the webGL aquarium demo, which shows 50 fish swimming, I got a
   rate of just 2 fps on the Raspberry Pi 4 and a mere 1 fps on the Pi 3
   B+. I guess that makes the Pi 4 twice as fast, but 2 fps is still
   useless.

  Web Hosting

   It's very easy to use set up a Raspberry Pi web server and this is one
   of the most popular use cases for the computer. In fact, at Tom’s
   Hardware, we use a Raspberry Pi 3 B as a server on our local network
   that we use to host our laptop battery test. Raspberry Pi 4 promises
   even more robust web surfing thanks to its faster processor, greater
   amount of RAM and better network connectivity.

   Using the Phoronix Apache Test, the Raspberry Pi 4 handled 3,983
   requests per second versus 2,850 for the Pi 3 B+. That’s an improvement
   of 40 percent, which means that you can deliver heavier web pages or
   serve more visitors at the same time, without lag.

   Many web applications use the PHP server-side scripting language so
   faster processing of PHP can help a lot. On PHPBench, which measures
   PHP performance, the Raspberry Pi 4 B scored 101,540, more than double
   the Pi 3 B+'s mark of 41,351.

  A.I., Inference and Machine Learning

   Perhaps the most exciting new use case for the Raspberry Pi 4 is for
   inference and machine learning. With the earlier Pis, you could already
   use a camera to do simple object detection at low frame rates, but the
   added performance and I/O from this new model should open up a whole
   new world of use cases.

   To see how well the Pi 4 handles object detection, we followed the
   steps in this tutorial, which uses a combination of Google’s TensorFlow
   machine learning platform and OpenCV, a programming library that’s good
   for computer vision. After spending a good three hours compiling and
   installing all the software, I got the app running and watched as the
   webcam identified a few -- very few -- objects in my office, including
   sensing that I was a “person” and my chair was a “chair” with great
   confidence. It operated at a sluggish rate of 1.7 fps, but that’s 70
   percent better than the 1 fps I got when running it on the Pi 3 B+.

   However, with a more optimized framework, the Pi 4 should be able to do
   real-time facial and object recognition. And, because it has USB 3, an
   accelerator like the Google Coral TPU USB dongle should have much more
   bandwidth to send data back to the SoC. Imagine building a home
   companion robot that recognizes every member of your household by face
   or one that helps a farmer sort cucumbers by type. Some of these
   workloads are possible on earlier Raspberry Pi computers, but the Pi 4
   B should make them fast and accurate enough to use on a regular basis.
   We can’t wait to see what developers and what makers do with Pi 4 and
   A.I.

   Scikit-learn is a popular python module that enables machine learning.
   Performing a task in Scikit-learn was more than twice as quick on the
   Pi 4 B.

  Compiling Code

   With Linux, you sometimes have to compile programs you want to install.
   Several times during our testing, we had to compile software packages,
   including when we wanted to get an object recognition demo going.

   A speedier processor and better RAM help the Raspberry Pi 4 B compile
   code much faster than its predecessor.  When we ran a test which
   compiles a Linux Kernel, the 4 B finished 33 percent faster. So,
   whether you're a developer who is writing your own software or just a
   user who wants a program that's not available as a direct download, the
   Pi 4 will save you time.

  Overclocking the Pi 4

   We’ve explained how to overclock the Raspberry Pi 4 and what kind of
   results you get in a separate article. However, the top line is that
   you can easily get the 1.5 GHz CPU up to 2 GHz and increase the
   frequency of the GPU from 500 to 600 MHz without missing a beat. Just
   make sure that you have a fan like the Pimoroni Fan Shim.

  How Much Raspberry Pi 4 RAM Do You Need?

   The Raspberry Pi 4 B comes in three configurations, which are identical
   but for the amount of RAM. The $35 entry-level model has 1GB of RAM,
   the $45 unit has 2GB and the $55 SKU goes all the way to 4GB. One of
   the great advantages of all Raspberry Pis is that they are affordable
   enough to use in anything, so you need to choose wisely. If you are
   building a robot or other iOT device that just deals with motors and
   sensors, 1GB should be enough, because you aren’t running a slew of
   apps and you don’t even need the GUI.

   We recommend 2GB if you’re doing very light web surfing, setting up a
   kiosk or deploying a limited-use web server. The 4GB model is ideal for
   using your Pi as a PC or for more complex tasks such as A.I.

  Bottom Line

   The Raspberry Pi 4 represents a giant leap forward, not only for the
   Raspberry Pi, but for single-board computing. For the first time, it’s
   realistic to use your Pi as a secondary or backup PC (or perhaps a
   kids’ first PC). However, the larger real benefit will come not from
   folks who use Raspberry Pi 4s in lieu of x86 PCs, but from all the
   innovators who harness the system’s enhanced performance, I/O and
   graphics to create new iOT devices, media servers and robots. Kids
   building Pi projects in school will also have a world of new learning
   possibilities.

   If you need a Raspberry Pi computer today, though, you’ll have to live
   with some issues that are likely to get resolved in the near future via
   software updates. Key apps like Retropie don't yet run Raspberry Pi 4
   and video playback performance is disappointing. While it’s certain
   that major applications will be ported to the new computer, we still
   don’t know exactly how good video playback will get once the operating
   system is refined over time.

   Despite these small issues , the Pi 4 stands head and shoulders above
   its predecessors and all the other inexpensive single board computers
   on the market. The main question isn’t: what can the Pi 4 do for you
   out of the box, but what can you make with it?

   Editor’s Note: Some of the benchmarks in this article were performed by
   co-author Gareth Halfacree, who has posted his own, detailed analysis
   of Raspberry Pi 4 performance on Medium.

   Image Credits: Tom's Hardware, Gareth Halfacree

   MORE: Raspberry Pi GPIO Pinout: What Each Pin Does

   MORE: How to Use Raspberry Pi as a VPN Gateway

   MORE: Raspberry Pi Tutorials

   [javascript]
   Raspberry Pi 4
   $35CanaKit

   About the author
   Avram Piltch & Gareth Halfacree @geekinchief

   Avram Piltch is Tom's Hardware's editor-in-chief. When he's not playing
   with the latest gadgets at work or putting on VR helmets at trade
   shows, you'll find him rooting his phone, taking apart his PC or coding
   plugins. With his technical knowledge and passion for testing, Avram
   developed many real-world benchmarks, including our laptop battery
   test.
   Read more
     * Raspberry Pi

   46 comments
   Comment from the forums
       Your comment
     *
   TerryLaze [X]
       [ ]
       Any plans on comparing the atomic PI to the raspberry?
       It's based on x86 so it would be much more compatible with what
       people need to run.
       https://dlidirect.com/products/atomic-pi
     *
   bit_user [X]
       [ ]
       I'm pretty sure your table has the GPU specs swapped. It says the
       Pi 3 has a 500 MHz VidCore VI and the Pi 4 has a 400 MHz VidCore
       IV.
       Speaking of the GPU, I'm really interested in support for OpenCL,
       Vulkan, and which OpenGL version it supports. Any other specs on
       the GPU would also be welcome!
       I should add that I'm actually a bit disappointed by the OpenArena
       benchmark. Given how old and slow the earlier Pis' GPU was (yes,
       even the Pi 3 used the same GPU as the original model that launched
       in 2012, just clocked a bit higher), I fully expected a much bigger
       jump. Think about how far desktop GPUs have come in that time -
       since Nvidia's GeForce 600 series and AMD's HD 7000 series.
     *
   bit_user [X]
       [ ]

     Quote:

     Any plans on comparing the atomic PI to the raspberry? It's based on
     x86 so it would be much more compatible with what people need to
     run. https://dlidirect.com/products/atomic-pi
       Overall, I think the Pi 4 would definitely win. The Atomic's SoC
       was designed for cell phones, that were none too popular. The
       Cortex-A72 cores in the Pi 4 are a bit newer and more efficient.
       Also, the Atomic Pi uses single-channel DDR3L, while the Pi 4 uses
       DDR4L.
       Of course, the Pi 4's biggest weakness remains storage. So, for I/O
       intensive tasks, the Atomic would still pull out some wins.
     *
   bit_user [X]
       [ ]
       In terms of specs, the best (sub-$100, at least) is still the
       ODROID N2:
       https://www.hardkernel.com/shop/odroid-n2-with-4gbyte-ram/
     *
   LordConrad [X]
       [ ]
       It'll be a nice change to have the support of the Raspberry Pi
       community AND a fast SBC to go with it. I never bought any of their
       previous SBCs because, for an extra $10-20, their competitors were
       always much better.
     *
   AllanGH [X]
       [ ]

     Quote:

     Any plans on comparing the atomic PI to the raspberry
       The Atomic Pi was dead before it was put up for sale....just
       somebody flipping old stock and system pulls from a mini-robot
       product. You won't be seeing support or new iterations of that
       platform--not from the purported manufacturer, at any rate.
     *
   punkncat [X]
       [ ]

     Quote:

     The Atomic Pi was dead before it was put up for sale....just
     somebody flipping old stock and system pulls from a mini-robot
     product. You won't be seeing support or new iterations of that
     platform--not from the purported manufacturer, at any rate.
       Agreed.
       I was taken in at first by all the promise and claims being made.
       Really glad I waited to purchase one until more detail came out.
       It seems that in addition to a fairly high fail rate, there are
       significant problems with audio, the well known poorly dealt with
       power delivery, as well as issues running "heavy" OS like it was
       purported to be capable of.
       The lack of possible future support was the final straw for me.
       This is a once it's gone, it's gone kind of thing. The whole thing
       was made for a different purpose and has connectors and
       functionality that, as of yet are (mostly) unknown and unable to be
       utilized.
       Many of the reviewers stated that even though the audio and OS
       issues were mostly resolvable that the time spent doing so
       outweighed the value. By and large the consensus among many of the
       purchasers was to go RPi for the community, the support, and the
       I/O functionality.
     *
   TJ Hooker [X]
       [ ]
       The 'Everything We Know about the Pi 4" article (from ~4 months
       ago) didn't age well :P
       "Release Date: Not Coming in 2019"
       https://www.tomshardware.com/news/raspberry-pi-4-everything-we-know
       ,38539.html
     *
   technome79 [X]
       [ ]
       The article mentions "true Gigabit Ethernet ". What is true gigabit
       ethernet? And what makes other gigabit ethernet false?
     *
   TJ Hooker [X]
       [ ]

     Quote:

     The article mentions "true Gigabit Ethernet ". What is true gigabit
     ethernet? And what makes other gigabit ethernet false?
       The 'gigabit' ethernet on previous models was fed by a USB 2
       connection, which is not capable of providing actual gigabit
       speeds. The "true gigabit ethernet" now allows for actual Gb/s
       throughput.
     *
   AllanGH [X]
       [ ]
       I never really did a drill-down into the specs before pre-ordering
       2 of the model 4s, and it now occurs to me that I really am hoping
       that RPF decided to add a lucid soft power switch facility to the
       board.
       I suppose I could actually look to see, but that would spoil the
       surprise. Nevertheless, if they didn't, the old method of adding a
       soft-power button will likely still work.
     *
   AlistairAB [X]
       [ ]
       Odroid N2 looks nice. Anyone have a Snapdragon 845 single board
       computer for a low cost? $450 for the Thundercomm one... ouch...
       The new Pi4 might beat/equal the nVidia nano now in CPU
       performance, wild.
     *
   AllanGH [X]
       [ ]
       I'm wondering about the performance, too.....
       My dad needed a computer on his desk at the local Senior's Center,
       where he is their VP, but the lady who is President is a bit of a
       snit (total control freak, actually) about things on the desks in
       the office.
       She outlined an area on his desk that was no larger than an LCD
       monitor, and said that's all he was allocated to use, because
       anything larger would be "ugly". SMH
       I screwed a nicely cased Pi-B3+ onto the back of an LCD monitor,
       hooked it up and gave him a wireless keyboard and mouse combo that
       he can lock in his desk drawer, and he's been deliriously happy
       about it ever since. I'm sure the fact that the President is green
       around the gills over it, doesn't hurt, either. ;)
       I've used it a few times (maintenance tasks, for the most part) and
       do see a bit of a lack in performance with a few things, but I'm
       used to much more horse power out of a desktop system.
       I'm now looking forward to swapping-out the old board for the new
       one, and seeing what sort of difference it makes in terms of
       performance.
     *
   R_1 [X]
       [ ]
       how about putting the price of the unit reviewed rather than the
       price of the cheapest variant?
       you tested and reviewed the 55 dollar 4GB unit, but list the price
       as 35 which is the 1GB unit.
       the link to CanaKit goes to the 1GB unit NOT the unit reviewed.
       if you review model A link to model A and reference model A
     *
   AllanGH [X]
       [ ]

     Quote:

     the link to CanaKit goes to the 1GB unit NOT the unit reviewed
       REALLY?
       [MEDIA=imgur]a/yiksYNH[/MEDIA]
       View: https://imgur.com/a/yiksYNH
       Clickity-click....
     *
   evilpaul [X]
       [ ]
       If you're interested in the Atomic Pi, see if you can get somebody
       to pay for part of the cost of a Compute Stick, cheap NUC, Latte
       Panda, etc for you, and buy one of those instead. It'll be just
       like an Atomic Pi, except functional without soldering crap to it,
       and there will be faster similar products produced to replace it in
       the future. And you can buy 30,001 of them if you want, because
       there's companies manufacturing them rather than pulling them out
       of talking Roombas that never made it to market.
       [USER=96206]@op[/USER], did you try tinkering with chrome://flags
       and seeing if you could force hardware video decoding in Chromium?
       The Youtube channel Explaining Computers's guy seemed to get
       streaming video to work OK in his review. Also, I'm pretty sure
       that ~40 FPS in Quake III Arena was what my Voodoo Banshee managed
       to pull off. Or that might have been the used Voodoo 3 that
       replaced it. Details are fuzzy, for some reason.
     *
   g-unit1111 [X]
       [ ]
       I think this might be the time I actually take the plunge and buy a
       Raspberry Pi. Does anyone know what kind of OS it can support?
       Could I try loading my spare Windows 7 license on it? Will Pi 4
       support memory cards larger than 32GB?
     *
   AllanGH [X]
       [ ]
       OS support is, AFAIK:
       NOOBS - no idea what that is, because I never tried it or read
       anything about it.
       and
       Debian Stretch, ported to the Pi...which works quite well, as long
       as you can ignore all the cutesy icons. o_O
       and
       Debian Jessie and Jessie-Lite, ported to the Pi. Same caveat about
       icons for the full-Jessie release.
       and
       For the Model 4, you'll need to download Debian Buster (still in
       testing), ported to the Pi....and Buster is backward compatible
       with all the Pi boards.
       #### EDIT ####
       I also just ran across this review of the Buster iteration of the
       OS:
       https://www.raspberrypi.org/magpi/raspberry-pi-4-raspbian-buster/
       ############
       There are also several other CLI-only (no-GUI) releases that I've
       tinkered with, but Jessie-Lite was quite a bit better than what I
       saw.
       I'm sure that there are other OS options available for the ARM
       Cortex-A53, and people have spoken of windows on the Pi (win10
       ARM64 or win10 IoT Core), but I've never been interested enough to
       bother looking.
     *
   AllanGH [X]
       [ ]
       Oh....the platform will support up to 128GB MicroSD or SD cards
       (depending on the specific board you have).
       You can get some answers to potential questions here.
       ################
       And I just ran across some third-party OS offerings:
       https://www.raspberrypi.org/downloads/
     *
   shivansps [X]
       [ ]
       Looks fine, the main bottleneck on the PI3 was the i/o, with USB3
       that is gone.
       The PI4 should the able to run Win10 ARM OK if a IGP driver is ever
       released.
       Freespace 2 Demo would have been a much better game to test here...
       IF OpenGL is working in the distro.
       https://www.raspberrypi.org/forums/viewtopic.php?t=230316
     *
   bit_user [X]
       [ ]

     Quote:

     The new Pi4 might beat/equal the nVidia nano now in CPU performance,
     wild.
       Yeah, but the point of the Nvidia platforms wasn't really CPU
       performance.
       Plus, their Nano is just a re-purposed Tegra X1, from 4 years ago.
       I guess they also disabled half of the CUDA cores (probably for
       power & yield reasons). Anyway, surpassing its CPU performance
       isn't really saying much...
     *
   bit_user [X]
       [ ]

     Quote:

     I screwed a nicely cased Pi-B3+ onto the back of an LCD monitor,
       Why not a NUC? ...unless it'd cost too much. AFAIK, they include a
       VESA mounting bracket.
     *
   bit_user [X]
       [ ]

     Quote:

     If you're interested in the Atomic Pi, see if you can get somebody
     to pay for part of the cost of a Compute Stick, cheap NUC, Latte
     Panda, etc for you, and buy one of those instead.
       Or an ODROID-H2, which looks to be an excellent implementation of
       the latest and greatest of Intel's low-power SoCs:
       https://www.hardkernel.com/shop/odroid-h2/
       (BTW, they have US distributors - I think you needn't order it
       direct form SK)
     *
   bit_user [X]
       [ ]

     Quote:

     I think this might be the time I actually take the plunge and buy a
     Raspberry Pi.
       Do it.

     Quote:

     Does anyone know what kind of OS it can support? Could I try loading
     my spare Windows 7 license on it?
       About running Windows on it, here's what their FAQ says:

     Quote:

     As of summer 2015, a version of Windows 10 is available for use on
     the Raspberry Pi 2 and 3. This is an entirely new version of the
     operating system designed exclusively for embedded use, dubbed the
     Windows 10 Internet of Things (IoT) Core. It does not include the
     user interface (shell) or the desktop operating system.
       https://www.raspberrypi.org/documentation/faqs/#pi-software

   Display more comments

   Most Popular
    1. Where to Buy the Raspberry Pi 4, Including the 4GB Model
    2. Modular 'Pi-Tops' Raise Safety Concerns After Student 'Burns'
       Finger
    3. How to Create Custom Keyboard Shortcuts for Raspberry Pi

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate next

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * Reference

             About Tom’s Hardware: Our Staff, Ratings and History

   by The Staff of Tom's Hardware September 9, 2019 at 9:42 AM

     *
     *
     *
     *
     *
     *

   [ ] (*) ( ) ( ) ( ) ( ) ( )
   Page 1:Our Mission
   Page 2:Our Team
   Page 3:Our Contributors
   Page 4:How We Test and Rate Products
   Page 5:22 Years of Tom's Hardware History
   Page 6:Europe, Sister Sites & Contact
     *

Page 1:Our Mission
     *

Page 2:Our Team
     *

Page 3:Our Contributors
     *

Page 4:How We Test and Rate Products
     *

Page 5:22 Years of Tom's Hardware History
     *

Page 6:Europe, Sister Sites & Contact

Our Mission

   Tom's Hardware is the leading destination for tech enthusiasts of all
   skill levels. Whether you're building a PC, buying a laptop or learning
   how to create robots with your kids, we've got comprehensive editorial
   resources and a vibrant expert community to help you on your journey.

Our Team

Avram Piltch (@geekinchief)

   Avram Piltch Avram PiltchAvram's been in love with PCs since he played
   original Castle Wolfenstein on an Apple II+.  Before joining Tom's
   Hardware, for 10 years, he served as Online Editorial Director for
   sister sites Tom's Guide and Laptop Mag, where he programmed the CMS
   and many of the benchmarks. When he's not editing, writing or stumbling
   around trade show halls, you'll find him building Arduino robots with
   his son and watching every single superhero show on the CW.

   Contact Avram: Email | Twitter

Matt Safford, Managing Editor (@mattsafford)

   Matt Safford Matt SaffordMatt began piling up computer experience as a
   child with his Mattel Aquarius. He built his first PC in the late 1990s
   and ventured into mild PC modding in the early 2000s. He’s spent the
   last decade covering emerging technology for Smithsonian, Popular
   Science, and Consumer Reports, while testing components and PCs for
   Computer Shopper and Digital Trends. When not writing about tech, he’s
   often walking—through the streets of New York, over the sheep-dotted
   hills of Scotland, or just at his treadmill desk at home in front of
   the 50-inch 4K HDR TV that serves as his PC monitor.

   Contact Matt: Email | Twitter

Anj Bryant, Assistant Managing Editor (@anjbryant)

   Anj Bryant Anj Bryant

   Anj provides content layout and development support, and coordinates
   editorial initiatives for the talented group of authors and editors at
   Tom's Hardware. She enjoys putting her love for technology and her past
   IT experience to good use. With a background in Enterprise software
   that started with Cybermedia she eventually caught the hardware bug and
   hasn't looked back. Outside of Tom's, she's mom to two tech-savvy girls
   who keep her busy with questions about Minecraft modding.

   Contact Anj: Email | Twitter

Scharon Harding, Senior Editor (@ScharHar)

   Scharon Harding Scharon HardingScharon helps out with any and all
   articles on Tom’s Hardware with a special affinity for laptops and
   desktops, virtual reality and monitors. She previously covered business
   technology, including hardware, software, cyber security and other IT
   happenings, at Channelnomics. When she’s not exploring all things PC,
   Scharon is usually outdoors searching for sunshine, trees and music, or
   watching movies she should’ve seen ages ago (sorry, Infinity War).

   Contact Scharon: Email | Twitter

Thomas Soderstrom, Senior Editor

   Thomas Soderstrom Thomas SoderstromThomas oversees the motherboard,
   cooling and case review teams while handling most of our DRAM reviews
   himself. After starting with hardware mod guides in our community
   forums, he freelanced for other sites before returning to Tom’s
   Hardware as motherboard editor. After he puts the mouse down for the
   day, he enjoys hiking, biking, and practical applications of skilled
   trades, often while spending time with his family.

   Contact Thomas: Email | Twitter

Paul Alcorn, Senior Editor (@PaulyAlcorn)

   Paul Alcorn Paul AlcornAs a teenager, Paul scraped up enough money to
   buy a 486-powered PC with a turbo button (yes, a turbo button). Back
   when floppies were still popular he was already chasing after the
   fastest spinners for his personal computer, which led him down the long
   and winding storage road, covering enterprise storage. His current
   focus is on consumer processors, though he still keeps a close eye on
   the latest storage news. In his spare time, you’ll find Paul hanging
   out with his kids or indulging his love of the Kansas City Chiefs and
   Royals.

   Contact Paul: Email | Twitter

Andrew E. Freedman, Editor (@FreedmanAE)

   Andrew Freedman Andrew FreedmanAndrew oversees laptop and desktop
   coverage and keeps up with the latest news in tech and gaming. His work
   has been published in Kotaku, PCMag, Complex, Tom’s Guide and Laptop
   Mag, among others. He fondly remembers his first computer: a Gateway
   that still lives in a spare room in his parents' home, albeit without
   an internet connection. When he’s not writing about tech, you can find
   him playing video games, checking social media and waiting for the next
   Marvel movie.

   Contact Andrew: Email | Twitter

Kenneth Butler, Social Media Editor (@KRichButler)

   Kenneth Butler Kenneth ButlerKenneth digs into the world of enthusiast
   PC and tech culture to help tell stories that get readers looking,
   voting, sharing, thinking and laughing on social media platforms. He’s
   worked as a fact checker, staff writer, and production director for
   Laptop Mag and Tom’s Guide. Off hours, his hobbies include early
   morning runs, writing comedy, obsessing over details in the Marvel
   Cinematic Universe and planning his ultimate Halloween costume, Major
   Payne.

   Contact Kenneth:Email | Twitter

Joe Pishgar, Community Director (@Pishgar)

   Joe Pishgar Joe PishgarJoe is our head of community, managing an
   international team of administrators, moderators, and content
   specialists who keep our communities civil, safe and helpful. Joe
   previously led community efforts in game development for Disney, Sony
   and Microsoft as a subject matter expert on user-generated content,
   engagement, and retention. When not managing the community, Joe is
   gaming, writing or hiking.

   Contact Joe: Email | Twitter

Josh Simenhoff, Community Manager

   Josh Simenhoff Josh SimenhoffJosh helps Joe manage the forums and serve
   the millions of Tom’s Hardware members across the globe. In this role,
   he helps our moderators maintain a supportive and engaging community.
   Josh also assists editorial on topics such as gaming and
   cryptocurrency. In his free time, Josh enjoys wargaming, boardgaming or
   writing.

   Contact Josh: Email

Our Contributors

     * Christian Eberle, Contributing Writer (Monitors)
     * Aris Mpitziopoulos, Contributing Writer (Power Supplies)
     * Nathaniel Mott, Contributing Writer (News)
     * Zhiye Liu, Contributing Writer (News)
     * Amy Oztan, Contributing Writer (Digital Parenting)
     * Lucian Armasu, Contributing Writer (News)
     * Kevin Carbotte, Contributing Writer (Graphics, News & VR)
     * Adam Darling, Contributing Page Setter (Layout)
     * Sean Webster, Contributing Writer (Storage)
     * Jacob Terkelsen, Contributing Writer (Motherboards)
     * Garrett Carver, Contributing Writer (Coolers)
     * Steven Lynch, Contributing Writer (Cases)
     * Matthew Connatser, Contributing Writer (News)
     * Arne Verheyde, Contributing Writer (News)
     * Allen 'Splave' Golibersuch, Contributing Writer (Overclocking)
     * Joe Shields, Contributing Writer (Motherboards)
     * LowSpecGamer Alex, Contributing Writer (Gaming)

How We Test and Rate Products

   Tom’s Hardware is renowned for its benchmark testing. We subject every
   product we review to a rigorous set of quantifiable tests based on a
   combination of homegrown, Tom’s Hardware-only benchmarks, and industry
   standard benchmarks where applicable.

   As of May 2018, all new product reviews are rated on a scale of 1 to 5,
   with 5 being the best. Each product may also receive an Editor's Choice
   badge, which designates it as the best within its niche. The ratings
   mean the following:

   5 = Practically perfect

   4.5 = Superior

   4 = Totally worth it

   3.5 = Very good

   3 = Worth considering

   2.5 = Meh

   2 = Not worth the money

   1.5 = Buy for an enemy

   1 = Fails horribly

   0.5 = Laughably bad

22 Years of Tom's Hardware History

   Tom’s Hardware has its name and roots in Dr. Thomas Pabst, who was one
   of the first people to bring technology journalism to the internet, as
   early as 1996. Back in these early days, the site was still called
   “Tom’s Hardware and Performance Guide” and its domain was
   sysdoc.pair.com, pair.com being a Pittsburgh-based hosting company.

   One of Tom’s Hardware’s journalistic milestones was Tom’s findings
   regarding the Intel Pentium III 1.13 GHz processor, which forced the
   chip company to postpone its launch by months. Since then, Tom’s
   Hardware has kept up the tradition with unrivaled scrutiny of
   technology.

   The current domain, tomshardware.com, was added on September 11, 1997,
   followed by additional language versions, including the French, German
   and Italian sites, all of which are run by independent teams. Pabst
   moved on to other pursuits in 2008, Tom's Hardware and sister
   site, Tom's Guide, became part of the Purch company in 2013 and Purch
   was purchased by Future Plc in 2018.

   Today's Tom's Hardware is more than just a site for PC builders. While
   we've maintained our rich tradition of thorough component testing and
   reviews, we've expanded our coverage to meet a broader swath of
   enthusiasts with different needs and levels of experience. If you'd
   rather buy a laptop or desktop, you're on your first PC build or you
   want to share your love of tech with your family, we're there to
   empower you with accessible editorial and a helpful, supportive
   community.

Europe, Sister Sites & Contact

Sister Sites

     * Tom's Guide: Our similarly-named sister site is designed to serve a
       slightly broader audience than Tom's Hardware. Where we serve
       enthusiasts, Tom's Guide focuses on mainstream consumers.
     * Laptop Mag: The leading destination for laptop reviews and buying
       advice.

International Tom's Hardware Sites

     * Tom's Hardware France
     * Tom's Hardware Italy
     * Tom's Hardware UK

Contact Info

   Tom's Hardware
   Future Plc
   11 West 42nd Street, 15th Floor
   New York, NY 10036

   Advertise With Us

   About the author
   The Staff of Tom's Hardware

   Most Popular
    1. Laptop Prices Will Jump 19% Due to Tariffs, Report Claims
    2. Net Neutrality Fights Back With the Save the Internet Act of 2019
    3. Microsoft Makes Gaming More Accessible With the Xbox Adaptive
       Controller (Updated)

     * Latest in
          +
        News

Laptop Prices Will Jump 19% Due to Tariffs, Report Claims
            Laptop Prices Will Jump 19% Due to Tariffs, Report Claims
          +
        Net Neutrality Fights Back With the Save the Internet Act of 2019
        News

Net Neutrality Fights Back With the Save the Internet Act of 2019
            by Nathaniel Mott Mar 6, 2019, 10:42 AM
            Microsoft Makes Gaming More Accessible With the Xbox Adaptive
            Controller (Updated)
        News

Microsoft Makes Gaming More Accessible With the Xbox Adaptive Controller
(Updated)
            by Kevin Carbotte Jul 25, 2018, 7:50 AM
     *

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate prev next

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * CPUs

     Review

      AMD Ryzen 9 3900X and Ryzen 7 3700X Review: Zen 2 and 7nm Unleashed

   by Paul Alcorn July 7, 2019 at 7:07 AM

     *
     *
     *
     *
     *
     *

   180 Comments

   [ ] ( ) (*) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
   Page 1:Into the 7nm Era
   Page 2:7nm Process, Zen 2, and the X570 Chipset
   Page 3:Ryzen 3000 IPC Measurements and Power Consumption
   Page 4:Overclocking and Ryzen Master
   Page 5:Updated Test Metholodgy and Setup
   Page 6:VRmark, 3DMark and AotS: Escalation
   Page 7:Civilization VI Graphics and AI, Dawn of War III
   Page 8:Far Cry 5 and Final Fantasy XV
   Page 9:GTA: V and Hitman 2
   Page 10:Project Cars 2, The Division 2, and World of Tanks enCore
   Page 11:Office, Web Browser, and Productivity
   Page 12:Rendering, Encoding, Compression, Encryption
   Page 13:Conclusion
     *

Page 1:Into the 7nm Era
     *

Page 2:7nm Process, Zen 2, and the X570 Chipset
     *

Page 3:Ryzen 3000 IPC Measurements and Power Consumption
     *

Page 4:Overclocking and Ryzen Master
     *

Page 5:Updated Test Metholodgy and Setup
     *

Page 6:VRmark, 3DMark and AotS: Escalation
     *

Page 7:Civilization VI Graphics and AI, Dawn of War III
     *

Page 8:Far Cry 5 and Final Fantasy XV
     *

Page 9:GTA: V and Hitman 2
     *

Page 10:Project Cars 2, The Division 2, and World of Tanks enCore
     *

Page 11:Office, Web Browser, and Productivity
     *

Page 12:Rendering, Encoding, Compression, Encryption
     *

Page 13:Conclusion

7nm Process, Zen 2, and the X570 Chipset

TSMC 7nm Process

   AMD tapped TSMC's 7nm process for the Ryzen 3000 series processors.
   AMD's first-gen Ryzen processors debuted on GlobalFoundries' 14nm GPP
   node, but the 2000-series CPUs moved to GloFo's 12nm LP process
   technology. The ported-over design helped boost transistor performance,
   but did not affect die area or transistor density. As a result,
   Pinnacle Ridge's ~4.8 billion transistors and 213mm^2 area remained the
   same as first-gen Ryzen.

   (*) ( ) ( ) ( )
     * Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-006
       Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-006
     * Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-007
       Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-007
     * Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-008
       Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-008
     * Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-013
       Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-013

   (*) ( ) ( ) ( )
     * Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-006

Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-006
     * Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-007

Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-007
     * Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-008

Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-008
     * Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-013

Mark_Papermaster-Next_Horizon_Gaming-Architecture_06092019-page-013

   In contrast, TSMC's 7nm process represents a true shrink that provides
   twice the transistor density. AMD says the process allowed it to shrink
   the CCX by 29% relative to the 12nm process, which helped pave the way
   for Zen 2's enhancements, like the doubled L3 cache capacity and the
   ability to double core counts within the same package dimensions. It's
   also important to note that the company also removed I/O and memory
   controllers from the compute chiplet this time around, resulting in
   even smaller packages.

   AMD also claims the process affords up to 350 more MHz of core
   frequency at the same voltage as the 12nm LP process. The new process
   also delivers on the energy efficiency front with up to 75% higher
   performance-per-watt compared to the 12nm LP process. AMD also says the
   7nm node produces up to 58% higher performance-per-watt than Intel's
   aging, but highly refined, 14nm++ process, but be aware that the Ryzen
   3000 chips still have a 12nm I/O die that contributes to the chip's
   overall power consumption.

   AMD says that the 15% increase in IPC throughput from the Zen 2
   microarchitecture serves up 60% of the performance improvements seen in
   the Ryzen 3000 series chips, with the remaining 40% coming from the 7nm
   process and frequency improvements.

Zen 2 Microarchitecture

   Zen's modular and scalable design provides AMD with plenty of
   advantages in terms of cost and time to market, and fine-grained tuning
   to the architecture has yielded phenomenal results.

Credit: AMD Credit: AMD

   AMD has improved IPC by roughly 15% (though that can vary by workload),
   doubled the L3 cache size to keep data as close to the execution units
   as possible, and doubled floating point performance by expanding
   floating point bandwidth to 256-bit to improve performance with AVX2
   instructions.

   (*) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-002
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-002
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-004
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-004
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-003
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-003
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-005
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-005
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-006
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-006
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-007
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-007
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-008
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-008
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-009
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-009
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-010
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-010
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-011
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-011
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-012
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-012
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-013
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-013
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-020
       Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-020

   (*) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-002

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-002
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-004

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-004
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-003

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-003
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-005

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-005
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-006

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-006
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-007

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-007
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-008

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-008
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-009

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-009
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-010

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-010
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-011

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-011
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-012

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-012
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-013

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-013
     * Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-020

Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-020

   Headline improvements include a doubled micro-op (4K) and L3 cache
   (32MB), which came at the expense of a slightly smaller L1 instruction
   cache that is now an 8-way associative 32K block as opposed to the 64K
   block with 4-way associativity on first-gen Ryzen. AMD also beefed up
   the Translation Lookaside Buffer (TLB) to 2,000 entries.

   AMD now has a double-stage branch predictor, with its Perceptron
   predictor handling the first stage while a new TAGE branch predictor,
   which features larger lookup tables to improve performance, serves as
   the second stage. That's paired with larger L1 and L2 branch target
   predictors (BTBs) that increase throughput by reducing stalls. AMD says
   the improved branch predictor expends extra energy on the front end,
   but the 30% lower misprediction rate ultimately saves more energy on
   the backend. A third address generation unit (AGU) also keeps the
   hungry execution cores fed with data from main memory.

Enter the X570 Chipset

   AMD's Ryzen 3000 series processors have dynamic algorithms that adjust
   parameters based upon several factors, with power delivery and heat
   dissipation being the chief variables that can unlock extra
   performance. As such, motherboard selection is going to be a big factor
   in the amount of performance you receive if you choose to use AMD's
   Precision Boost Overdrive (PBO - next page) and Auto Overclock
   features. The X570 motherboard ecosystem has proven to be pricey
   compared to the previous-gen X470 models, but AMD says that the Ryzen
   3000 series processors will operate at full performance at stock
   settings with X470 motherbaords. You just sacrifice access to the PCIe
   4.0 interface. Credit: AMD Credit: AMD

   The actual X570 chipset is a 14nm variant of the 12nm I/O die inside
   the Ryzen 3000-series processors, which is a clever reuse of the
   technology that ultimately lowers costs. This chipset is also fully
   produced by AMD, whereas the X470 chipset came from ASMedia, which says
   it will continue to produce some chipsets for Ryzen 3000 series
   processors. AMD uses the smaller 12nm process for the processor's
   in-package I/O die to leverage the increased frequency potential for
   the memory controllers. That improves memory data transfer rates, but
   AMD uses the more economical 14nm variant, which has its memory
   controllers disabled, for the chipset die. Credit: AMD Credit: AMD

   Most X570 motherboards come with a fan on the chipset to provide active
   cooling for the chipset, which now consumes ~11-15W compared to the 6W
   consumed by the X470 chipset. That's due to the power-hungry nature of
   the PCIe 4.0 interface when it is under full load. Small fans like
   these tend to be noisy, but our motherboard team is hard at work on the
   first wave of X570 motherboard reviews and will provide more
   perspective.

   Credit: AMD Credit: AMD

   Ryzen 3000-series chips are compatible with most previous-gen
   motherboards with the AM4 socket, but some updates are left to vendor
   discretion. As such, you won't be able to drop a new Third-gen Ryzen
   chip into all X370 and B350 motherboards, and A320's upgrade path is
   blocked entirely. Due to the uneven application of BIOS updates across
   the various vendors, and even among different motherboards in the
   respective product stacks, you'll have to check the CPU support list
   for your X370 or B350 motherboard to ensure it supports Third-gen
   Ryzen.

   (*) ( )
     * AMD-socket-am4-motherboard-ryzen-compatibility-chart
       AMD-socket-am4-motherboard-ryzen-compatibility-chart
     * AMD-ryzen-3000-series-processor-compatible-x570-chipset
       AMD-ryzen-3000-series-processor-compatible-x570-chipset

   (*) ( )
     * AMD-socket-am4-motherboard-ryzen-compatibility-chart

AMD-socket-am4-motherboard-ryzen-compatibility-chart
     * AMD-ryzen-3000-series-processor-compatible-x570-chipset

AMD-ryzen-3000-series-processor-compatible-x570-chipset

   There are plenty of X470 and B450 motherboards still on the market, but
   some of the boards that have been in the supply chain for a while will
   need a BIOS update before you install a Third-gen Ryzen processor.
   As we've seen in the past, that isn't always possible if you don't
   already have a Ryzen processor or if the motherboard doesn't have an
   out-of-band BIOS update feature, like BIOS Flashback. AMD also
   announced that all motherboards that support Ryzen 3000 processors out
   of the box will come with a new badge to help simplify things.

   MORE: Best CPUs

   MORE: Intel & AMD Processor Hierarchy

   MORE: All CPUs Content

   [javascript]
   AMD Ryzen 7 3700X
   $329.99Newegg

   Previous Next

   Summary
    1. Into the 7nm Era
    2. 7nm Process, Zen 2, and the X570 Chipset
    3. Ryzen 3000 IPC Measurements and Power Consumption
    4. Overclocking and Ryzen Master
    5. Updated Test Metholodgy and Setup
    6. VRmark, 3DMark and AotS: Escalation
    7. Civilization VI Graphics and AI, Dawn of War III
    8. Far Cry 5 and Final Fantasy XV
    9. GTA: V and Hitman 2
   10. Project Cars 2, The Division 2, and World of Tanks enCore
   11. Office, Web Browser, and Productivity
   12. Rendering, Encoding, Compression, Encryption
   13. Conclusion

   About the author
   Paul Alcorn @PaulyAlcorn

   Paul Alcorn is a Senior Editor for Tom's Hardware US. He writes news
   and reviews on CPUs, storage and enterprise hardware.
   [javascript]
   Read more
     * CPUs
     * AMD
     * Components

   180 comments
   Comment from the forums
       Your comment
     *
   Isokolon [X]
       [ ]
       too bad there wasn't a 3800X included, would be interesting to see
       if the price tag for the 3800X over the 3700X is indeed worth it
     *
   feelinfroggy777 [X]
       [ ]
       Nice for worksation task, but disappointing it basically ties Intel
       in gaming if not just a tad behind.
     *
   salgado18 [X]
       [ ]

     Quote:

     Nice for worksation task, but disappointing it basically ties Intel
     in gaming if not just a tad behind.
       It tied to Intel for less money, with a great bundled cooler, and a
       cheaper platform (edit: and less power too). Also, unless you use a
       144Hz monitor, the diference is purely synthetic. Did you expect it
       to be way faster than a 5 GHz Intel magically?
     *
   Phaaze88 [X]
       [ ]

     Quote:

     Thank you so much and to be honest the only reason why I bought the
     750tx Corsair psu is because it was 30$, can I ask you if the gtx770
     2gb will run battlefield 1 on high?

     Quote:

     too bad there wasn't a 3800X included, would be interesting to see
     if the price tag for the 3800X over the 3700X is indeed worth it
       I can't imagine it would be.
       If looking at the 3800x as a binned 3700x - that's basically what
       it would be - grab one if it goes on sale closer to the 3700x's
       price.
       These chips don't overclock any better than their predecessors,
       which wasn't good to begin with, so whatever extra clocks you get
       with a 3800x will hardly be noticeable and won't be worth a $50+
       price increase over 3700x.
     *
   velocityg4 [X]
       [ ]
       The overclocking results were disappointing. 4.1Ghz max on all
       cores. Given that the 3950x does a 4.7Ghz single core turbo boost
       and the 3900x does 4.6Hz single core turbo boost. I'd have assumed
       any of the Ryzen 3000 would OC to 4.6/4.7Ghz on all cores with
       decent air/water cooling.
       Power consumption: AIDA 64 seems to punish AMD a lot more than
       Intel. When you were using Prime95 Intel was punished a lot more.
       It seems the switch from Prime95 to AIDA 64 gives Intel an unfair
       advantage in the stress test power consumption test. While Prime95
       gave AMD an unfair advantage. I'd suggest using both in reviews or
       find another torture test that will fully punish both AMD and Intel
       for a max load test. With such wild variation. I can't see how
       either is an accurate measure of a CPU under full load.
       Example Review:
       https://www.tomshardware.com/reviews/intel-core-i9-9900k-9th-gen-cp
       u,5847-11.html
       The Intel i9-9900K hit 204.6W in your old reviews stress test. This
       time it is only 113W.
       The AMD Ryzen 2700x hit 104.7W in your old review. Now it is 133W.
     *
   feelinfroggy777 [X]
       [ ]

     Quote:

     It tied to Intel for less money, with a great bundled cooler, and a
     cheaper platform (edit: and less power too). Also, unless you use a
     144Hz monitor, the diference is purely synthetic. Did you expect it
     to be way faster than a 5 GHz Intel magically?
       It did tie Intel in gaming. It tied basically the same Intel CPUs
       that have been on the market since 2015 with Skylake. We are in the
       back half of 2019 and we see the same gaming performance that we
       had in 2015 mainstream CPUs.
       We know AMD is cheaper and comparing clockspeeds against different
       architectures between Intel and AMD is silly. But it would be nice
       to see some tangible improvement regarding fps with CPUs. The GPU
       still remains king when it comes to a quality gaming build.
     *
   delaro [X]
       [ ]
       I've seen reviews from 5 different sites and the conclusions bounce
       all over the place, which makes me think there is much to do on the
       software optimization side. :unsure: I was expecting gaming FPS to
       not change all that much with many of the titles being tested have
       partnered or optimized around Intel.
     *
   jimmysmitty [X]
       [ ]

     Quote:

     Nice for worksation task, but disappointing it basically ties Intel
     in gaming if not just a tad behind.
       Minus the ability to overclock yes tied. Most people who buy the
       9900K will not be buying it to leave it stock.

     Quote:

     The overclocking results were disappointing. 4.1Ghz max on all
     cores. Given that the 3950x does a 4.7Ghz single core turbo boost
     and the 3900x does 4.6Hz single core turbo boost. I'd have assumed
     any of the Ryzen 3000 would OC to 4.6/4.7Ghz on all cores with
     decent air/water cooling. Power consumption: AIDA 64 seems to punish
     AMD a lot more than Intel. When you were using Prime95 Intel was
     punished a lot more. It seems the switch from Prime95 to AIDA 64
     gives Intel an unfair advantage in the stress test power consumption
     test. While Prime95 gave AMD an unfair advantage. I'd suggest using
     both in reviews or find another torture test that will fully punish
     both AMD and Intel for a max load test. With such wild variation. I
     can't see how either is an accurate measure of a CPU under full
     load. Example Review:
     https://www.tomshardware.com/reviews/intel-core-i9-9900k-9th-gen-cpu
     ,5847-11.html The Intel i9-9900K hit 204.6W in your old reviews
     stress test. This time it is only 113W. The AMD Ryzen 2700x hit
     104.7W in your old review. Now it is 133W.
       Its what I wanted to know. Ryzen has always been pushed to the
       limit in terms of clock speed and Zen 2 is no different it seems.
       Little to no headroom. AnandTech was able to get it to 4.3GHz all
       core but with manual OCing it seems to disable boost clocking which
       in turn cuts 300MHz from single core performance.
       As for the power consumption, the differences are probably what
       they prioritize. I know Prime 95 heavily uses AVX which is a power
       hog. Not as sure on AIDA 64 since I never used it. I always use
       Prime 95 and IBT for stability.

     Quote:

     It did tie Intel in gaming. It tied basically the same Intel CPUs
     that have been on the market since 2015 with Skylake. We are in the
     back half of 2019 and we see the same gaming performance that we had
     in 2015 mainstream CPUs. We know AMD is cheaper and comparing
     clockspeeds against different architectures between Intel and AMD is
     silly. But it would be nice to see some tangible improvement
     regarding fps with CPUs. The GPU still remains king when it comes to
     a quality gaming build.
       Its not silly to compare clock speeds as those can be advantages.
       Intel still clearly has a clock speed advantage and that advantage
       will keep them priced higher. We might see some drops but I doubt
       we will see enough to make it feel like Athlon 64 again.
       As much crap as people give Intel for getting stuck at 14nm I have
       to give them props for having a 5 year old process tech beat modern
       process tech, especially one that's supposed to be "half" the size.
       I know its not quite as most 7nms out there are still less dense
       than Intels initial 10nm plans but still it goes to show that the
       nm part has become pointless and a marketing gimmick more than
       anything.
       The only thing a CPU matters gaming wise is how long it will last
       before it will bottleneck the GPU. While its still early the clock
       speed and overclocking advantage Intel has might make their CPUs
       last longer in gaming than Zen 2. Only time will tell but maybe AMD
       will get a better process tech in a few years and finally compete
       like the old days.
     *
   martinch [X]
       [ ]

     Quote:

     Its not silly to compare clock speeds [between AMD and Intel] as
     those can be advantages.
       Unless you're trying to give an indication of "performance-per-MHz"
       of varying architectures, yes, comparing clock speeds between
       differing architectures is a fundamentally invalid comparison (it's
       also not exactly an accurate predictor of per-core performance).
     *
   feelinfroggy777 [X]
       [ ]

     Quote:

     Its not silly to compare clock speeds as those can be advantages.
     Intel still clearly has a clock speed advantage and that advantage
     will keep them priced higher. We might see some drops but I doubt we
     will see enough to make it feel like Athlon 64 again.
       Clockspeeds between AMD and Intel are not apples to apples.
       Bulldozer hit 5ghz and it was a terrible CPU. Just because it could
       hit 5ghz, did not make a good chip.
     *
   InvalidError [X]
       [ ]

     Quote:

     Unless you're trying to give an indication of "performance-per-MHz"
     of varying architectures, yes, comparing clock speeds between
     differing architectures is a fundamentally invalid comparison (it's
     also not exactly an accurate predictor of per-core performance).
       Even real-world aren't reliable indicators unless you can find
       benchmarks of the specific software and task within that software
       you are interested in. Anything else is a general indicator at
       best.
       So, I predicted all-core overclocks on par with XFR boost at best
       and it seems Tom's review samples get nowhere near that on
       practical cooling. Looks like AMD is doing a pretty good job of
       leaving little to nothing for practical overclockers to do with the
       X-chips already squeezing everything they are worth out of
       themselves with little more than a few checkboxes.
     *
   feelinfroggy777 [X]
       [ ]

     Quote:

     Even real-world aren't reliable indicators unless you can find
     benchmarks of the specific software and task within that software
     you are interested in. Anything else is a general indicator at best.
     So, I predicted all-core overclocks on par with XFR boost at best
     and it seems Tom's review samples get nowhere near that on practical
     cooling. Looks like AMD is doing a pretty good job of leaving little
     to nothing for practical overclockers to do with the X-chips already
     squeezing everything they are worth out of themselves with little
     more than a few checkboxes.
       They pretty much overclock themselves these days.
     *
   yeti_yeti [X]
       [ ]
       I think this Ryzen lineup is great and delivers awesome
       performance, especially in professional applications. Some people
       seem to be disappointed, that Ryzen 7/9 didn't beat their Intel
       counterparts in gaming, which was something a lot of
       people(myself included) didn't think was going to happen anyway.
       However, I do think that AMD could have been a bit more transparent
       and modest, when showcasing their own benchmarks of their products
       beating or matching the competing Intel chips, which would result
       in less people being let down.
       Other than that, I think AMD did great in both GPU and CPU
       department and look forward to more products from them.
     *
   kinggremlin [X]
       [ ]
       Good effort by AMD, but still doesn't move the performance bar any
       for home users. For anyone who has been waiting 4 years to get a
       5960x for less than $500, you've now got your CPU. For anyone
       looking for something tangibly faster than Intel's Haswell
       generation CPU's, you're still waiting.
     *
   lxtbell2 [X]
       [ ]

     Quote:

     Good effort by AMD, but still doesn't move the performance bar any
     for home users. For anyone who has been waiting 4 years to get a
     5960x for less than $500, you've now got your CPU. For anyone
     looking for something tangibly faster than Intel's Haswell
     generation CPU's, you're still waiting.
       Zen 2 seems to have some 30% better power efficiency than Intel
       processors, and that's huge. Power efficiency is everything for
       SFF, laptops, HTPC, fanless builds etc etc. So I would say it can
       potentially move the performance bar a lot for those.
       Regarding absolute performance, the 12-core destroys all Haswell
       desktop CPUs in productivity, and I can't care less about "tangibly
       faster" in gaming as long as my graphics card is the bottleneck in
       1440p+.
     *
   InvalidError [X]
       [ ]

     Quote:

     Zen 2 seems to have some 30% better power efficiency than Intel
     processors, and that's huge. Power efficiency is everything for SFF,
     laptops, HTPC, fanless builds etc etc. So I would say it can
     potentially move the performance bar a lot for those.
       For the lower-power segments, Intel has Icelake which is more than
       a match for Zen 2 CPU-wise.
     *
   salgado18 [X]
       [ ]
       About bad overclock numbers, my 2 cents:
       1. Nobody had the time to properly test them. I believe Tom's had
       like two or so days to run dozens of tests on two brand new
       processors, and still produce the article. Also, these are early
       BIOS and drivers. It could improve with time.
       2. The 3700x reaches a max of 4.3, but the 3950x reaches 4.6. Maybe
       it's inconsistent because of such a new process, after all, these
       are the first 7nm cpus produced, right? And lots of changes, some
       sillicon revisions could help a lot.
     *
   kinggremlin [X]
       [ ]

     Quote:

     Regarding absolute performance, the 12-core destroys all Haswell
     desktop CPUs in productivity, and I can't care less about "tangibly
     faster" in gaming as long as my graphics card is the bottleneck in
     1440p+.
       A 12 core CPU will not be any faster than an 8 core for home users.
       The software used does not scale with that many cores. If it was so
       easy to make typical software scale with more cores, programmers
       would have already done it. Most mainstream software is not capable
       of being highly parallelized like a graphics workload, so this
       problem isn't going to get fixed without a completely different
       type of computing platform. Adding more cores doesn't make a PC
       faster when what you really need is faster cores.
     *
   kinggremlin [X]
       [ ]

     Quote:

     About bad overclock numbers, my 2 cents: 1. Nobody had the time to
     properly test them. I believe Tom's had like two or so days to run
     dozens of tests on two brand new processors, and still produce the
     article. Also, these are early BIOS and drivers. It could improve
     with time. 2. The 3700x reaches a max of 4.3, but the 3950x reaches
     4.6. Maybe it's inconsistent because of such a new process, after
     all, these are the first 7nm cpus produced, right? And lots of
     changes, some sillicon revisions could help a lot.
       May want to watch this link from world reknown overclocker de8auer:
       [MEDIA=youtube]WXbCdGENp5I[/MEDIA]
       View: https://www.youtube.com/watch?v=WXbCdGENp5I
       He has 10 CPU's (mix of 6/8/12 cores). Forget overclocking, he
       couldn't get any of his 8 or 12 core CPU's to hit AMD's advertised
       max clocks (4.5 for 3800x, 4.6 for 3900x) even with a custom water
       cooling loop. There were leaks before launch that these chips would
       hit 5GHz. De8auer says in this video to forget that, he has chips
       that wouldn't hit 5GHz using liquid nitrogen.
     *
   ingtar33 [X]
       [ ]

     Quote:

     I've seen reviews from 5 different sites and the conclusions bounce
     all over the place, which makes me think there is much to do on the
     software optimization side. :unsure: I was expecting gaming FPS to
     not change all that much with many of the titles being tested have
     partnered or optimized around Intel.
       Linus had a possible cause for this. He noted the same thing (with
       results all over the place) and realized it's a result of the
       windows scheduler bouncing heavy tasks from core to core without
       consideration for boost clocks. when they locked the application to
       the cores that were boosting the results for the AMD cpu shoot went
       up and became much more consistent. He believes once the scheduling
       issue in windows is worked out the Ryzen bench results will
       probably increase noticeably.
     *
   Ncogneto [X]
       [ ]

     Quote:

     Good effort by AMD, but still doesn't move the performance bar any
     for home users. For anyone who has been waiting 4 years to get a
     5960x for less than $500, you've now got your CPU. For anyone
     looking for something tangibly faster than Intel's Haswell
     generation CPU's, you're still waiting.
       One of these days you might actually understand what matters, and
       what doesn't. AMD delivered on performance that matters, not some
       silly gaming benchmark(s) that is Intel's last gasp as it hangs on
       by it's fingernails. Performance that you can only see if you
       couple either platform with a $1200 GPU no less, and even then, you
       can't actually notice a difference while playing your game (155fps
       vs 135 FPS, who gives a rats a**). Show me a game in which the
       Intel CPU is performing at a level that is visually noticeably
       better (without looking at some silly fps counter).
       On every other front, the Intel CPU gets stomped in the dirt, all
       while costing more and consuming more power, and needing an exotic
       cooling solution as well.
       Easy win for AMD, for anyone other than those only interested in
       nothing but pure fps bragging rights, which is so High School level
       silly.
       Spectre/meltdown/zombieload......disable HT, etc etc etc. Every
       month brings another Intel bug.
       They should be ashamed.
     *
   fr3sgnint [X]
       [ ]
       Shame we didn't see any CAD / Solidworks ETC type benchmarks on the
       productivity side.
     *
   acquinn [X]
       [ ]
       So impressive. AMD hasn't been on my radar since the Athlon days.
       Competition is good! I wonder how long it'll take Intel to get to
       7NM. Also, where's Intel's response hardware-wise? I mean isn't the
       9900K almost a year old at this point? And it's still beating AMD
       in a lot of areas.
     *
   kinggremlin [X]
       [ ]

     Quote:

     Shame we didn't see any CAD / Solidworks ETC type benchmarks on the
     productivity side.
       You don't have to. Both are single threaded applications with
       random addons that may support multithreading, so you know Intel is
       going to win, just like most stuff from Adobe. People like Ncogneto
       don't seem to grasp how much commonly used software doesn't benefit
       at all from increased core counts beyond a few.

   Display more comments

   Most Popular
    1. India's First CPUs Are Ready for App Development
    2. PCPartPicker Reveals AMD Ryzen 3000 CPU Packaging
    3. Silicon Lottery to Bin and Sell Ryzen 3000 CPUs

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   alternate alternate prev next

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

     * Graphics

     Review

  AMD Radeon RX 5700 XT and Radeon RX 5700 Review: New Prices Keep Navi In The
                                      Game

   by Chris Angelini July 7, 2019 at 7:00 AM

     *
     *
     *
     *
     *
     *

   22 Comments

   [ ] ( ) (*) ( ) ( ) ( ) ( ) ( )
   Page 1:AMD Radeon RX 5700 XT and Radeon RX 5700 Review
   Page 2:Performance Results: 2560 x 1440
   Page 3:Performance Results: 3840 x 2160
   Page 4:Power Consumption: Radeon RX 5700
   Page 5:Power Consumption: Radeon RX 5700 XT
   Page 6:Fan Speeds, Clock Rates, and Temperatures
   Page 7:Conclusion
     *

Page 1:AMD Radeon RX 5700 XT and Radeon RX 5700 Review
     *

Page 2:Performance Results: 2560 x 1440
     *

Page 3:Performance Results: 3840 x 2160
     *

Page 4:Power Consumption: Radeon RX 5700
     *

Page 5:Power Consumption: Radeon RX 5700 XT
     *

Page 6:Fan Speeds, Clock Rates, and Temperatures
     *

Page 7:Conclusion

Performance Results: 2560 x 1440

   Given what we’ve already seen from GeForce RTX 2060 Super and 2070
   Super, both of AMD’s Radeon RX 5700-series cards are best suited to
   gaming at 2560 x 1440. They’re also beyond ample for 1920 x 1080,
   though we don’t think you need to spend $350 or $400 dollars for smooth
   frame rates at that resolution.

   Across our benchmark suite, Radeon RX 5700 XT averages 9.9%-higher
   frame rates than the GeForce RTX 2060 Super at 2560 x 1440. Radeon RX
   5700 averages 11%-higher frame rates than the GeForce RTX 2060 at the
   same resolution. The GeForce RTX 2070 Super does serve up average frame
   rates 6.9% higher than Radeon RX 5700 XT, but it costs 25% more.

   Compared to AMD’s previous generation, Radeon RX 5700 XT achieves
   15%-higher average frame rates than Radeon RX Vega 64 at a $100-lower
   launch price.

   Before any of the GeForce RTX Super cards were announced, AMD said it
   was targeting GeForce RTX 2070 with its Radeon RX 5700 XT. The top-end
   Navi card does deliver 6.4%-higher average frame rates than its
   intended GeForce mark, which means it often lands close to the
   venerable GeForce GTX 1080 Ti.

Battlefield V (DX12)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Battlefield V - FPS - 2560x1440, DX12 Ultra
       Battlefield V - FPS - 2560x1440, DX12 Ultra
     * Battlefield V - AvFPSoB - 2560x1440, DX12 Ultra
       Battlefield V - AvFPSoB - 2560x1440, DX12 Ultra
     * Battlefield V - FrameTime - 2560x1440, DX12 Ultra
       Battlefield V - FrameTime - 2560x1440, DX12 Ultra
     * Battlefield V - FPS99th - 2560x1440, DX12 Ultra
       Battlefield V - FPS99th - 2560x1440, DX12 Ultra
     * Battlefield V - FrameTimeVariance - 2560x1440, DX12 Ultra
       Battlefield V - FrameTimeVariance - 2560x1440, DX12 Ultra
     * Battlefield V - Unevenness - 2560x1440, DX12 Ultra
       Battlefield V - Unevenness - 2560x1440, DX12 Ultra
     * Battlefield V - FrameTimeoB - 2560x1440, DX12 Ultra
       Battlefield V - FrameTimeoB - 2560x1440, DX12 Ultra

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Battlefield V - FPS - 2560x1440, DX12 Ultra

Battlefield V - FPS - 2560x1440, DX12 Ultra
     * Battlefield V - AvFPSoB - 2560x1440, DX12 Ultra

Battlefield V - AvFPSoB - 2560x1440, DX12 Ultra
     * Battlefield V - FrameTime - 2560x1440, DX12 Ultra

Battlefield V - FrameTime - 2560x1440, DX12 Ultra
     * Battlefield V - FPS99th - 2560x1440, DX12 Ultra

Battlefield V - FPS99th - 2560x1440, DX12 Ultra
     * Battlefield V - FrameTimeVariance - 2560x1440, DX12 Ultra

Battlefield V - FrameTimeVariance - 2560x1440, DX12 Ultra
     * Battlefield V - Unevenness - 2560x1440, DX12 Ultra

Battlefield V - Unevenness - 2560x1440, DX12 Ultra
     * Battlefield V - FrameTimeoB - 2560x1440, DX12 Ultra

Battlefield V - FrameTimeoB - 2560x1440, DX12 Ultra

Destiny 2 (DX11)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Destiny 2 - FPS - 2560x1440, DX11, SMAA Highest
       Destiny 2 - FPS - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - AvFPSoB - 2560x1440, DX11, SMAA Highest
       Destiny 2 - AvFPSoB - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - FrameTimeVariance - 2560x1440, DX11, SMAA Highest
       Destiny 2 - FrameTimeVariance - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - Unevenness - 2560x1440, DX11, SMAA Highest
       Destiny 2 - Unevenness - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - FrameTime - 2560x1440, DX11, SMAA Highest
       Destiny 2 - FrameTime - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - FPS99th - 2560x1440, DX11, SMAA Highest
       Destiny 2 - FPS99th - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - FrameTimeoB - 2560x1440, DX11, SMAA Highest
       Destiny 2 - FrameTimeoB - 2560x1440, DX11, SMAA Highest

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Destiny 2 - FPS - 2560x1440, DX11, SMAA Highest

Destiny 2 - FPS - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - AvFPSoB - 2560x1440, DX11, SMAA Highest

Destiny 2 - AvFPSoB - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - FrameTimeVariance - 2560x1440, DX11, SMAA Highest

Destiny 2 - FrameTimeVariance - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - Unevenness - 2560x1440, DX11, SMAA Highest

Destiny 2 - Unevenness - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - FrameTime - 2560x1440, DX11, SMAA Highest

Destiny 2 - FrameTime - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - FPS99th - 2560x1440, DX11, SMAA Highest

Destiny 2 - FPS99th - 2560x1440, DX11, SMAA Highest
     * Destiny 2 - FrameTimeoB - 2560x1440, DX11, SMAA Highest

Destiny 2 - FrameTimeoB - 2560x1440, DX11, SMAA Highest

Far Cry 5 (DX11)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Far Cry 5 - FPS - 2560x1440, DX11, HD Textures On Ultra
       Far Cry 5 - FPS - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - AvFPSoB - 2560x1440, DX11, HD Textures On Ultra
       Far Cry 5 - AvFPSoB - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - Unevenness - 2560x1440, DX11, HD Textures On Ultra
       Far Cry 5 - Unevenness - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - FrameTimeVariance - 2560x1440, DX11, HD Textures On
       Ultra
       Far Cry 5 - FrameTimeVariance - 2560x1440, DX11, HD Textures On
       Ultra
     * Far Cry 5 - FPS99th - 2560x1440, DX11, HD Textures On Ultra
       Far Cry 5 - FPS99th - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - FrameTime - 2560x1440, DX11, HD Textures On Ultra
       Far Cry 5 - FrameTime - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - FrameTimeoB - 2560x1440, DX11, HD Textures On Ultra
       Far Cry 5 - FrameTimeoB - 2560x1440, DX11, HD Textures On Ultra

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Far Cry 5 - FPS - 2560x1440, DX11, HD Textures On Ultra

Far Cry 5 - FPS - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - AvFPSoB - 2560x1440, DX11, HD Textures On Ultra

Far Cry 5 - AvFPSoB - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - Unevenness - 2560x1440, DX11, HD Textures On Ultra

Far Cry 5 - Unevenness - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - FrameTimeVariance - 2560x1440, DX11, HD Textures On
       Ultra

Far Cry 5 - FrameTimeVariance - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - FPS99th - 2560x1440, DX11, HD Textures On Ultra

Far Cry 5 - FPS99th - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - FrameTime - 2560x1440, DX11, HD Textures On Ultra

Far Cry 5 - FrameTime - 2560x1440, DX11, HD Textures On Ultra
     * Far Cry 5 - FrameTimeoB - 2560x1440, DX11, HD Textures On Ultra

Far Cry 5 - FrameTimeoB - 2560x1440, DX11, HD Textures On Ultra

Final Fantasy XV (DX11)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Final Fantasy XV - FPS - 2560x1440, DX11, TAA High
       Final Fantasy XV - FPS - 2560x1440, DX11, TAA High
     * Final Fantasy XV - AvFPSoB - 2560x1440, DX11, TAA High
       Final Fantasy XV - AvFPSoB - 2560x1440, DX11, TAA High
     * Final Fantasy XV - FrameTimeVariance - 2560x1440, DX11, TAA High
       Final Fantasy XV - FrameTimeVariance - 2560x1440, DX11, TAA High
     * Final Fantasy XV - Unevenness - 2560x1440, DX11, TAA High
       Final Fantasy XV - Unevenness - 2560x1440, DX11, TAA High
     * Final Fantasy XV - FrameTime - 2560x1440, DX11, TAA High
       Final Fantasy XV - FrameTime - 2560x1440, DX11, TAA High
     * Final Fantasy XV - FrameTimeoB - 2560x1440, DX11, TAA High
       Final Fantasy XV - FrameTimeoB - 2560x1440, DX11, TAA High
     * Final Fantasy XV - FPS99th - 2560x1440, DX11, TAA High
       Final Fantasy XV - FPS99th - 2560x1440, DX11, TAA High

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Final Fantasy XV - FPS - 2560x1440, DX11, TAA High

Final Fantasy XV - FPS - 2560x1440, DX11, TAA High
     * Final Fantasy XV - AvFPSoB - 2560x1440, DX11, TAA High

Final Fantasy XV - AvFPSoB - 2560x1440, DX11, TAA High
     * Final Fantasy XV - FrameTimeVariance - 2560x1440, DX11, TAA High

Final Fantasy XV - FrameTimeVariance - 2560x1440, DX11, TAA High
     * Final Fantasy XV - Unevenness - 2560x1440, DX11, TAA High

Final Fantasy XV - Unevenness - 2560x1440, DX11, TAA High
     * Final Fantasy XV - FrameTime - 2560x1440, DX11, TAA High

Final Fantasy XV - FrameTime - 2560x1440, DX11, TAA High
     * Final Fantasy XV - FrameTimeoB - 2560x1440, DX11, TAA High

Final Fantasy XV - FrameTimeoB - 2560x1440, DX11, TAA High
     * Final Fantasy XV - FPS99th - 2560x1440, DX11, TAA High

Final Fantasy XV - FPS99th - 2560x1440, DX11, TAA High

Forza Horizon 4 (DX12)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Forza Horizon 4 - FPS - 2560x1440, DX12, 2x MSAA Ultra
       Forza Horizon 4 - FPS - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - Unevenness - 2560x1440, DX12, 2x MSAA Ultra
       Forza Horizon 4 - Unevenness - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - AvFPSoB - 2560x1440, DX12, 2x MSAA Ultra
       Forza Horizon 4 - AvFPSoB - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - FrameTimeVariance - 2560x1440, DX12, 2x MSAA
       Ultra
       Forza Horizon 4 - FrameTimeVariance - 2560x1440, DX12, 2x MSAA
       Ultra
     * Forza Horizon 4 - FPS99th - 2560x1440, DX12, 2x MSAA Ultra
       Forza Horizon 4 - FPS99th - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - FrameTimeoB - 2560x1440, DX12, 2x MSAA Ultra
       Forza Horizon 4 - FrameTimeoB - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - FrameTime - 2560x1440, DX12, 2x MSAA Ultra
       Forza Horizon 4 - FrameTime - 2560x1440, DX12, 2x MSAA Ultra

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Forza Horizon 4 - FPS - 2560x1440, DX12, 2x MSAA Ultra

Forza Horizon 4 - FPS - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - Unevenness - 2560x1440, DX12, 2x MSAA Ultra

Forza Horizon 4 - Unevenness - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - AvFPSoB - 2560x1440, DX12, 2x MSAA Ultra

Forza Horizon 4 - AvFPSoB - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - FrameTimeVariance - 2560x1440, DX12, 2x MSAA
       Ultra

Forza Horizon 4 - FrameTimeVariance - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - FPS99th - 2560x1440, DX12, 2x MSAA Ultra

Forza Horizon 4 - FPS99th - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - FrameTimeoB - 2560x1440, DX12, 2x MSAA Ultra

Forza Horizon 4 - FrameTimeoB - 2560x1440, DX12, 2x MSAA Ultra
     * Forza Horizon 4 - FrameTime - 2560x1440, DX12, 2x MSAA Ultra

Forza Horizon 4 - FrameTime - 2560x1440, DX12, 2x MSAA Ultra

Grand Theft Auto V (DX11)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Grand Theft Auto V - FPS - 2560x1440, DX11, 4x MSAA Very High
       Grand Theft Auto V - FPS - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - AvFPSoB - 2560x1440, DX11, 4x MSAA Very High
       Grand Theft Auto V - AvFPSoB - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - Unevenness - 2560x1440, DX11, 4x MSAA Very
       High
       Grand Theft Auto V - Unevenness - 2560x1440, DX11, 4x MSAA Very
       High
     * Grand Theft Auto V - FrameTime - 2560x1440, DX11, 4x MSAA Very High
       Grand Theft Auto V - FrameTime - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - FrameTimeVariance - 2560x1440, DX11, 4x MSAA
       Very High
       Grand Theft Auto V - FrameTimeVariance - 2560x1440, DX11, 4x MSAA
       Very High
     * Grand Theft Auto V - FPS99th - 2560x1440, DX11, 4x MSAA Very High
       Grand Theft Auto V - FPS99th - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - FrameTimeoB - 2560x1440, DX11, 4x MSAA Very
       High
       Grand Theft Auto V - FrameTimeoB - 2560x1440, DX11, 4x MSAA Very
       High

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Grand Theft Auto V - FPS - 2560x1440, DX11, 4x MSAA Very High

Grand Theft Auto V - FPS - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - AvFPSoB - 2560x1440, DX11, 4x MSAA Very High

Grand Theft Auto V - AvFPSoB - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - Unevenness - 2560x1440, DX11, 4x MSAA Very
       High

Grand Theft Auto V - Unevenness - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - FrameTime - 2560x1440, DX11, 4x MSAA Very High

Grand Theft Auto V - FrameTime - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - FrameTimeVariance - 2560x1440, DX11, 4x MSAA
       Very High

Grand Theft Auto V - FrameTimeVariance - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - FPS99th - 2560x1440, DX11, 4x MSAA Very High

Grand Theft Auto V - FPS99th - 2560x1440, DX11, 4x MSAA Very High
     * Grand Theft Auto V - FrameTimeoB - 2560x1440, DX11, 4x MSAA Very
       High

Grand Theft Auto V - FrameTimeoB - 2560x1440, DX11, 4x MSAA Very High

Metro: Exodus (DX12)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Metro Exodus - FPS - 2560x1440, DX12 Ultra
       Metro Exodus - FPS - 2560x1440, DX12 Ultra
     * Metro Exodus - AvFPSoB - 2560x1440, DX12 Ultra
       Metro Exodus - AvFPSoB - 2560x1440, DX12 Ultra
     * Metro Exodus - Unevenness - 2560x1440, DX12 Ultra
       Metro Exodus - Unevenness - 2560x1440, DX12 Ultra
     * Metro Exodus - FrameTimeoB - 2560x1440, DX12 Ultra
       Metro Exodus - FrameTimeoB - 2560x1440, DX12 Ultra
     * Metro Exodus - FrameTimeVariance - 2560x1440, DX12 Ultra
       Metro Exodus - FrameTimeVariance - 2560x1440, DX12 Ultra
     * Metro Exodus - FPS99th - 2560x1440, DX12 Ultra
       Metro Exodus - FPS99th - 2560x1440, DX12 Ultra
     * Metro Exodus - FrameTime - 2560x1440, DX12 Ultra
       Metro Exodus - FrameTime - 2560x1440, DX12 Ultra

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Metro Exodus - FPS - 2560x1440, DX12 Ultra

Metro Exodus - FPS - 2560x1440, DX12 Ultra
     * Metro Exodus - AvFPSoB - 2560x1440, DX12 Ultra

Metro Exodus - AvFPSoB - 2560x1440, DX12 Ultra
     * Metro Exodus - Unevenness - 2560x1440, DX12 Ultra

Metro Exodus - Unevenness - 2560x1440, DX12 Ultra
     * Metro Exodus - FrameTimeoB - 2560x1440, DX12 Ultra

Metro Exodus - FrameTimeoB - 2560x1440, DX12 Ultra
     * Metro Exodus - FrameTimeVariance - 2560x1440, DX12 Ultra

Metro Exodus - FrameTimeVariance - 2560x1440, DX12 Ultra
     * Metro Exodus - FPS99th - 2560x1440, DX12 Ultra

Metro Exodus - FPS99th - 2560x1440, DX12 Ultra
     * Metro Exodus - FrameTime - 2560x1440, DX12 Ultra

Metro Exodus - FrameTime - 2560x1440, DX12 Ultra

Shadow of the Tomb Raider (DX12)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Shadow of the Tomb Raider - FPS - 2560x1440, DX12, SMAAT2x Highest
       Shadow of the Tomb Raider - FPS - 2560x1440, DX12, SMAAT2x Highest
     * Shadow of the Tomb Raider - FrameTime - 2560x1440, DX12, SMAAT2x
       Highest
       Shadow of the Tomb Raider - FrameTime - 2560x1440, DX12, SMAAT2x
       Highest
     * Shadow of the Tomb Raider - Unevenness - 2560x1440, DX12, SMAAT2x
       Highest
       Shadow of the Tomb Raider - Unevenness - 2560x1440, DX12, SMAAT2x
       Highest
     * Shadow of the Tomb Raider - AvFPSoB - 2560x1440, DX12, SMAAT2x
       Highest
       Shadow of the Tomb Raider - AvFPSoB - 2560x1440, DX12, SMAAT2x
       Highest
     * Shadow of the Tomb Raider - FPS99th - 2560x1440, DX12, SMAAT2x
       Highest
       Shadow of the Tomb Raider - FPS99th - 2560x1440, DX12, SMAAT2x
       Highest
     * Shadow of the Tomb Raider - FrameTimeoB - 2560x1440, DX12, SMAAT2x
       Highest
       Shadow of the Tomb Raider - FrameTimeoB - 2560x1440, DX12, SMAAT2x
       Highest
     * Shadow of the Tomb Raider - FrameTimeVariance - 2560x1440, DX12,
       SMAAT2x Highest
       Shadow of the Tomb Raider - FrameTimeVariance - 2560x1440, DX12,
       SMAAT2x Highest

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Shadow of the Tomb Raider - FPS - 2560x1440, DX12, SMAAT2x Highest

Shadow of the Tomb Raider - FPS - 2560x1440, DX12, SMAAT2x Highest
     * Shadow of the Tomb Raider - FrameTime - 2560x1440, DX12, SMAAT2x
       Highest

Shadow of the Tomb Raider - FrameTime - 2560x1440, DX12, SMAAT2x Highest
     * Shadow of the Tomb Raider - Unevenness - 2560x1440, DX12, SMAAT2x
       Highest

Shadow of the Tomb Raider - Unevenness - 2560x1440, DX12, SMAAT2x Highest
     * Shadow of the Tomb Raider - AvFPSoB - 2560x1440, DX12, SMAAT2x
       Highest

Shadow of the Tomb Raider - AvFPSoB - 2560x1440, DX12, SMAAT2x Highest
     * Shadow of the Tomb Raider - FPS99th - 2560x1440, DX12, SMAAT2x
       Highest

Shadow of the Tomb Raider - FPS99th - 2560x1440, DX12, SMAAT2x Highest
     * Shadow of the Tomb Raider - FrameTimeoB - 2560x1440, DX12, SMAAT2x
       Highest

Shadow of the Tomb Raider - FrameTimeoB - 2560x1440, DX12, SMAAT2x Highest
     * Shadow of the Tomb Raider - FrameTimeVariance - 2560x1440, DX12,
       SMAAT2x Highest

Shadow of the Tomb Raider - FrameTimeVariance - 2560x1440, DX12, SMAAT2x
Highest

Strange Brigade (Vulkan)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Strange Brigade - FPS - 2560x1440, Vulkan, Async Compute Ultra
       Strange Brigade - FPS - 2560x1440, Vulkan, Async Compute Ultra
     * Strange Brigade - FPS99th - 2560x1440, Vulkan, Async Compute Ultra
       Strange Brigade - FPS99th - 2560x1440, Vulkan, Async Compute Ultra
     * Strange Brigade - AvFPSoB - 2560x1440, Vulkan, Async Compute Ultra
       Strange Brigade - AvFPSoB - 2560x1440, Vulkan, Async Compute Ultra
     * Strange Brigade - Unevenness - 2560x1440, Vulkan, Async Compute
       Ultra
       Strange Brigade - Unevenness - 2560x1440, Vulkan, Async Compute
       Ultra
     * Strange Brigade - FrameTimeVariance - 2560x1440, Vulkan, Async
       Compute Ultra
       Strange Brigade - FrameTimeVariance - 2560x1440, Vulkan, Async
       Compute Ultra
     * Strange Brigade - FrameTime - 2560x1440, Vulkan, Async Compute
       Ultra
       Strange Brigade - FrameTime - 2560x1440, Vulkan, Async Compute
       Ultra
     * Strange Brigade - FrameTimeoB - 2560x1440, Vulkan, Async Compute
       Ultra
       Strange Brigade - FrameTimeoB - 2560x1440, Vulkan, Async Compute
       Ultra

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Strange Brigade - FPS - 2560x1440, Vulkan, Async Compute Ultra

Strange Brigade - FPS - 2560x1440, Vulkan, Async Compute Ultra
     * Strange Brigade - FPS99th - 2560x1440, Vulkan, Async Compute Ultra

Strange Brigade - FPS99th - 2560x1440, Vulkan, Async Compute Ultra
     * Strange Brigade - AvFPSoB - 2560x1440, Vulkan, Async Compute Ultra

Strange Brigade - AvFPSoB - 2560x1440, Vulkan, Async Compute Ultra
     * Strange Brigade - Unevenness - 2560x1440, Vulkan, Async Compute
       Ultra

Strange Brigade - Unevenness - 2560x1440, Vulkan, Async Compute Ultra
     * Strange Brigade - FrameTimeVariance - 2560x1440, Vulkan, Async
       Compute Ultra

Strange Brigade - FrameTimeVariance - 2560x1440, Vulkan, Async Compute Ultra
     * Strange Brigade - FrameTime - 2560x1440, Vulkan, Async Compute
       Ultra

Strange Brigade - FrameTime - 2560x1440, Vulkan, Async Compute Ultra
     * Strange Brigade - FrameTimeoB - 2560x1440, Vulkan, Async Compute
       Ultra

Strange Brigade - FrameTimeoB - 2560x1440, Vulkan, Async Compute Ultra

Tom Clancy’s The Division 2 (DX12)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Tom Clancy\'s The Division 2 - FPS - 2560x1440, DX12 Ultra
       Tom Clancy\'s The Division 2 - FPS - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - AvFPSoB - 2560x1440, DX12 Ultra
       Tom Clancy\'s The Division 2 - AvFPSoB - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - FrameTime - 2560x1440, DX12 Ultra
       Tom Clancy\'s The Division 2 - FrameTime - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - FrameTimeVariance - 2560x1440, DX12
       Ultra
       Tom Clancy\'s The Division 2 - FrameTimeVariance - 2560x1440, DX12
       Ultra
     * Tom Clancy\'s The Division 2 - Unevenness - 2560x1440, DX12 Ultra
       Tom Clancy\'s The Division 2 - Unevenness - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - FPS99th - 2560x1440, DX12 Ultra
       Tom Clancy\'s The Division 2 - FPS99th - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - FrameTimeoB - 2560x1440, DX12 Ultra
       Tom Clancy\'s The Division 2 - FrameTimeoB - 2560x1440, DX12 Ultra

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Tom Clancy\'s The Division 2 - FPS - 2560x1440, DX12 Ultra

Tom Clancy\'s The Division 2 - FPS - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - AvFPSoB - 2560x1440, DX12 Ultra

Tom Clancy\'s The Division 2 - AvFPSoB - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - FrameTime - 2560x1440, DX12 Ultra

Tom Clancy\'s The Division 2 - FrameTime - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - FrameTimeVariance - 2560x1440, DX12
       Ultra

Tom Clancy\'s The Division 2 - FrameTimeVariance - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - Unevenness - 2560x1440, DX12 Ultra

Tom Clancy\'s The Division 2 - Unevenness - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - FPS99th - 2560x1440, DX12 Ultra

Tom Clancy\'s The Division 2 - FPS99th - 2560x1440, DX12 Ultra
     * Tom Clancy\'s The Division 2 - FrameTimeoB - 2560x1440, DX12 Ultra

Tom Clancy\'s The Division 2 - FrameTimeoB - 2560x1440, DX12 Ultra

Tom Clancy’s Ghost Recon (DX11)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Tom Clancy\'s Ghost Recon - FPS - 2560x1440, DX11 Very High
       Tom Clancy\'s Ghost Recon - FPS - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - AvFPSoB - 2560x1440, DX11 Very High
       Tom Clancy\'s Ghost Recon - AvFPSoB - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - FrameTime - 2560x1440, DX11 Very High
       Tom Clancy\'s Ghost Recon - FrameTime - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - Unevenness - 2560x1440, DX11 Very High
       Tom Clancy\'s Ghost Recon - Unevenness - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - FrameTimeVariance - 2560x1440, DX11
       Very High
       Tom Clancy\'s Ghost Recon - FrameTimeVariance - 2560x1440, DX11
       Very High
     * Tom Clancy\'s Ghost Recon - FPS99th - 2560x1440, DX11 Very High
       Tom Clancy\'s Ghost Recon - FPS99th - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - FrameTimeoB - 2560x1440, DX11 Very High
       Tom Clancy\'s Ghost Recon - FrameTimeoB - 2560x1440, DX11 Very High

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Tom Clancy\'s Ghost Recon - FPS - 2560x1440, DX11 Very High

Tom Clancy\'s Ghost Recon - FPS - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - AvFPSoB - 2560x1440, DX11 Very High

Tom Clancy\'s Ghost Recon - AvFPSoB - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - FrameTime - 2560x1440, DX11 Very High

Tom Clancy\'s Ghost Recon - FrameTime - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - Unevenness - 2560x1440, DX11 Very High

Tom Clancy\'s Ghost Recon - Unevenness - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - FrameTimeVariance - 2560x1440, DX11
       Very High

Tom Clancy\'s Ghost Recon - FrameTimeVariance - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - FPS99th - 2560x1440, DX11 Very High

Tom Clancy\'s Ghost Recon - FPS99th - 2560x1440, DX11 Very High
     * Tom Clancy\'s Ghost Recon - FrameTimeoB - 2560x1440, DX11 Very High

Tom Clancy\'s Ghost Recon - FrameTimeoB - 2560x1440, DX11 Very High

The Witcher 3 (DX11)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * The Witcher 3 - FPS - 2560x1440, DX11 Ultra
       The Witcher 3 - FPS - 2560x1440, DX11 Ultra
     * The Witcher 3 - FPS99th - 2560x1440, DX11 Ultra
       The Witcher 3 - FPS99th - 2560x1440, DX11 Ultra
     * The Witcher 3 - FrameTimeVariance - 2560x1440, DX11 Ultra
       The Witcher 3 - FrameTimeVariance - 2560x1440, DX11 Ultra
     * The Witcher 3 - Unevenness - 2560x1440, DX11 Ultra
       The Witcher 3 - Unevenness - 2560x1440, DX11 Ultra
     * The Witcher 3 - FrameTimeoB - 2560x1440, DX11 Ultra
       The Witcher 3 - FrameTimeoB - 2560x1440, DX11 Ultra
     * The Witcher 3 - AvFPSoB - 2560x1440, DX11 Ultra
       The Witcher 3 - AvFPSoB - 2560x1440, DX11 Ultra
     * The Witcher 3 - FrameTime - 2560x1440, DX11 Ultra
       The Witcher 3 - FrameTime - 2560x1440, DX11 Ultra

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * The Witcher 3 - FPS - 2560x1440, DX11 Ultra

The Witcher 3 - FPS - 2560x1440, DX11 Ultra
     * The Witcher 3 - FPS99th - 2560x1440, DX11 Ultra

The Witcher 3 - FPS99th - 2560x1440, DX11 Ultra
     * The Witcher 3 - FrameTimeVariance - 2560x1440, DX11 Ultra

The Witcher 3 - FrameTimeVariance - 2560x1440, DX11 Ultra
     * The Witcher 3 - Unevenness - 2560x1440, DX11 Ultra

The Witcher 3 - Unevenness - 2560x1440, DX11 Ultra
     * The Witcher 3 - FrameTimeoB - 2560x1440, DX11 Ultra

The Witcher 3 - FrameTimeoB - 2560x1440, DX11 Ultra
     * The Witcher 3 - AvFPSoB - 2560x1440, DX11 Ultra

The Witcher 3 - AvFPSoB - 2560x1440, DX11 Ultra
     * The Witcher 3 - FrameTime - 2560x1440, DX11 Ultra

The Witcher 3 - FrameTime - 2560x1440, DX11 Ultra

Wolfenstein II: The New Colossus (Vulkan)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Wolfenstein II The New Colossus - FPS - 2560x1440, Vulkan Über,
       TSSAA (8TX)
       Wolfenstein II The New Colossus - FPS - 2560x1440, Vulkan Über,
       TSSAA (8TX)
     * Wolfenstein II The New Colossus - AvFPSoB - 2560x1440, Vulkan Über,
       TSSAA (8TX)
       Wolfenstein II The New Colossus - AvFPSoB - 2560x1440, Vulkan Über,
       TSSAA (8TX)
     * Wolfenstein II The New Colossus - FrameTime - 2560x1440, Vulkan
       Über, TSSAA (8TX)
       Wolfenstein II The New Colossus - FrameTime - 2560x1440, Vulkan
       Über, TSSAA (8TX)
     * Wolfenstein II The New Colossus - FrameTimeVariance - 2560x1440,
       Vulkan Über, TSSAA (8TX)
       Wolfenstein II The New Colossus - FrameTimeVariance - 2560x1440,
       Vulkan Über, TSSAA (8TX)
     * Wolfenstein II The New Colossus - Unevenness - 2560x1440, Vulkan
       Über, TSSAA (8TX)
       Wolfenstein II The New Colossus - Unevenness - 2560x1440, Vulkan
       Über, TSSAA (8TX)
     * Wolfenstein II The New Colossus - FPS99th - 2560x1440, Vulkan Über,
       TSSAA (8TX)
       Wolfenstein II The New Colossus - FPS99th - 2560x1440, Vulkan Über,
       TSSAA (8TX)
     * Wolfenstein II The New Colossus - FrameTimeoB - 2560x1440, Vulkan
       Über, TSSAA (8TX)
       Wolfenstein II The New Colossus - FrameTimeoB - 2560x1440, Vulkan
       Über, TSSAA (8TX)

   (*) ( ) ( ) ( ) ( ) ( ) ( )
     * Wolfenstein II The New Colossus - FPS - 2560x1440, Vulkan Über,
       TSSAA (8TX)

Wolfenstein II The New Colossus - FPS - 2560x1440, Vulkan Über, TSSAA (8TX)
     * Wolfenstein II The New Colossus - AvFPSoB - 2560x1440, Vulkan Über,
       TSSAA (8TX)

Wolfenstein II The New Colossus - AvFPSoB - 2560x1440, Vulkan Über, TSSAA
(8TX)
     * Wolfenstein II The New Colossus - FrameTime - 2560x1440, Vulkan
       Über, TSSAA (8TX)

Wolfenstein II The New Colossus - FrameTime - 2560x1440, Vulkan Über, TSSAA
(8TX)
     * Wolfenstein II The New Colossus - FrameTimeVariance - 2560x1440,
       Vulkan Über, TSSAA (8TX)

Wolfenstein II The New Colossus - FrameTimeVariance - 2560x1440, Vulkan Über,
TSSAA (8TX)
     * Wolfenstein II The New Colossus - Unevenness - 2560x1440, Vulkan
       Über, TSSAA (8TX)

Wolfenstein II The New Colossus - Unevenness - 2560x1440, Vulkan Über, TSSAA
(8TX)
     * Wolfenstein II The New Colossus - FPS99th - 2560x1440, Vulkan Über,
       TSSAA (8TX)

Wolfenstein II The New Colossus - FPS99th - 2560x1440, Vulkan Über, TSSAA
(8TX)
     * Wolfenstein II The New Colossus - FrameTimeoB - 2560x1440, Vulkan
       Über, TSSAA (8TX)

Wolfenstein II The New Colossus - FrameTimeoB - 2560x1440, Vulkan Über, TSSAA
(8TX)

   MORE: Best Graphics Cards

   MORE: Desktop GPU Performance Hierarchy Table

   MORE: All Graphics Content

   [javascript]
   AMD Radeon RX 5700
   $350Amazon

   Previous Next

   Summary
    1. AMD Radeon RX 5700 XT and Radeon RX 5700 Review
    2. Performance Results: 2560 x 1440
    3. Performance Results: 3840 x 2160
    4. Power Consumption: Radeon RX 5700
    5. Power Consumption: Radeon RX 5700 XT
    6. Fan Speeds, Clock Rates, and Temperatures
    7. Conclusion

   About the author
   Chris Angelini @chris_angelini

   Chris Angelini is an Editor Emeritus at Tom's Hardware US. He edits
   hardware reviews and covers high-profile CPU and GPU launches.
   [javascript]
   Read more
     * Graphics
     * AMD
     * Components

   22 comments
   Comment from the forums
       Your comment
     *
   ICWiener [X]
       [ ]
       Honestly I'd pay the extra just for Nvidia's decent cooler, let
       alone power consumption, RTX, etc. Hard pass on another blower.
     *
   justin.m.beauvais [X]
       [ ]
       Dear AMD,
       Why? Wasn't your line GTX 1080 performance at RX 580 prices? points
       at Navi This is not that.
       Regrettably yours,
       Your Fans
     *
   alextheblue [X]
       [ ]

     Quote:

     Honestly I'd pay the extra just for Nvidia's decent cooler, let
     alone power consumption, RTX, etc. Hard pass on another blower.
       I read the review, don't know what you mean by "let alone power
       consumption". Efficiency is virtually the same. I personally prefer
       blowers as long as they're not crazy loud, and the review says
       they're not so I'm all for it. If you DON'T like blowers, I'm sure
       there will be third-party coolers that improve cooling performance
       and acoustics further (and dump heat into the chassis like crazy).

     Quote:

     Dear AMD, Why? Wasn't your line GTX 1080 performance at RX 580
     prices? points at Navi This is not that. Regrettably yours, Your
     Fans
       Who ever promised that? They are 10-11% faster than same-priced
       Nvidia models. They're not going to drop 5700 $100 bucks when
       they're already ahead by 11%, even though I would love lower
       prices, there's no incentive for them to do so.
     *
   face-plants [X]
       [ ]
       I'm pleasantly surprised by both the performance AND the
       last-minute drop in price. Personally I don't like blower style
       cards and will be on the lookout for third-party offerings with
       more traditional dual fan coolers. This is totally personal
       preference as I tend to build in bigger cases with plenty of
       ventilation to deal with the extra heat. If the prices don't get
       too out of line from the AIBs then I'll gladly give a few of these
       cards a go in upcoming builds. I'm also expecting a decent
       improvement in thermals so hopefully these FE cards aren't
       exemplary of the best cooling you can get on air. The prospect of
       building all AMD machines in the coming months has got me totally
       nerding out.
     *
   kinggremlin [X]
       [ ]

     Quote:

     I read the review, don't know what you mean by "let alone power
     consumption". Efficiency is virtually the same. I personally prefer
     blowers as long as they're not crazy loud, and the review says
     they're not so I'm all for it. If you DON'T like blowers, I'm sure
     there will be third-party coolers that improve cooling performance
     and acoustics further (and dump heat into the chassis like crazy).
     Who ever promised that? They are 10-11% faster than same-priced
     Nvidia models. They're not going to drop 5700 $100 bucks when
     they're already ahead by 11%, even though I would love lower prices,
     there's no incentive for them to do so.
       You didn't read the review. Ignoring the Furmark results which
       don't mirror any realworld scenario. The 5700xt is slower than the
       2070 Super while using more power and running over 10degrees C
       hotter in gaming.
     *
   alextheblue [X]
       [ ]

     Quote:

     You didn't read the review. Ignoring the Furmark results which don't
     mirror any realworld scenario. The 5700xt is slower than the 2070
     Super while using more power and running over 10degrees C hotter in
     gaming.
       Yes, I did, stop being obstinate. Ah, I get it, you must have read
       "efficiency" in my post as "power consumption", or something. Why
       are you comparing it to the 2070 Super? The 5700 is the same price
       as the 2060 and 11-12% faster on average (between TH and AT), and
       the 5700XT is the same price as the 2060 Super and roughly 10-11%
       (TH-AT) faster. Yes, they use more power, but they're faster. As a
       result the efficiency is pretty close. It varies based on workload
       (game title), but I have read the reviews here and AT (so far,
       haven't looked at a third review yet). Here:
       https://www.anandtech.com/show/14618/the-amd-radeon-rx-5700-xt-rx-5
       700-review/15
       Factor in the performance gain (in AT's suite it was 11% for the XT
       and 12 for vanilla 5700) and you'll see their power consumption is
       pretty good. Average that with TH's results in Metro: LL and the
       final efficiency is pretty neck and neck with their direct
       competitors.
       I didn't say they didn't run hot. For people that don't like
       blowers (as I already said) there will be cooler, quieter third
       party options.
     *
   digitalgriffin [X]
       [ ]

     Quote:

     I read the review, don't know what you mean by "let alone power
     consumption". Efficiency is virtually the same. I personally prefer
     blowers as long as they're not crazy loud, and the review says
     they're not so I'm all for it. If you DON'T like blowers, I'm sure
     there will be third-party coolers that improve cooling performance
     and acoustics further (and dump heat into the chassis like crazy).
     Who ever promised that? They are 10-11% faster than same-priced
     Nvidia models. They're not going to drop 5700 $100 bucks when
     they're already ahead by 11%, even though I would love lower prices,
     there's no incentive for them to do so.
       In all honesty, all these cards are expensive. $350 would have been
       the most I wanted to pay for a 5700XT. And the card does run hot.
       Pascal was a small move up in prices. Turing was just insane
       pricing wise.
       That being said I bought one today and said "F"-it. I just don't
       like NVIDIA's business ethics. It will get the job done for two to
       three years.
     *
   Axiss [X]
       [ ]
       any idea when the board partner cards come out?
     *
   randomizer [X]
       [ ]
       How many engineers looked at those fan curves in lab testing and
       thought they were good?
       I think this release is a bit underwhelming. Local pricing here
       makes the RX 5700 fairly unattractive compared to a 2060, but the
       XT is better positioned against the 2070. Not sure it's really
       worthwhile upgrading a 970 though.
     *
   daglesj [X]
       [ ]
       So for the many of us on a RX480?...
       Worth it? Could someone not dig out the previous AMD midrange value
       demon to test against? C'mon...
     *
   feelinfroggy777 [X]
       [ ]
       The last $200 card AMD has released was 2.5 years ago. Let that
       sink in for a minute.
     *
   jeremyj_83 [X]
       [ ]

     Quote:

     How many engineers looked at those fan curves in lab testing and
     thought they were good? I think this release is a bit underwhelming.
     Local pricing here makes the RX 5700 fairly unattractive compared to
     a 2060, but the XT is better positioned against the 2070. Not sure
     it's really worthwhile upgrading a 970 though.
       On average the 5700XT is about twice as fast as the GTX 980 and the
       980 is 10-15% faster on average than the 970. That means that going
       to the XT will double your framerate assuming your CPU can keep up.
       I would say that is a worthwhile upgrade.
       https://www.anandtech.com/bench/product/2522?vs=2529
     *
   randomizer [X]
       [ ]

     Quote:

     On average the 5700XT is about twice as fast as the GTX 980 and the
     980 is 10-15% faster on average than the 970. That means that going
     to the XT will double your framerate assuming your CPU can keep up.
     I would say that is a worthwhile upgrade.
     https://www.anandtech.com/bench/product/2522?vs=2529
       I usually like to triple my framerates while sticking to a similar
       price bracket :)
       Also I don't think I can afford to run that space heater in summer.
     *
   redgarl [X]
       [ ]
       For anyone that want to know what matter...
       https://static.techspot.com/articles-info/1870/bench/Cost.png
       https://static.techspot.com/articles-info/1870/bench/Cost1.png
       With the actual scoring, this review is a joke. Navi is disrupting
       pricing and almost match a 1080 TI for 400$... however at
       tomshardware it sux.
     *
   alextheblue [X]
       [ ]

     Quote:

     In all honesty, all these cards are expensive. $350 would have been
     the most I wanted to pay for a 5700XT. And the card does run hot.
     Pascal was a small move up in prices. Turing was just insane pricing
     wise. That being said I bought one today and said "F"-it. I just
     don't like NVIDIA's business ethics. It will get the job done for
     two to three years.
       They ARE expensive - both Nvidia and AMD. This is the new
       "mid-range" unfortunately. I'd like to see Xe undercut them both
       and force this tier back down to the ~$250 range. I still might end
       up getting one, we'll see.

     Quote:

     How many engineers looked at those fan curves in lab testing and
     thought they were good? I think this release is a bit underwhelming.
     Local pricing here makes the RX 5700 fairly unattractive compared to
     a 2060, but the XT is better positioned against the 2070. Not sure
     it's really worthwhile upgrading a 970 though.
       If you're talking about the erratic behavior with the speed
       dropping over time, that looks to me like a bug or an issue with
       that sample. Their XT didn't act that way. Pricing can suck
       depending where you are... in the US it certainly offers more bang
       for the buck than the 2060. Maybe pricing will be more reasonable
       when partner boards become widespread. Like your icon, BTW, big fan
       of new Genesis/MD titles - awaiting Xeno Crisis currently.
     *
   randomizer [X]
       [ ]

     Quote:

     If you're talking about the erratic behavior with the speed dropping
     over time, that looks to me like a bug or an issue with that sample.
     Their XT didn't act that way
       While not as bad, I wouldn't call the XT's fan curve good either.
       It's going to start roaring the moment you get to the main menu
       (and you'll probably notice the sudden change) and it won't stop
       until you're back at the desktop.

     Quote:

     Like your icon, BTW, big fan of new Genesis/MD titles - awaiting
     Xeno Crisis currently.
       It is actually from a Genesis/MD game, but one that was released
       last year, not in the 90s. :)
     *
   TJ Hooker [X]
       [ ]

     Quote:

     For anyone that want to know what matter...
     https://static.techspot.com/articles-info/1870/bench/Cost.png
     https://static.techspot.com/articles-info/1870/bench/Cost1.png With
     the actual scoring, this review is a joke. Navi is disrupting
     pricing and almost match a 1080 TI for 400$... however at
     tomshardware it sux.
       Those performance numbers aren't very different than what TH is
       reporting. And values will obviously vary depending on your test
       suite. This review reflects the fact that these otherwise great
       value cards are paired with a mediocre blower cooler, it hardly
       says the cards "suck". Which amounts to saying that we should wait
       for AIB cards, which is pretty common advice for every GPU launch.
     *
   AgentLozen [X]
       [ ]

     Quote:

     For anyone that want to know what matter...
     https://static.techspot.com/articles-info/1870/bench/Cost.png
     https://static.techspot.com/articles-info/1870/bench/Cost1.png With
     the actual scoring, this review is a joke. Navi is disrupting
     pricing and almost match a 1080 TI for 400$... however at
     tomshardware it sux.
       While I was reading this article I was thinking "please please
       please let redgarl post in the forums!!" My wish was granted and
       Redgarl definitely provided.
       Redgarl describes this review as a "joke". He claims that Navi is
       "disrupting pricing" and "almost match a 1080 TI". He provides
       links to the TechSpot 5700 review for "anyone that want to know
       what matter..." This made me genuinely really curious. Is today the
       day that redgarl proves Tomshardware is run by a bunch of hacks? I
       had to know for myself.
       First of all I looked at the test setups for each review. Techspot
       uses a 9900K Intel CPU and 32GB DDR4-3200. Tomshardware's build is
       different. They use a 8086K Intel CPU and 64GB of DDR4 2400.
       Tomshardware documents the drivers used as 431.16 Nvidia Driver for
       the 2060Super and 2070Super but the 430.86 for every other Nvidia
       card. For AMD cards, they use the 19.7.1 driver with the RX 5700
       cards and the 19.6.3 driver for everything else. By comparison,
       Techspot reports that they used the "latest drivers available at
       the time". Its not clear how that compares to Tomshardware. The
       test machine on each respective site is different and that may
       cause differences in benchmarks. I predict (jk. I've already seen
       the results) the Tech Spot benchmarks may be a little higher
       judging by the faster memory and the better CPU.
       I then compared Techspots benchmarks to those featured on
       Tomshardware. First I looked at the Assassin's Creed Odyssey
       results. At 1440p, Techspot reports the Radeon RX 5700XT hit an
       average of 70fps. Meanwhile, Tomshardware was showing... oh dear,
       Odyssey wasn't featured on Tom's. Let's just move on. The
       Techspot's Destiny 2 1440p results show that.. uh oh, it's
       happening again. Techspot didn't feature Destiny 2 benchmarks. This
       is a problem because you can't compare apples-to-apples when the
       same benchmarks aren't used.
       Assassin's Creed Odyssey, Destiny 2, DiRT Rally 2.0, Far Cry 5, Far
       Cry New Dawn, Final Fantasy XV, GTA V, Resident Evil 2, Strange
       Brigade, Tom Clancy's Ghost Recon, Tom Clancy's Rainbow Six Siege,
       World War Z, The Witcher 3, Wolfenstein II: The New Colossus are
       either benchmarked by Tom's or TechSpot but not both. The only
       overlapping games are Battlefield V, Forza Horizon 4, Metro Exodus,
       Shadow of the Tomb Raider, and Tom Clancy's The Division 2.
       Let's look at the Shadow of the Tomb Raider results. Techspot and
       Tom's ran the game at 1440p, highest quality.... oh no.
       Tomshardware used SMAAT2x in it's Shadow of the Tomb Raider
       benchmark. Techspot didn't report using that setting. Well that's
       not apples-to-apples either. A similar situation happens with
       Battlefield V where Tomshardware uses DX12 and Techspot uses DX11.
       For Tom Clancy's The Division 2, I double checked to make sure the
       software is the same between both Techspot and Tom's platforms.
       DX12, 1440p, Ultra quality. Check. Now here is something that
       appears to be genuinely inconsistent between reviews. Techspot's
       averages for the GeForce 2080, 2070 Super, 2060 Super, Radeon RX
       5700, and 5700XT are higher than the Tomshardware numbers. Well
       there you have it. Redgarl has proven that "With the actual
       scoring, this review is a joke." Tomshardware losses, redgarl wins.
       Except that the test systems between Tomshardware and TechSpot are
       different. Isn't this a roller coaster? TechSpot has a better CPU
       and faster ram. I looked carefully at the trends between both
       reviews and lo and behold they show the same thing. Both sites list
       the video card pecking order as: 2080, 2070 Super, Radeon VII, 1080
       Ti, 5700XT, 2070, 2060 Super, Vega 64, 5700, 1080, 2060, and so on
       and so forth. It's very plausible that the differences in specific
       numbers can be chalked up to differences in platforms or margin of
       error.
       The charts that redgarl cherry picked show the results between Toms
       and Techspot are totally different. If you only considered those
       charts then you might think Tomshardware has an incompetent staff
       the way redgarl wants you to believe. However, when you look at the
       whole picture, its obvious that you can't directly compare the
       benchmark results between both sites. A total of 19 games were
       reviewed by both sites. Only five overlap. I threw out Shadow of
       the Tomb Raider because we can't be sure both sites used the same
       settings. That leaves four. When we directly compare results, the
       numbers vary probably due to differences in test hardware, but the
       trends are consistent. In conclusion: there isn't enough evidence
       to prove that the Tomshardware review did anything wrong.
       To redgarl, this is just for fun my dude. No hard feelings.
     *
   alextheblue [X]
       [ ]

     Quote:

     It is actually from a Genesis/MD game, but one that was released
     last year, not in the 90s. :)
       Yeah I know, that's why I pointed it out. It's from Tanglewood.
       That's what I was talking about when I said I am a fan of new
       Genesis games. Xeno Crisis, for example, isn't even (quite) out
       yet.
     *
   randomizer [X]
       [ ]

     Quote:

     Yeah I know, that's why I pointed it out. It's from Tanglewood.
     That's what I was talking about when I said I am a fan of new
     Genesis games. Xeno Crisis, for example, isn't even (quite) out yet.
       Righto, I thought it was just the style that reminded you of
       Genesis games. I didn't expect to run into anyone else who had
       actually heard of this game :LOL:
     *
   treetops422 [X]
       [ ]
       "Across our benchmark suite, Radeon RX 5700 XT averages 9.9%-higher
       frame rates than the GeForce RTX 2060 Super at 2560 x 1440. Radeon
       RX 5700 averages 11%-higher frame rates than the GeForce RTX 2060
       at the same resolution. The GeForce RTX 2070 Super does serve up
       average frame rates 6.9% higher than Radeon RX 5700 XT, but it
       costs 25% more. "
       That's all I need to know. Who would RT and half their performance?
       Esp on a 2060
     *
   B-Real85 [X]
       [ ]

     Quote:

     Honestly I'd pay the extra just for Nvidia's decent cooler, let
     alone power consumption, RTX, etc. Hard pass on another blower.
          + Most consumers buy AIB models, so it's not a problem.
          + Power consumption? RX 5700 consumes about the same as the RTX
            2060 , therefore it has better performance/W ratio. The RX
            5700 XT is near the 2070's ratio.
          + RT: 3 games so far, in which only the RTX 2080 Ti can produce
            tolerable fps with about 40-45 minimums, and it's still on
            FHD. RTX 2060 produces 50-60 average and 30 lows... That's
            what you expect from a 350$ card? Don't think so.

     Quote:

     Dear AMD, Why? Wasn't your line GTX 1080 performance at RX 580
     prices? points at Navi This is not that. Regrettably yours, Your
     Fans
       No, sadly not. There was only a rumoured news on WCCFTech. There
       was 7 or 8 models with 200$ Vega 56 and so on. But AMD never spoke
       about such prices. That was only a rumour (sadly).

     Quote:

     You didn't read the review. Ignoring the Furmark results which don't
     mirror any realworld scenario. The 5700xt is slower than the 2070
     Super while using more power and running over 10degrees C hotter in
     gaming.
       And costs 100$ more. And, wow, really, 5W more than a 2070 Super,
       fantastic. AIB models help you. As 99% of the NV customers also buy
       AIB NV models.

     Quote:

     In all honesty, all these cards are expensive. $350 would have been
     the most I wanted to pay for a 5700XT. And the card does run hot.
     Pascal was a small move up in prices. Turing was just insane pricing
     wise. That being said I bought one today and said "F"-it. I just
     don't like NVIDIA's business ethics. It will get the job done for
     two to three years.
       You are 100% right, just mentioning: if someone needs performance
       over RT feature, RX5700 is absolutely better than the RTX 2060 and
       the RX5700XT is absolutely better than the RTX2060 Super and RTX
       2070.

   Most Popular
    1. Confirmed: AMD’s Navi RX 5700 Graphics Cards Will Be Cheaper Than
       We Thought
    2. Update: Nvidia VP Says Next-Gen GPUs Will Be Made by TSMC and
       Samsung
    3. AMD Radeon RX 5700 XT and RX 5700 Packaging Leak

   The Latest On Tom's Hardware
     * Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight - Review
     * HTC Vive Cosmos VR Headset Review: Solid Upgrade - Review
     * AMD vs Intel: Which CPU Cooks Better Pancakes? - Feature
     * All we Know About Microsoft's Custom SQ1 Processor Insi... - News
     * Microsoft Surface Earbuds Hands-on: Super Comfortable - News
     * Gigabyte Debuts X299X Motherboards for Intel Cascade La... - News
     * AMD Commands 81 Percent of Sales at Mindfactory as Supp... - News
     * Hands-on With AMD-Powered Microsoft Surface Laptop 3: G... - News
     * Microsoft Surface Pro X and Pro 7 Hands-on: Future Forw... - News
     * Specs for Intel's New Xeon Glacier Falls W Workstation... - News
     * Intel Reveals USB 4 Linux Kernel Support Patches - News
     * Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition... - News

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
   #Tom's Hardware

   IFRAME: //www.googletagmanager.com/ns.html?id=GTM-BPDQ

     * Tom's Guide
     * / Tom's Hardware
     * / Laptop Mag
     * / TopTenReviews
     * / AnandTech

   [ ] [ ] [ ]

   ____________________
   (BUTTON) Go
     * Product Reviews
          +
               o Best Overall
               o Cheap CPUs
               o CPUs
               o GPUs
               o PSUs
               o SSDs
               o Cooling
               o Cases
               o Motherboards
               o Memory
               o Thermal Paste
               o Best Builds
               o External Hard Drives
          +
               o Best for Gaming
               o Gaming CPUs
               o Gaming Monitors
               o 4K Gaming Monitors
               o Gaming Desktops
               o Gaming Laptops
               o Gaming Keyboards
               o Gaming Mice
               o VR Headsets
          +
               o Motherboards
               o AMD
               o Intel
               o Coffee Lake Compatible
          +
               o GPU Roundups
               o Nvidia GeForce GTX 1060
               o Nvidia GeForce GTX 1070
               o Nvidia GeForce GTX 1080
               o AMD Radeon RX 560
               o AMD Radeon RX 570
               o AMD Radeon RX 580
               o AMD Radeon RX 480
          +
               o Reviews
               o CPUs
               o Graphics
               o Cases
               o Laptops
               o Monitors
               o Motherboards
               o Power Supplies
               o SSDs
               o Cooling
               o Memory
               o Storage
               o Virtual Reality
               o Keyboards
     * [ ] Buying Guides
          + Monitors
          + Motherboards
          + Gaming Laptops
          + Graphics Cards
          + SSDs
          + PSUs
          + CPUs
     * Raspberry Pi
          +
               o Raspberry Pi 4
               o Pi 4 Review and Tests
               o Overclocking the Pi 4
               o Where to Buy Pi 4
               o Upgrade to Raspbian Buster
          +
               o Getting Started
               o Setting Up for First Time
               o Headless Install
               o Why You Should Buy a Pi
               o 25+ Linux Commands
               o How Windows 10 Runs on a Pi
               o 10 Pi Facts You Didn't Know
          +
               o Tutorials / Projects
               o Make a News Ticker Shirt
               o Run Windows 10 on a Pi
               o Set Up a Web Server
               o Pi GPIO Pinout
               o Make a VPN Gateway
     * Deals
     * Forum
     *

Latest Articles

   See all
   (*) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
     *

Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight
     *

HTC Vive Cosmos VR Headset Review: Solid Upgrade
     *

AMD vs Intel: Which CPU Cooks Better Pancakes?
     *

Hands-on With AMD-Powered Microsoft Surface Laptop 3: Good Keys, USB-C
     *

Microsoft Surface Pro X and Pro 7 Hands-on: Future Forward
     *

Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition CPUs Debut in Surface
Laptop 3 (Updated)
     *

Microsoft Unveils New Surface Laptop and Pro Lineup, Neo Foldable
     *

Cougar Helor 360 Review: Big AIO Cooling With Bigger ARGB Splash
     *

Intel Announces Cascade Lake-X With up to 50% Price Cut, More PCIe (Updated)
     *

CLX Ra Review: Refined Ryzen Power
     *

Report: AMD Radeon RX 5500 to Rival Nvidia GTX 1660
     *

New Surface Pro, Laptop, Arm Tablet Leak Ahead of Microsoft Event
     *

Adata Ultimate SU750 SATA SSD Review: Not Quite The Ultimate SSD
     *

Hands On: Logitech’s G Pro X Keyboard Brings Swappable Mechanical Switches to
the Mainstream
     *

AMD Announces Ryzen 3000 PRO Processors, Picasso PRO APUs, and Athlon PRO

Latest news

   See all
   (*) ( ) ( )
     * cpus
    All we Know About Microsoft's Custom SQ1 Processor Inside the Surface
       Pro X
     * peripherals
    Microsoft Surface Earbuds Hands-on: Super Comfortable
     * motherboards
    Gigabyte Debuts X299X Motherboards for Intel Cascade Lake-X CPUs
     * cpus
    AMD Commands 81 Percent of Sales at Mindfactory as Supply Issues
       Dwindle
     * laptops
    Hands-on With AMD-Powered Microsoft Surface Laptop 3: Good Keys, USB-C
     * laptops
    Microsoft Surface Pro X and Pro 7 Hands-on: Future Forward
     * cpus
    Specs for Intel's New Xeon Glacier Falls W Workstation CPUs Leaked
     * intel
    Intel Reveals USB 4 Linux Kernel Support Patches
     * cpus
    Microsoft Embraces AMD: Custom Ryzen 7 Surface Edition CPUs Debut in
       Surface Laptop 3 (Updated)
     * microsoft
    Microsoft Unveils New Surface Laptop and Pro Lineup, Neo Foldable
     * security
    Intel SAPM Is a New Proposal for Mitigating Spectre Attacks
     * cooling
    Alphacool Debuts RGB Water Blocks for 5700, 5700 XT Graphics Cards
     * graphics
    Nvidia Is Probably Prepping The GeForce GTX 1650 Super
     * cpus
    Researchers Demonstrate First ‘Poor Man’s Qubit’ Probabilistic
       Computing Hardware

     * cpus
    Intel Announces Cascade Lake-X With up to 50% Price Cut, More PCIe
       (Updated)
     * cpus
    AMD Ryzen 5 3500 Allegedly Launches October 5 for $155
     * graphics
    Report: AMD Radeon RX 5500 to Rival Nvidia GTX 1660
     * cooling
    Cooler Master’s MA620M Is ARGB Shrouded in Secrecy
     * gaming
    Nvidia Releases Game Ready Driver for Tom Clancy's Latest
     * cpus
    GlobalFoundries Plans To Go Public in 2022
     * gaming
    IDC: Intel CPUs, Ray Tracing Help Boost Gaming PC Sales
     * windows
    Windows 10's Latest Update Messes With Printing
     * laptops
    New Surface Pro, Laptop, Arm Tablet Leak Ahead of Microsoft Event
     * robots
    DIY Dragon Robot Flies in to Teach Kids Code
     * cpus
    AMD and Nvidia Chip-Maker TSMC Countersues GlobalFoundries for Patent
       Infringement
     * cpus
    Oracle Invested $40 Million in Arm Server Chipmaker Ampere
     * keyboards
    Hands On: Logitech’s G Pro X Keyboard Brings Swappable Mechanical
       Switches to the Mainstream
     * cpus
    Intel Core i9-9900KS: $70 More For Another 300MHz (Update)

     * gaming
    Atari Partners With Antstream Arcade for VCS Retro Games
     * security
    U.S. Congress Thinks DNS Over HTTPS Is Anticompetitive
     * graphics
    Two New Game Bundles Arrive for Ryzen and Radeon Buyers
     * cpus
    AMD Announces Ryzen 3000 PRO Processors, Picasso PRO APUs, and Athlon
       PRO
     * security
    Someone Used a Former NATO Bunker As an Illegal Data Center
     * windows
    Microsoft Hides Windows 10's Offline Account Option
     * laptops
    HP’s Spectre x360 Gets Ice Lake, Slimmer Bezels
     * intel
    Intel VPs of PC Innovation and System Software Depart, New CMO
       Appointed (Updated)
     * security
    New Windows Malware Hides in Plain Sight
     * security
    Microsoft Stops Trusting SSD Makers
     * hot-deals
    Step Into The SSD World With Intel's 1TB SSD 660p For $85
     * cpus
    AMD Ryzen 9 3950X Might Be Available Without Stock Cooler
     * cases
    BitFenix Nova Mesh TG Gets ARGB’d
     * security
    Google Achieves Quantum Supremacy. Is Encryption Safe?

     *
     *
     *
     * 1
     * 2
     * 3

Latest reviews

   See all
   (*) ( ) ( ) ( ) ( )
     * storage

Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight
       Mushkin's new high-end M.2 drive boasts high endurance, fast
       performance and 256-bit AES encryption. Can it best the plethora of
       similarly equipped competition?
     * virtual-reality

HTC Vive Cosmos VR Headset Review: Solid Upgrade
       The HTC Vive Cosmos is a great VR headset held back by bad
       controller ergonomics. It's comfort and lack of base stations makes
       it a good upgrade from the Vive.
     * cooling

Cougar Helor 360 Review: Big AIO Cooling With Bigger ARGB Splash
       An aggressively Illuminated aRGB alternative to better-known
       all-in-ones, does Cougar’s Helor 360 CPU cooler have the
       performance and price to create its own niche?

     * desktops

CLX Ra Review: Refined Ryzen Power
       CLX harnesses the power of Ryzen with its Ra desktop. It’s a
       splashing, expertly assembled rig that’s able to handle most
       high-end tasks with ease.
     * storage

Adata Ultimate SU750 SATA SSD Review: Not Quite The Ultimate SSD
       Adata’s Ultimate SU750 has decent performance for a SATA drive, but
       it slurps power and competitors give you a lot more for the same
       price.
     * motherboards

Asus X570 ROG Crosshair VIII Hero Wi-Fi Review: Mid-Range Menagerie
       Asus’s sub-$400 X570 ROG motherboard is well worth considering
       thanks to its plethora of speedy USB, SATA, and other features, as
       well as robust cooling and solid performance.

     * cooling

Arctic Freezer 34 Esports Duo Cooler Review: A Twin-Fan Triple Threat
       Arctic’s new esports-themed air cooler boasts aggressive styling,
       disruptive pricing and impressive performance.
     * cases

NZXT H710i Case Teardown: Evolutionary Advancements
       NZXT's H710i is a pleasure to build in, looks great, and compared
       to its predecessor the H700i, packs in some nifty little features
       that really separate it out from the rest of the premium ATX tower
       pack.
     * gaming-headsets

Corsair Virtuoso RGB Wireless SE Headset Review: Audiophile Gaming Sound
       Corsair's Virtuoso RGB Wireless SE is the company's first certified
       Hi-Res audio headset, it packs a premium featureset and impressive
       quality, into a justifiable price tag.

     * gaming-headsets

HyperX Cloud Alpha S Gaming Headset Review: Airy Fit, Virtual Surround Sound
       The HyperX Cloud Alpha S gaming headset adds virtual surround sound
       and bass sliders to an already impressive headset, but with varying
       results.
     * storage

Micron 9300 Series Enterprise NVMe SSD Review: Finding Balance in Performance
       NVMe devices have taken off these past few years, but adoption is
       accelerating in 2019. They now surpass both SAS and SATA as the
       preferred interface and are getting faster with each generation.
       That's a trend that won’t go away any time soon because we a
     * gaming-headsets

Roccat Noz Gaming Headset Review: Low Price, Lightweight
       The Roccat Noz gaming headset has competitive budget pricing but
       flounders where it matters most: delivering clear, tight sound.

     * augmented-reality

North Focals Review: Stealthy, Stylish Smart Glasses
       Focals, new smart glasses from North, act and look like
       prescription glasses and have an AR display only the wearer can
       see, with apps like Alexa and Uber.
     * cases

Corsair iCUE 465X RGB Case Review: Smart and Cool
       Priced at $150 (£110), Corsair's latest mid-tower chassis has just
       the right amount of RGB bling and great thermal performance to
       boot, but is it really a smart choice?
     * gaming-headsets

Beyerdynamic Custom Game Gaming Headset Review: Great for RPGs
       Although pricey, the Beyerdynamic Custom Game gaming headset for PC
       and console is a fantastic choice for anyone looking to immerse
       themselves in the RPG world.

     *
     *
     *
     *
     *
     * 1
     * 2
     * 3
     * 4
     * 5

Our best picks

   Selected by our editors who thoroughly test and evaluate each product
   to help you make an informed buying decision.
     * Builds
     * CPUs
     * Graphics
     * SSDs
     * Motherboards
     * Cooling
     * Monitors [ ]
     *
     * Power Supplies
     * Laptops
     * Memory
     * Gaming Desktop
     * Cases
     * VR Headsets

New & Updated Reviews

   See all
   26 mn
   Mushkin Pilot-E M.2 NVMe SSD Review: Taking Flight
   Mushkin's new high-end M.2 drive boasts high endurance, fast
   performance and 256-bit AES encryption. Can it best the plethora of
   similarly equipped competition?
   Oct 3, 2019
   HTC Vive Cosmos VR Headset Review: Solid Upgrade
   The HTC Vive Cosmos is a great VR headset held back by bad controller
   ergonomics. It's comfort and lack of base stations makes it a good
   upgrade from the Vive.
   Oct 2, 2019
   Cougar Helor 360 Review: Big AIO Cooling With Bigger ARGB Splash
   An aggressively Illuminated aRGB alternative to better-known
   all-in-ones, does Cougar’s Helor 360 CPU cooler have the performance
   and price to create its own niche?
   Oct 1, 2019
   CLX Ra Review: Refined Ryzen Power
   CLX harnesses the power of Ryzen with its Ra desktop. It’s a splashing,
   expertly assembled rig that’s able to handle most high-end tasks with
   ease.
   Oct 1, 2019
   Adata Ultimate SU750 SATA SSD Review: Not Quite The Ultimate SSD
   Adata’s Ultimate SU750 has decent performance for a SATA drive, but it
   slurps power and competitors give you a lot more for the same price.

Watch This Next

     *

       IFRAME: https://content.jwplatform.com/players/67Yco6KI.html

View this next

     *

       IFRAME: https://content.jwplatform.com/players/EHmm6Yha.html

Reference and tutorials

   See all
   (*)
     * cpus
    AMD vs Intel: Which CPU Cooks Better Pancakes?
     * cpus
    Even at -180 Degrees, Ryzen 3000 May Not Hit Boost Clock Speeds
     * graphics
    How to Play Borderlands 3 with Integrated Graphics
     * graphics
    Borderlands 3: How it Plays on Different Graphics Cards

   Never regret a purchase again. Get instant access to our latest
   reviews, helpful tips and exclusive deals.
   ____________________
   Subscribe

In Pictures

   (*)
     * web-life

Bluetooth is Named After What? The Real History Behind Tech Brand Names We
Say Every Day
       From Asus to Aorus, many of the top tech brand names have ancient
       origins.
     * modding

Our Favorite Computex 2019 Case Mods: From Beer, to the Iron Throne and Pork
Ramen
       Here are the 23 best PC case mods from Computex 2019.
     * computex

Best of Computex 2019: Overclocked with Innovations
       Much of Computex 2019 hinged on AMD's Ryzen 3000 CPUs and the X570
       chipset. But we also saw plenty of excellent unrelated hardware.
       Here’s the best of what we saw at Taipei’s big tech trade show.

Trending Across ShopSavvy

   (*)
     *

Razer Blade Pro Review
       Thanks to its overclocked Core i7 processor and Nvidia 1080
       graphics, the Razer Blade Pro is one of world's thinnest and most
       powerful gaming laptops
     *

Best Robot Vacuums
       We spent 300 hours testing how robot vacuums cleaned different
       messes on multiple surfaces. Here are the best and worst vacuums
       available
     *

Amazon Echo (Gen 2)
       Amazon's second-gen Echo is more attractive and more affordable
       than the original, and it sounds just as good, too.

From our Community

   Search for a question or topic
   (*) ( )
     *

Apply thermal paste to Ryzen Zen 2 Cpu's.
       Last reply 31 mn by Crosslhs82x2
     *

Delayed Alt+Tab
       Last reply 34 mn by dna112
     *

Looking for a display
       Started by PolarBear27
     *

Dell 7577 No wattage detection after repaste
       Last reply 35 mn by Newtonius
     *

Label Tickets remove or not?
       Last reply 35 mn by ioannis2015v
     *

Need Helping with Used GPU
       Last reply 36 mn by taeioum
     *

Mobo VGA and GPU hmi at the same time?
       Last reply 36 mn by SkyNetRising
     *

Bluetooth dongle refuses to be usable
       Last reply 37 mn by christopher.john.sr
     *

Need BSOD Help Please
       Last reply 38 mn by Tbrown0824
     *

How to fix?
       Last reply 38 mn by Gamingserious5
     *

Help me solve a mystery ?
       Last reply 38 mn by Colif
     *

Ms-7104 bios v3 update
       Last reply 38 mn by corequinha
     *

High clock speed with 3% cpu usage !!
       Last reply 40 mn by Admix.
     *

No display after changing Ram
       Last reply 41 mn by Newtonius
     *

BSODs (mostly ntoskrnl.exe)
       Last reply 42 mn by monteso
     *

Unstable connection using onboard wifi
       Last reply 44 mn by Newtonius
     *

Replacing GPU
       Last reply 44 mn by geofelt
     *

Can I stream with my laptop?
       Started by rosepastel
     *

Changing Motherboard Lenovo To Asus..
       Last reply 44 mn by liwahadri2
     *

Packet loss on Apex Legends
       Last reply 45 mn by Ralston18

     *

rigid disk - size
       Last reply 47 mn by jpatterer
     *

Masterbox 530P hardrive bay?
       Last reply 49 mn by SkyNetRising
     *

Help in Upgrading ram
       Last reply 50 mn by Newtonius
     *

ASRock N68S
       Started by WillimpRove93

     *
     *
     * 1
     * 2

   Edition
     * [ ] United States
          + United Kingdom
          + Italy
          + France
          + Russia
          + Germany

   Subscribe to our newsletter
   ____________________ (BUTTON) icon arrow stem circle
   Company
     * About Tom's Hardware
     * About Purch
     * Advertising
     * Licensing and Reprints

   Resources
     * Contact Us
     * Privacy
     * Copyright
     * Terms Of Use

   Other Purch sites
     * LaptopMag
     * Live Science
     * Tom's Hardware
     * Space.com
     * TopTenReviews
     * AnandTech
     * Tom's Guide
     * Newsarama

     * © 2019 Purch All Rights Reserved.

     *
     *
     *
     *
